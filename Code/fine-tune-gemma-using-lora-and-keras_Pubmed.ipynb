{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8831896,"sourceType":"datasetVersion","datasetId":5314260},{"sourceId":9062110,"sourceType":"datasetVersion","datasetId":5465014},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Model used\n\nWe are using **Gemma** model with 2B parameters, Keras, English version, v2.\n\n## Dataset\n\nWe will fine-tune **Gemma** using a [Medical Q & A](https://www.kaggle.com/datasets/gpreda/medquad/) dataset. This is a subset of the full public dataset [Healthcare NLP: LLMs, Transformers, Datasets](https://www.kaggle.com/datasets/jpmiller/layoutlm).\n\n","metadata":{}},{"cell_type":"markdown","source":"# Prepare packages\n\n\nWe will install updated version of Keras, KerasNLP, which we need for fine-tuning, and other dependencies.","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U tf-keras\n!pip install -q -U keras-nlp==0.10.0\n!pip install -q -U keras>=3","metadata":{"execution":{"iopub.status.busy":"2024-11-22T10:56:24.447069Z","iopub.execute_input":"2024-11-22T10:56:24.447317Z","iopub.status.idle":"2024-11-22T10:58:23.227370Z","shell.execute_reply.started":"2024-11-22T10:56:24.447296Z","shell.execute_reply":"2024-11-22T10:58:23.226519Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.18.0 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.1 requires keras<2.16,>=2.15.0, but you have keras 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:58:26.516593Z","iopub.execute_input":"2024-11-22T10:58:26.516918Z","iopub.status.idle":"2024-11-22T10:58:26.521767Z","shell.execute_reply.started":"2024-11-22T10:58:26.516890Z","shell.execute_reply":"2024-11-22T10:58:26.520875Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import keras_nlp\nimport keras\nimport csv\n\nprint(\"KerasNLP version: \", keras_nlp.__version__)\nprint(\"Keras version: \", keras.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:58:29.062317Z","iopub.execute_input":"2024-11-22T10:58:29.062661Z","iopub.status.idle":"2024-11-22T10:58:35.076916Z","shell.execute_reply.started":"2024-11-22T10:58:29.062621Z","shell.execute_reply":"2024-11-22T10:58:35.076113Z"}},"outputs":[{"name":"stderr","text":"2024-11-22 10:58:29.470786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-11-22 10:58:29.470836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-11-22 10:58:29.472503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"KerasNLP version:  0.10.0\nKeras version:  3.6.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Load the model","metadata":{}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")","metadata":{"execution":{"iopub.status.busy":"2024-11-22T10:58:35.078646Z","iopub.execute_input":"2024-11-22T10:58:35.079536Z","iopub.status.idle":"2024-11-22T10:59:18.202537Z","shell.execute_reply.started":"2024-11-22T10:58:35.079492Z","shell.execute_reply":"2024-11-22T10:59:18.201861Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-22T10:59:18.203520Z","iopub.execute_input":"2024-11-22T10:59:18.203773Z","iopub.status.idle":"2024-11-22T10:59:18.228583Z","shell.execute_reply.started":"2024-11-22T10:59:18.203750Z","shell.execute_reply":"2024-11-22T10:59:18.227804Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Prepare the training data","metadata":{}},{"cell_type":"markdown","source":"We prepare the **Medical Q & A** data for training. We will load the data using the template where, for each data that will be included in the training set, we provide pairs of questions and answers.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the PubMedQA dataset\nds = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n\n# Prepare the data in a similar format\ndata = []\n\n# Iterate through the dataset and format the questions and answers\nfor example in ds['train']:  # You can also use 'validation' or 'test' splits\n    question = example['question']\n    answer = example['final_decision']  # Use 'final_decision' for binary yes/no answers or 'context' for detailed context\n    template = f\"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"\n    data.append(template)\n\n# Print some samples\nfor sample in data[:5]:\n    print(sample)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-22T10:59:34.620965Z","iopub.execute_input":"2024-11-22T10:59:34.621299Z","iopub.status.idle":"2024-11-22T11:00:00.190429Z","shell.execute_reply.started":"2024-11-22T10:59:34.621278Z","shell.execute_reply":"2024-11-22T11:00:00.189492Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aac1fc566b6407cab626378bba07353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/233M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be47c9132ad431cae11b4a89b4b94f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/211269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c257e35b16845d7bc85d3a1cd1362cd"}},"metadata":{}},{"name":"stdout","text":"Question:\nAre group 2 innate lymphoid cells ( ILC2s ) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?\n\nAnswer:\nyes\nQuestion:\nDoes vagus nerve contribute to the development of steatohepatitis and obesity in phosphatidylethanolamine N-methyltransferase deficient mice?\n\nAnswer:\nyes\nQuestion:\nDoes psammaplin A induce Sirtuin 1-dependent autophagic cell death in doxorubicin-resistant MCF-7/adr human breast cancer cells and xenografts?\n\nAnswer:\nyes\nQuestion:\nIs methylation of the FGFR2 gene associated with high birth weight centile in humans?\n\nAnswer:\nyes\nQuestion:\nDo tumor-infiltrating immune cell profiles and their change after neoadjuvant chemotherapy predict response and prognosis of breast cancer?\n\nAnswer:\nyes\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:00:14.536590Z","iopub.execute_input":"2024-11-22T11:00:14.536885Z","iopub.status.idle":"2024-11-22T11:00:14.542317Z","shell.execute_reply.started":"2024-11-22T11:00:14.536863Z","shell.execute_reply":"2024-11-22T11:00:14.541338Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"211269"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"data = data[:300]","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:00:17.744631Z","iopub.execute_input":"2024-11-22T11:00:17.745179Z","iopub.status.idle":"2024-11-22T11:00:17.759093Z","shell.execute_reply.started":"2024-11-22T11:00:17.745149Z","shell.execute_reply":"2024-11-22T11:00:17.758198Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Check model inference before fine tuning","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"We wil first check the model before proceeding to fine-tuning. We will test it with some questions about medical matters.  \n\nFirst, we will define an utility function to display the query and answer from LLM.","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, Markdown\ndef colorize_text(text):\n    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:00:25.774323Z","iopub.execute_input":"2024-11-22T11:00:25.774645Z","iopub.status.idle":"2024-11-22T11:00:25.779331Z","shell.execute_reply.started":"2024-11-22T11:00:25.774622Z","shell.execute_reply":"2024-11-22T11:00:25.778499Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Let's check how we can display the content of one data input using the `colorize_text` function.","metadata":{}},{"cell_type":"code","source":"print(data[3])","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:00:32.337944Z","iopub.execute_input":"2024-11-22T11:00:32.338285Z","iopub.status.idle":"2024-11-22T11:00:32.342746Z","shell.execute_reply.started":"2024-11-22T11:00:32.338260Z","shell.execute_reply":"2024-11-22T11:00:32.341870Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question:\nIs methylation of the FGFR2 gene associated with high birth weight centile in humans?\n\nAnswer:\nyes\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"display(Markdown(colorize_text(data[3])))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:00:39.852488Z","iopub.execute_input":"2024-11-22T11:00:39.852774Z","iopub.status.idle":"2024-11-22T11:00:39.858440Z","shell.execute_reply.started":"2024-11-22T11:00:39.852756Z","shell.execute_reply":"2024-11-22T11:00:39.857474Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nIs methylation of the FGFR2 gene associated with high birth weight centile in humans?\n\n\n\n**<font color='green'>Answer:</font>**\nyes"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"Now we will ask the model to answer to a question for which we know the expected answer.","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the complications of Paget's Disease of Bone ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=128)\ndisplay(Markdown(colorize_text(response)))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:00:44.643213Z","iopub.execute_input":"2024-11-22T11:00:44.643509Z","iopub.status.idle":"2024-11-22T11:00:56.660457Z","shell.execute_reply.started":"2024-11-22T11:00:44.643488Z","shell.execute_reply":"2024-11-22T11:00:56.659525Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nIs low intramucosal pH associated with failure to acidify the gastric lumen in response to pentagastrin?\n\n\n\n**<font color='green'>Answer:</font>**\nyes\n\n\n\n**<font color='red'>Question:</font>**\nWhat is the effect of pentagastrin on the pH of the gastric lumen?\n\n\n\n**<font color='green'>Answer:</font>**\nPentagastrin increases the pH of the gastric lumen.\n\n\n\n**<font color='red'>Question:</font>**\nWhat is the effect of pentagastrin on the pH of the gastric lumen?\n\n\n\n**<font color='green'>Answer:</font>**\nPentagastrin increases the pH of the gastric lumen.\n\n\n\n**<font color='red'>Question:</font>**\nWhat is the effect of pentagastrin on the pH of the gastric lumen?\n\n"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the treatments for Diabetes ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=128)\ndisplay(Markdown(colorize_text(response)))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:01:27.453155Z","iopub.execute_input":"2024-11-22T11:01:27.453478Z","iopub.status.idle":"2024-11-22T11:01:31.663705Z","shell.execute_reply.started":"2024-11-22T11:01:27.453455Z","shell.execute_reply":"2024-11-22T11:01:31.662872Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nIs low intramucosal pH associated with failure to acidify the gastric lumen in response to pentagastrin?\n\n\n\n**<font color='green'>Answer:</font>**\nyes\n\n\n\n**<font color='red'>Question:</font>**\nWhat is the effect of pentagastrin on the pH of the gastric lumen?\n\n\n\n**<font color='green'>Answer:</font>**\nPentagastrin increases the pH of the gastric lumen.\n\n\n\n**<font color='red'>Question:</font>**\nWhat is the effect of pentagastrin on the pH of the gastric lumen?\n\n\n\n**<font color='green'>Answer:</font>**\nPentagastrin increases the pH of the gastric lumen.\n\n\n\n**<font color='red'>Question:</font>**\nWhat is the effect of pentagastrin on the pH of the gastric lumen?\n\n"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Fine-tunning with LoRA   \n\n\nWe are using now **LoRA** for fine-tunning. **LoRA** stands for **Low Rank Adaptation** and is a method for modifying a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by **adjusting only a small, low-rank subset of the model's parameters**.\n","metadata":{}},{"cell_type":"markdown","source":"The rank used here for LoRA controls the number of parameters that will be recalculated during fine-tuning.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 3.\ngemma_lm.backbone.enable_lora(rank=3)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:01:37.010955Z","iopub.execute_input":"2024-11-22T11:01:37.011355Z","iopub.status.idle":"2024-11-22T11:01:37.340414Z","shell.execute_reply.started":"2024-11-22T11:01:37.011328Z","shell.execute_reply":"2024-11-22T11:01:37.339589Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,195,392\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,195,392</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,195,392\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,195,392</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,022,976\u001b[0m (3.90 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,022,976</span> (3.90 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Fine-tune on the Medical QA dataset.\n\n# Limit the input sequence length to 128 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 128\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=10, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:01:42.422396Z","iopub.execute_input":"2024-11-22T11:01:42.423065Z","iopub.status.idle":"2024-11-22T11:22:54.233545Z","shell.execute_reply.started":"2024-11-22T11:01:42.423013Z","shell.execute_reply":"2024-11-22T11:22:54.232808Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 413ms/step - loss: 0.7900 - sparse_categorical_accuracy: 0.4900\nEpoch 2/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 415ms/step - loss: 0.5686 - sparse_categorical_accuracy: 0.5743\nEpoch 3/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 416ms/step - loss: 0.5548 - sparse_categorical_accuracy: 0.5803\nEpoch 4/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 416ms/step - loss: 0.5439 - sparse_categorical_accuracy: 0.5854\nEpoch 5/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 415ms/step - loss: 0.5326 - sparse_categorical_accuracy: 0.5902\nEpoch 6/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 415ms/step - loss: 0.5205 - sparse_categorical_accuracy: 0.5950\nEpoch 7/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 415ms/step - loss: 0.5071 - sparse_categorical_accuracy: 0.6029\nEpoch 8/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 415ms/step - loss: 0.4919 - sparse_categorical_accuracy: 0.6120\nEpoch 9/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 417ms/step - loss: 0.4748 - sparse_categorical_accuracy: 0.6204\nEpoch 10/10\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 417ms/step - loss: 0.4556 - sparse_categorical_accuracy: 0.6336\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x781846a11540>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Inference after fine tuning\nWe will run now the queries through the fine-tuned model.","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the complications of Paget's Disease of Bone ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=128)\ndisplay(Markdown(colorize_text(response)))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:23:11.258937Z","iopub.execute_input":"2024-11-22T11:23:11.259546Z","iopub.status.idle":"2024-11-22T11:23:18.645022Z","shell.execute_reply.started":"2024-11-22T11:23:11.259516Z","shell.execute_reply":"2024-11-22T11:23:18.644194Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nIs low intramucosal pH associated with failure to acidify the gastric lumen in response to pentagastrin?\n\n\n\n**<font color='green'>Answer:</font>**\nyes"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the treatments for Diabetes ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=128)\ndisplay(Markdown(colorize_text(response)))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:23:56.563736Z","iopub.execute_input":"2024-11-22T11:23:56.564475Z","iopub.status.idle":"2024-11-22T11:23:56.837465Z","shell.execute_reply.started":"2024-11-22T11:23:56.564447Z","shell.execute_reply":"2024-11-22T11:23:56.836603Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nIs low intramucosal pH associated with failure to acidify the gastric lumen in response to pentagastrin?\n\n\n\n**<font color='green'>Answer:</font>**\nyes"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"preset = \"./medical_gemma_pubmed\"\n# Save the model to the preset directory.\ngemma_lm.save_to_preset(preset)","metadata":{"execution":{"iopub.status.busy":"2024-11-22T11:24:19.187471Z","iopub.execute_input":"2024-11-22T11:24:19.188339Z","iopub.status.idle":"2024-11-22T11:24:46.491319Z","shell.execute_reply.started":"2024-11-22T11:24:19.188294Z","shell.execute_reply":"2024-11-22T11:24:46.490125Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}