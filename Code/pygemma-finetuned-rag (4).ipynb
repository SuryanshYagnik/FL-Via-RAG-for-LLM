{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7856134,"sourceType":"datasetVersion","datasetId":4607812},{"sourceId":7970419,"sourceType":"datasetVersion","datasetId":4616621},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -q bitsandbytes\n%pip install -q transformers\n%pip install -q peft\n%pip install -q accelerate\n%pip install -q trl\n%pip install -q torch\n%pip install -q qdrant-client langchain pypdf sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:32:10.303968Z","iopub.execute_input":"2025-02-02T08:32:10.304353Z","iopub.status.idle":"2025-02-02T08:32:38.695113Z","shell.execute_reply.started":"2025-02-02T08:32:10.304321Z","shell.execute_reply":"2025-02-02T08:32:38.693932Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## **Load all libraries**","metadata":{}},{"cell_type":"code","source":"!pip install langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T08:32:38.696470Z","iopub.execute_input":"2025-02-02T08:32:38.696778Z","iopub.status.idle":"2025-02-02T08:32:42.220952Z","shell.execute_reply.started":"2025-02-02T08:32:38.696741Z","shell.execute_reply":"2025-02-02T08:32:42.219952Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.16)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\nRequirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.17)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.33)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.7.1)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.24.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (0.3.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.16->langchain_community) (2.10.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain_community) (2.27.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%capture\nimport os, torch\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nfrom datasets import Dataset\nfrom IPython.display import Markdown, display\nfrom langchain_community.document_loaders import PyPDFDirectoryLoader\nfrom langchain.vectorstores import Qdrant\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import HuggingFacePipeline","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:32:42.222911Z","iopub.execute_input":"2025-02-02T08:32:42.223170Z","iopub.status.idle":"2025-02-02T08:33:08.622915Z","shell.execute_reply.started":"2025-02-02T08:32:42.223146Z","shell.execute_reply":"2025-02-02T08:33:08.622193Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"<h3><strong>Know More about <a href=\"https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2\">4-bit quantization</a></strong></h3>","metadata":{}},{"cell_type":"code","source":"model = \"/kaggle/input/m/google/gemma/transformers/2b-it/2\"\n\nbnbConfig = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model, quantization_config=bnbConfig, device_map=\"auto\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model,\n    device_map = \"auto\",\n    quantization_config=bnbConfig\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:08.624165Z","iopub.execute_input":"2025-02-02T08:33:08.624454Z","iopub.status.idle":"2025-02-02T08:33:44.870376Z","shell.execute_reply.started":"2025-02-02T08:33:08.624431Z","shell.execute_reply":"2025-02-02T08:33:44.869640Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd53d48d3b934f7cbe292d76610109eb"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\nuser = \"Write a Python code to display text in a star pattern.\"\n\nprompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=1000)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nMarkdown(text.split(\"AI:\")[1])","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:44.871192Z","iopub.execute_input":"2025-02-02T08:33:44.871432Z","iopub.status.idle":"2025-02-02T08:33:50.232981Z","shell.execute_reply.started":"2025-02-02T08:33:44.871413Z","shell.execute_reply":"2025-02-02T08:33:50.232132Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" \n\n```python\n# This Python code displays a text in a star pattern.\n\n# Define the length of the star.\nlength = 5\n\n# Print the star pattern.\nfor i in range(length):\n    print(\"*\", end=\"\")\n    \n# Print the center star.\nprint(\"*\")\n```\n\n**Output:**\n\n```\n    *\n   ***\n  *****\n *******\n*********\n```"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# **3. Fine Tune Model**","metadata":{}},{"cell_type":"markdown","source":"## **Load the dataset**","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndata = pd.read_csv(\"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\")\n\n# Split into three equal parts\nsplit_ratio = len(data) // 3\ndata_1, data_2, data_3 = data[:split_ratio], data[split_ratio:2*split_ratio], data[2*split_ratio:]\n\n# Convert to Hugging Face datasets\ndataset_1 = Dataset.from_pandas(data_1)\ndataset_2 = Dataset.from_pandas(data_2)\ndataset_3 = Dataset.from_pandas(data_3)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:50.233857Z","iopub.execute_input":"2025-02-02T08:33:50.234097Z","iopub.status.idle":"2025-02-02T08:33:50.311902Z","shell.execute_reply.started":"2025-02-02T08:33:50.234077Z","shell.execute_reply":"2025-02-02T08:33:50.311181Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## **Define a formatting function for the model output.**","metadata":{}},{"cell_type":"code","source":"def formatting_func(example):\n    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n    line = template.format(instruction=example['Question'], response=example['Answer'])\n    return [line]","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:50.312569Z","iopub.execute_input":"2025-02-02T08:33:50.312828Z","iopub.status.idle":"2025-02-02T08:33:50.316763Z","shell.execute_reply.started":"2025-02-02T08:33:50.312807Z","shell.execute_reply":"2025-02-02T08:33:50.315885Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:50.319181Z","iopub.execute_input":"2025-02-02T08:33:50.319415Z","iopub.status.idle":"2025-02-02T08:33:50.335541Z","shell.execute_reply.started":"2025-02-02T08:33:50.319395Z","shell.execute_reply":"2025-02-02T08:33:50.334871Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r = 8,\n    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type = \"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:50.337287Z","iopub.execute_input":"2025-02-02T08:33:50.337486Z","iopub.status.idle":"2025-02-02T08:33:50.354056Z","shell.execute_reply.started":"2025-02-02T08:33:50.337469Z","shell.execute_reply":"2025-02-02T08:33:50.353178Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define training function\ndef fine_tune_model(model, dataset, output_dir):\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        args=TrainingArguments(\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=4,\n            warmup_steps=2,\n            max_steps=50,\n            learning_rate=2e-4,\n            fp16=True,\n            logging_steps=1,\n            output_dir=output_dir,\n            optim=\"paged_adamw_8bit\"\n        ),\n        peft_config=lora_config,\n        formatting_func=formatting_func,\n    )\n    trainer.train()\n    return trainer\n\n# Fine-tune three separate models\nfine_tune_model(model, dataset_1, \"outputs_model_1\")\nfine_tune_model(model, dataset_2, \"outputs_model_2\")\nfine_tune_model(model, dataset_3, \"outputs_model_3\")","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:33:50.354869Z","iopub.execute_input":"2025-02-02T08:33:50.355070Z","iopub.status.idle":"2025-02-02T08:37:03.935469Z","shell.execute_reply.started":"2025-02-02T08:33:50.355045Z","shell.execute_reply":"2025-02-02T08:37:03.934322Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/139 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15b4820eb564984bd792514e18a3aac"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:58, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.124900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.124900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.022500</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.847100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.729400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.635300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.544000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.462800</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.393800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.332900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.277200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.230700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.191900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.158000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.127900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.100300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.077500</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.062000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.040200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.034200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.013900</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.012400</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.011500</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.009000</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.008700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.008600</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.008400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.008300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/139 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b395eaf3fcc0478aa9ec2e0b15a56e6c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:01, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.308600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.308600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.194900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.017600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.876200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.579300</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.494500</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.414800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.336700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.271500</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.220700</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.175200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.137600</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.110100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.087200</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.056700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.046600</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.027000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.012800</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.012200</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.011900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.011500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.009600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/141 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca43a9959a34dfeae436e481668dd74"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:01, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.147400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.147400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.034900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.838400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.717100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.638700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.556700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.478100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.410100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.293300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.241200</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.199100</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.165400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.138600</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.115900</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.096900</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.077800</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.061800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.047700</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.033600</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.027100</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.013300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.012300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.009400</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.009200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.009000</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.008700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7c3447b31ba0>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## **Test the Fine-Tuned Model**","metadata":{}},{"cell_type":"code","source":"system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\nquestion =system + \"What is the difference between a variable and an object\"\n\nprompt = f\"Question: {question} \\n Answer: \"\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=512)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nMarkdown(text.split(\"Answer:\")[1])","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:37:03.936722Z","iopub.execute_input":"2025-02-02T08:37:03.937342Z","iopub.status.idle":"2025-02-02T08:37:29.264711Z","shell.execute_reply.started":"2025-02-02T08:37:03.937301Z","shell.execute_reply":"2025-02-02T08:37:29.263876Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" \n\nA variable is a named memory location that stores a single value. An object is a collection of related variables that are associated with a single logical unit. \n\n**Variables:**\n\n* Are declared with the `=` operator.\n* Are assigned a single value.\n* Are used to store a single value of a specific type.\n* Variables are declared within functions, but they are not created until they are used.\n* Variables are used to store values in memory.\n\n**Objects:**\n\n* Are declared with the `class` keyword.\n* Are created by calling a constructor function.\n* Can contain multiple variables, each of which is associated with a unique logical unit.\n* Objects are created when we need to create an instance of a class.\n* Objects are used to represent real-world entities.\n\nHere is an example that illustrates the difference between a variable and an object:\n\n```python\n# Variable\nname = \"John\"\n\n# Object\nperson = {\"name\": \"John\", \"age\": 30}\n```\n\nIn this example, the `name` variable is a variable that stores the string \"John\". The `person` object is an object that contains two variables, \"name\" and \"age\", both of which are strings.\n\nVariables are created using the `=` operator, while objects are created by calling a constructor function. Variables are used to store a single value of a specific type, while objects can contain multiple values of different types.\n\nVariables are declared within functions, but they are not created until they are used. Objects are created when we need to create an instance of a class. Objects are used to represent real-world entities."},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## **Load documents for RAG**","metadata":{}},{"cell_type":"code","source":"# Instantiate a PyPDFDirectoryLoader object with the specified directory path\npdf_loader = PyPDFDirectoryLoader(\"/kaggle/input/knowledge-base\")\n\n# Load PDF documents from the specified directory\npdfs = pdf_loader.load()","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:37:29.265453Z","iopub.execute_input":"2025-02-02T08:37:29.265789Z","iopub.status.idle":"2025-02-02T08:38:24.652671Z","shell.execute_reply.started":"2025-02-02T08:37:29.265753Z","shell.execute_reply":"2025-02-02T08:38:24.651556Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# import the HuggingFaceEmbeddings class, \nembeddings = HuggingFaceEmbeddings(\n    # This argument specifies the pre-trained model name to be used for generating embeddings.\n    # Here, \"sentence-transformers/all-mpnet-base-v2\" is a pre-trained sentence transformer model \n    # from the Sentence Transformers library (not Transformers).\n    # Sentence transformer models are specifically trained to generate meaningful representations \n    # of sentences that capture semantic similarity.\n    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n\n    # This argument is likely specific to the HuggingFaceEmbeddings class and might \n    # not be present in the base Transformers library.\n    # It sets the device to \"cuda\" to leverage the GPU for faster processing if available.\n    model_kwargs={\"device\": \"cuda\"}\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:38:24.654002Z","iopub.execute_input":"2025-02-02T08:38:24.655245Z","iopub.status.idle":"2025-02-02T08:38:28.224664Z","shell.execute_reply.started":"2025-02-02T08:38:24.655211Z","shell.execute_reply":"2025-02-02T08:38:28.223689Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-13-54edb4065953>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Instantiate a RecursiveCharacterTextSplitter object with specified parameters\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n\n# Split documents into chunks using the RecursiveCharacterTextSplitter\nall_splits = text_splitter.split_documents(pdfs)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:38:28.225706Z","iopub.execute_input":"2025-02-02T08:38:28.226072Z","iopub.status.idle":"2025-02-02T08:38:28.616737Z","shell.execute_reply.started":"2025-02-02T08:38:28.226040Z","shell.execute_reply":"2025-02-02T08:38:28.616008Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Create a Qdrant collection from the document splits\n# For storing and searching document information we use a vector database called Qdrant. \n\nqdrant_collection = Qdrant.from_documents(\n    all_splits,                # List of document splits\n    embeddings,                # HuggingFaceEmbeddings object for generating embeddings\n    location=\":memory:\",       # Location to store the collection (in memory)\n    collection_name=\"all_documents\"  # Name of the Qdrant collection\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:38:28.617568Z","iopub.execute_input":"2025-02-02T08:38:28.617918Z","iopub.status.idle":"2025-02-02T08:42:30.084810Z","shell.execute_reply.started":"2025-02-02T08:38:28.617888Z","shell.execute_reply":"2025-02-02T08:42:30.083931Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Create a retriever\nretriever = qdrant_collection.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:42:30.085633Z","iopub.execute_input":"2025-02-02T08:42:30.085924Z","iopub.status.idle":"2025-02-02T08:42:30.089809Z","shell.execute_reply.started":"2025-02-02T08:42:30.085891Z","shell.execute_reply":"2025-02-02T08:42:30.088970Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# This code creates a pipeline for text generation using a pre-trained model (model) \n# and its tokenizer (tokenizer). It leverages mixed precision (torch.bfloat16) \n# for potentially faster inference and limits generated text to 512 tokens.\npipeline = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer,\n    model_kwargs = {\"torch.dtype\": torch.bfloat16},\n    max_new_tokens=512    \n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:42:30.090734Z","iopub.execute_input":"2025-02-02T08:42:30.091051Z","iopub.status.idle":"2025-02-02T08:42:30.108728Z","shell.execute_reply.started":"2025-02-02T08:42:30.091012Z","shell.execute_reply":"2025-02-02T08:42:30.108070Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"question = \"What is the difference between a variable and an object\"\n\nmessage = [\n    {\"role\": \"user\", \"content\": question},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=512,\n    add_special_tokens=True,\n    do_sample=True,\n    temperature=0.7,\n    top_k=10,\n    top_p=0.95\n)\nMarkdown(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:43:02.605247Z","iopub.execute_input":"2025-02-02T08:43:02.605591Z","iopub.status.idle":"2025-02-02T08:43:26.486297Z","shell.execute_reply.started":"2025-02-02T08:43:02.605560Z","shell.execute_reply":"2025-02-02T08:43:26.485355Z"},"trusted":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Variable**\n\n* A variable is a named memory location that stores a single value.\n* It is declared using the `=` operator.\n* Variables can be used to store different values, but they are associated with a specific memory address.\n* Variables are declared and initialized during the compilation phase.\n* Changes to a variable will not affect other parts of the program.\n\n**Object**\n\n* An object is a collection of zero or more variables and methods that are associated with a specific memory address.\n* It is created using the `new` keyword.\n* Objects can contain references to other objects, allowing them to interact with each other.\n* Objects are dynamically allocated memory.\n* Objects can be used to encapsulate data and code, making them reusable.\n\n**Example**\n\n```python\n# Variable\nname = \"John\"\n\n# Object\nperson = {\"name\": \"John\", \"age\": 30}\n```\n\n**Key Differences:**\n\n| Feature | Variable | Object |\n|---|---|---|\n| Storage | Memory address | Memory address |\n| Declaration | `=` | `new` |\n| Value type | Any | Objects |\n| Reusability | No | Yes |\n| Scope | Global | Local |\n| Lifetime | As long as the program is running | As long as it is referenced |\n\n**Conclusion**\n\nVariables are used to store individual values, while objects are used to store collections of related values and methods. Variables are declared and initialized manually, while objects are created dynamically."},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"gemma_llm = HuggingFacePipeline(\n    pipeline=pipeline,\n    model_kwargs={\n        \"temperature\": 0.7,\n        \"max_new_tokens\": 512,\n        \"add_special_tokens\": True,\n        \"do_sample\": True,\n        \"top_k\": 10,\n        \"top_p\": 0.95\n    },\n)\n# Create a RetrievalQA object\nqa = RetrievalQA.from_chain_type(\n    llm=gemma_llm,  # Pass the text-generation pipeline object\n    chain_type=\"stuff\",\n    retriever=retriever  # retriever object\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:43:33.753465Z","iopub.execute_input":"2025-02-02T08:43:33.753848Z","iopub.status.idle":"2025-02-02T08:43:33.761700Z","shell.execute_reply.started":"2025-02-02T08:43:33.753809Z","shell.execute_reply":"2025-02-02T08:43:33.760710Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-20-5e786cdfe137>:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  gemma_llm = HuggingFacePipeline(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"question = \"Write in detail about python\"\nmessage = [\n    {\"role\": \"user\", \"content\": question},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True, truncation=True)\nresult = qa.invoke(prompt)\nMarkdown(result['result'].split('Helpful Answer:')[1])","metadata":{"execution":{"iopub.status.busy":"2025-02-02T08:43:38.196591Z","iopub.execute_input":"2025-02-02T08:43:38.196959Z","iopub.status.idle":"2025-02-02T08:43:40.899172Z","shell.execute_reply.started":"2025-02-02T08:43:38.196932Z","shell.execute_reply":"2025-02-02T08:43:40.898390Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" The context does not provide any information about Python, so I cannot answer this question from the provided context."},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Federated Learning","metadata":{}},{"cell_type":"code","source":"# Fine-tune three separate models and save them\ntrainer_1 = fine_tune_model(model, dataset_1, \"outputs_model_1\")\ntrainer_1.model.save_pretrained(\"model_1\")  # Save model_1\n\ntrainer_2 = fine_tune_model(model, dataset_2, \"outputs_model_2\")\ntrainer_2.model.save_pretrained(\"model_2\")  # Save model_2\n\ntrainer_3 = fine_tune_model(model, dataset_3, \"outputs_model_3\")\ntrainer_3.model.save_pretrained(\"model_3\")  # Save model_3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T08:43:45.854555Z","iopub.execute_input":"2025-02-02T08:43:45.854897Z","iopub.status.idle":"2025-02-02T08:46:58.247236Z","shell.execute_reply.started":"2025-02-02T08:43:45.854872Z","shell.execute_reply":"2025-02-02T08:46:58.246178Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/139 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84ff9aa962d4dcc8276a8888e31adfc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:59, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.124900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.124900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.023400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.849600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.731700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.638800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.549400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.469500</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.400800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.340200</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.285000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.234300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.195200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.162900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.130500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.079600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.062800</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.051600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.042000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.036000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.013900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.013300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.012300</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.009400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.009200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/139 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de5585f43b7440595c8be859b4bb7c3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:00, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.308600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.308600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.195200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.017600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.876400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.668600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.579700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.494400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.414700</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.336700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.271300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.220700</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.175200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.137500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.110000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.087100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.056600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.046600</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.039200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.031900</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.027000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.012800</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.011900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.011500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.009600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/141 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0e2abb750047279dc3067ad063c95f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:00, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.147400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.147400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.035100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.838200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.716900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.638600</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.556500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.478000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.410100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.350100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.292900</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.241100</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.199100</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.165300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.138600</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.116000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.096900</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.077700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.061800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.047700</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.039200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.033600</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.027000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.013300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.012300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.009400</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.009200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.009000</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.008700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def load_model(model_path, dtype=torch.float16, device=\"cpu\"):\n    \"\"\"Load model with reduced precision and on CPU to save RAM.\"\"\"\n    return AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T08:47:09.377996Z","iopub.execute_input":"2025-02-02T08:47:09.378315Z","iopub.status.idle":"2025-02-02T08:47:09.382819Z","shell.execute_reply.started":"2025-02-02T08:47:09.378292Z","shell.execute_reply":"2025-02-02T08:47:09.381722Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def federated_averaging(model_paths):\n    \"\"\"Perform federated averaging with memory optimization.\"\"\"\n    global_model = load_model(model_paths[0])\n    global_model_state = global_model.state_dict()\n\n    for key in global_model_state.keys():\n        global_model_state[key] = global_model_state[key].to(torch.float32)  # Convert to float32 for accurate averaging\n\n    num_models = len(model_paths)\n\n    # Iterate through remaining models one by one to avoid memory overhead\n    for model_path in model_paths[1:]:\n        model = load_model(model_path)\n        model_state = model.state_dict()\n\n        for key in global_model_state.keys():\n            global_model_state[key] += model_state[key].to(torch.float32)  # Accumulate in float32\n\n        del model  # Free memory\n        torch.cuda.empty_cache()\n\n    # Compute final averaged parameters\n    for key in global_model_state.keys():\n        global_model_state[key] /= num_models  # Average across models\n\n    # Reload the averaged weights into a model\n    final_model = load_model(model_paths[0])  # Initialize from first model's structure\n    final_model.load_state_dict(global_model_state)\n\n    return final_model\n\n# Define model paths instead of loading them all at once\nmodel_paths = [\"model_1\", \"model_2\"]\nglobal_model = federated_averaging(model_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T08:47:13.004186Z","iopub.execute_input":"2025-02-02T08:47:13.004533Z","iopub.status.idle":"2025-02-02T08:48:09.967673Z","shell.execute_reply.started":"2025-02-02T08:47:13.004504Z","shell.execute_reply":"2025-02-02T08:48:09.964334Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40b9765f769404c8d7cdd7b3a16355c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a321fb2c70694b69ba5a03ed89bf2cdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd68fbc60d5a452fb114bc8d6e4bf8cc"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Save the federated averaged model\nsave_path = \"global_model\"\nglobal_model.save_pretrained(save_path)\nprint(f\"Global model saved at: {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T08:49:50.401156Z","iopub.execute_input":"2025-02-02T08:49:50.401542Z","iopub.status.idle":"2025-02-02T08:49:50.601458Z","shell.execute_reply.started":"2025-02-02T08:49:50.401517Z","shell.execute_reply":"2025-02-02T08:49:50.600722Z"}},"outputs":[{"name":"stdout","text":"Global model saved at: global_model\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Load the global model\nglobal_model = AutoModelForCausalLM.from_pretrained(\"global_model\")\n\n# Inspect the parameters\nfor name, param in global_model.named_parameters():\n    print(f\"{name}: {param.data}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T08:55:48.017362Z","iopub.execute_input":"2025-02-02T08:55:48.017791Z","iopub.status.idle":"2025-02-02T08:55:59.533369Z","shell.execute_reply.started":"2025-02-02T08:55:48.017758Z","shell.execute_reply":"2025-02-02T08:55:59.532445Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c9959091e794d3c8b686624a1d15e00"}},"metadata":{}},{"name":"stdout","text":"model.embed_tokens.weight: tensor([[ 5.2344e-01, -3.5889e-02,  5.9814e-02,  ...,  7.7637e-02,\n          2.3535e-01,  3.8330e-02],\n        [ 1.5137e-01, -1.4453e-01, -1.1719e-01,  ..., -1.9409e-02,\n          4.9133e-03, -2.0508e-02],\n        [ 1.0352e-01,  5.0354e-03, -3.2715e-02,  ..., -1.7334e-02,\n         -8.9111e-03, -1.0254e-02],\n        ...,\n        [ 2.7344e-01,  1.2390e-02,  4.2236e-02,  ..., -4.8584e-02,\n          1.9165e-02, -3.0151e-02],\n        [ 2.9102e-01, -6.4453e-02,  6.2012e-02,  ..., -1.9653e-02,\n          7.1289e-02, -1.6689e-04],\n        [ 5.2344e-01, -3.5156e-02,  6.0791e-02,  ...,  7.6172e-02,\n          2.3828e-01,  3.9795e-02]])\nmodel.layers.0.self_attn.q_proj.base_layer.weight: tensor([[-0.0001, -0.0031, -0.0053,  ...,  0.0056,  0.0104, -0.0052],\n        [-0.0002, -0.0087, -0.0012,  ...,  0.0034,  0.0041,  0.0063],\n        [-0.0005,  0.0011,  0.0011,  ..., -0.0062, -0.0064, -0.0018],\n        ...,\n        [-0.0001,  0.0030, -0.0053,  ..., -0.0028, -0.0004, -0.0083],\n        [ 0.0002, -0.0157,  0.0046,  ...,  0.0009, -0.0030, -0.0046],\n        [ 0.0003, -0.0059,  0.0071,  ...,  0.0089,  0.0050,  0.0071]])\nmodel.layers.0.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0047,  0.0010, -0.0174,  ..., -0.0059, -0.0032, -0.0060],\n        [-0.0003, -0.0062, -0.0088,  ...,  0.0131,  0.0080,  0.0081],\n        [ 0.0145,  0.0025,  0.0125,  ..., -0.0004,  0.0010,  0.0074],\n        ...,\n        [ 0.0130,  0.0140,  0.0088,  ..., -0.0078,  0.0030, -0.0038],\n        [-0.0045, -0.0034,  0.0040,  ...,  0.0002, -0.0018,  0.0079],\n        [ 0.0057,  0.0085, -0.0055,  ...,  0.0040, -0.0010, -0.0061]])\nmodel.layers.0.self_attn.q_proj.lora_B.default.weight: tensor([[ 6.1417e-04,  8.7643e-04,  3.3426e-04,  ...,  9.4604e-04,\n          7.1859e-04,  1.3237e-03],\n        [ 7.0238e-04, -5.8603e-04,  2.7657e-04,  ...,  6.0177e-04,\n          1.8692e-04, -6.5136e-04],\n        [ 9.9182e-04,  9.7466e-04, -6.7234e-05,  ...,  6.3705e-04,\n          2.3091e-04, -9.6273e-04],\n        ...,\n        [-1.4210e-03, -9.0313e-04,  1.8425e-03,  ...,  2.6474e-03,\n          2.7428e-03,  2.9125e-03],\n        [-1.3247e-03,  2.1248e-03,  7.9632e-04,  ..., -1.0586e-04,\n          3.6192e-04,  2.4242e-03],\n        [ 2.8610e-05,  1.9588e-03,  1.3161e-04,  ..., -1.1158e-04,\n         -2.3746e-04,  4.6468e-04]])\nmodel.layers.0.self_attn.k_proj.base_layer.weight: tensor([[-3.2043e-04,  8.1177e-03,  3.0365e-03,  ..., -5.3101e-03,\n          1.0605e-03,  2.9297e-03],\n        [ 1.8120e-05, -3.0823e-03,  6.8054e-03,  ..., -1.7090e-03,\n         -5.6458e-03, -9.3842e-04],\n        [-5.1117e-04,  1.0147e-03, -9.0942e-03,  ...,  1.0395e-04,\n         -2.3651e-03,  9.9487e-03],\n        ...,\n        [-4.5967e-04,  2.8038e-04, -1.0742e-02,  ...,  6.2256e-03,\n          3.8300e-03,  3.3951e-04],\n        [-2.0790e-04,  1.2939e-02,  3.2196e-03,  ...,  4.3335e-03,\n         -1.7242e-03,  1.3351e-04],\n        [-2.2697e-04,  3.9673e-03,  5.8899e-03,  ..., -4.0283e-03,\n         -4.4861e-03,  9.8267e-03]])\nmodel.layers.0.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0084,  0.0167,  0.0082,  ...,  0.0025,  0.0017, -0.0110],\n        [-0.0072, -0.0061, -0.0156,  ...,  0.0042, -0.0132,  0.0047],\n        [-0.0198, -0.0063, -0.0184,  ...,  0.0036,  0.0092, -0.0121],\n        ...,\n        [-0.0056, -0.0145, -0.0110,  ..., -0.0024, -0.0023, -0.0063],\n        [-0.0181, -0.0127, -0.0160,  ...,  0.0186, -0.0025,  0.0055],\n        [-0.0052, -0.0053, -0.0057,  ...,  0.0160,  0.0004, -0.0027]])\nmodel.layers.0.self_attn.k_proj.lora_B.default.weight: tensor([[ 0.0001, -0.0012, -0.0007,  ..., -0.0007,  0.0002, -0.0007],\n        [ 0.0004, -0.0012,  0.0017,  ...,  0.0013,  0.0016,  0.0013],\n        [ 0.0005, -0.0015,  0.0024,  ...,  0.0019,  0.0022,  0.0020],\n        ...,\n        [-0.0022,  0.0009,  0.0003,  ...,  0.0015,  0.0014,  0.0018],\n        [-0.0015,  0.0010, -0.0004,  ...,  0.0005,  0.0005,  0.0005],\n        [ 0.0010,  0.0012, -0.0021,  ..., -0.0011, -0.0003, -0.0009]])\nmodel.layers.0.self_attn.v_proj.base_layer.weight: tensor([[ 1.8311e-04,  6.5613e-03, -9.3384e-03,  ...,  3.8818e-02,\n          3.1433e-03,  4.3701e-02],\n        [ 2.9087e-05, -1.4648e-02, -6.1035e-03,  ..., -1.5381e-02,\n         -1.1108e-02, -8.7280e-03],\n        [ 1.3161e-04,  1.4160e-02, -1.2817e-02,  ..., -3.4180e-02,\n         -2.5879e-02, -1.8066e-02],\n        ...,\n        [ 4.5776e-04, -5.4321e-03,  1.0010e-02,  ..., -9.5215e-03,\n         -1.6968e-02, -3.6621e-02],\n        [ 1.0777e-04,  2.2339e-02, -1.4404e-02,  ...,  3.5095e-03,\n         -4.7607e-03, -1.6479e-03],\n        [ 2.8229e-04, -1.4587e-02, -3.7956e-04,  ..., -1.3916e-02,\n         -7.8735e-03,  3.0884e-02]])\nmodel.layers.0.self_attn.v_proj.lora_A.default.weight: tensor([[ 0.0019, -0.0089, -0.0038,  ..., -0.0057, -0.0087,  0.0060],\n        [-0.0093,  0.0092, -0.0010,  ...,  0.0094,  0.0011,  0.0007],\n        [ 0.0143,  0.0056,  0.0178,  ..., -0.0171,  0.0152,  0.0032],\n        ...,\n        [-0.0088, -0.0141,  0.0066,  ...,  0.0192,  0.0204,  0.0181],\n        [ 0.0019,  0.0161, -0.0061,  ...,  0.0004,  0.0002, -0.0033],\n        [-0.0046, -0.0126, -0.0060,  ...,  0.0046,  0.0039,  0.0156]])\nmodel.layers.0.self_attn.v_proj.lora_B.default.weight: tensor([[ 1.0624e-03, -2.9254e-04, -4.8351e-04,  ...,  1.5640e-03,\n          4.4250e-04,  4.0054e-04],\n        [ 8.7547e-04, -1.4725e-03, -1.9979e-04,  ...,  4.1199e-04,\n          7.2765e-04,  5.2404e-04],\n        [-2.0142e-03,  2.2926e-03,  3.6430e-04,  ..., -2.2564e-03,\n         -2.2354e-03, -5.1498e-05],\n        ...,\n        [-4.9233e-05,  5.8651e-04, -3.8242e-04,  ..., -6.4754e-04,\n         -6.2132e-04, -2.4891e-04],\n        [-8.2254e-04, -2.1343e-03, -1.4315e-03,  ...,  7.6914e-04,\n          1.0090e-03,  5.1212e-04],\n        [ 1.8930e-04,  1.8253e-03,  2.3155e-03,  ..., -2.1482e-04,\n         -8.9169e-05, -5.0497e-04]])\nmodel.layers.0.self_attn.o_proj.base_layer.weight: tensor([[ 0.0135,  0.0049, -0.0054,  ..., -0.0062,  0.0022, -0.0034],\n        [ 0.0231,  0.0198, -0.0056,  ...,  0.0018,  0.0078, -0.0054],\n        [ 0.0129,  0.0052, -0.0032,  ..., -0.0137,  0.0006,  0.0012],\n        ...,\n        [-0.0038, -0.0036, -0.0050,  ..., -0.0047, -0.0074,  0.0045],\n        [-0.0042, -0.0088, -0.0007,  ...,  0.0013, -0.0347,  0.0159],\n        [ 0.0042, -0.0084, -0.0057,  ..., -0.0019,  0.0004,  0.0025]])\nmodel.layers.0.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0014,  0.0029, -0.0095,  ...,  0.0030,  0.0057,  0.0024],\n        [-0.0100, -0.0135, -0.0033,  ..., -0.0158,  0.0087, -0.0070],\n        [ 0.0190,  0.0068,  0.0158,  ...,  0.0082, -0.0096,  0.0081],\n        ...,\n        [ 0.0054,  0.0125, -0.0100,  ..., -0.0137, -0.0024, -0.0182],\n        [-0.0010, -0.0054,  0.0096,  ..., -0.0003,  0.0016,  0.0033],\n        [-0.0077,  0.0028, -0.0129,  ..., -0.0068, -0.0167, -0.0039]])\nmodel.layers.0.self_attn.o_proj.lora_B.default.weight: tensor([[-6.7616e-04, -1.7929e-03, -2.1172e-04,  ..., -8.4448e-04,\n          1.1501e-03, -8.5831e-04],\n        [ 1.6899e-03,  7.3528e-04, -5.3310e-04,  ...,  1.1501e-03,\n         -4.8780e-04,  2.0103e-03],\n        [ 1.0519e-03, -1.6184e-03,  3.5191e-04,  ...,  1.3924e-04,\n         -8.2636e-04, -1.7271e-03],\n        ...,\n        [ 4.8637e-04, -1.4143e-03,  4.5037e-04,  ..., -6.1226e-04,\n         -1.2338e-05,  2.9039e-04],\n        [-1.4763e-03, -1.0939e-03,  7.0477e-04,  ..., -9.6703e-04,\n          8.7738e-04, -4.4155e-04],\n        [-2.3842e-05,  6.9380e-05, -1.1406e-03,  ..., -1.7118e-03,\n         -1.1120e-03, -4.4537e-04]])\nmodel.layers.0.mlp.gate_proj.base_layer.weight: tensor([[ 2.9144e-03,  7.3853e-03,  2.5330e-03,  ...,  7.5150e-04,\n          2.7771e-03,  2.6855e-03],\n        [ 1.9409e-02, -3.2806e-04,  2.7466e-03,  ...,  2.5940e-03,\n          7.4463e-03,  5.6458e-03],\n        [ 6.5002e-03,  3.7079e-03,  1.0376e-03,  ...,  2.5330e-03,\n          9.0942e-03, -5.3406e-03],\n        ...,\n        [-1.1047e-02,  1.2457e-05, -1.0559e-02,  ..., -3.6316e-03,\n          6.4392e-03, -2.8076e-03],\n        [ 6.3171e-03, -5.2185e-03,  1.6479e-03,  ...,  5.9814e-03,\n         -3.7079e-03,  3.7384e-03],\n        [ 7.0190e-03,  1.1719e-02,  4.7913e-03,  ..., -1.6022e-03,\n         -5.6152e-03, -8.4839e-03]])\nmodel.layers.0.mlp.gate_proj.lora_A.default.weight: tensor([[-5.3062e-03,  9.3765e-03,  1.3969e-02,  ..., -9.2468e-03,\n          8.1711e-03,  1.2665e-03],\n        [-5.5046e-03,  2.0554e-02, -4.5776e-05,  ..., -1.2276e-02,\n         -1.4229e-03,  1.2245e-03],\n        [ 3.0289e-03, -7.7248e-04, -5.8861e-03,  ..., -9.9640e-03,\n          4.9553e-03, -1.2695e-02],\n        ...,\n        [ 1.9440e-02,  6.4316e-03, -7.8583e-04,  ...,  3.4676e-03,\n         -6.9923e-03, -3.0556e-03],\n        [-9.2010e-03, -2.8076e-03,  5.1193e-03,  ...,  1.6441e-03,\n          8.4076e-03, -4.4632e-03],\n        [-7.9117e-03, -1.5427e-02, -5.6076e-04,  ..., -1.5221e-03,\n          1.9608e-03, -8.4610e-03]])\nmodel.layers.0.mlp.gate_proj.lora_B.default.weight: tensor([[ 6.0320e-04,  1.9226e-03,  2.8419e-03,  ..., -1.7996e-03,\n         -2.8133e-03,  6.7711e-05],\n        [ 2.7275e-03,  1.5507e-03,  1.6394e-03,  ..., -1.1129e-03,\n         -9.8133e-04, -1.4305e-03],\n        [ 7.4768e-04,  4.7016e-04, -1.1711e-03,  ..., -1.5521e-04,\n          1.8501e-03,  9.9659e-04],\n        ...,\n        [ 1.3685e-03,  3.9220e-04,  1.0595e-03,  ...,  4.2462e-04,\n          1.3971e-03, -2.1915e-03],\n        [-1.3180e-03, -1.8768e-03, -2.6436e-03,  ...,  3.4428e-04,\n         -4.8637e-05,  1.0300e-04],\n        [ 7.1526e-07, -1.5440e-03, -4.7541e-04,  ...,  1.1921e-05,\n         -2.3127e-04, -1.6689e-03]])\nmodel.layers.0.mlp.up_proj.base_layer.weight: tensor([[ 1.8311e-03,  1.7166e-03,  3.2196e-03,  ...,  6.8970e-03,\n         -5.1575e-03,  4.3945e-03],\n        [-1.4221e-02,  4.0894e-03,  5.1498e-04,  ...,  5.4169e-04,\n          9.0332e-03,  1.5869e-02],\n        [-2.5392e-05, -1.0742e-02, -1.5717e-03,  ..., -4.2725e-03,\n          2.9602e-03,  1.4099e-02],\n        ...,\n        [-1.9531e-03, -4.9133e-03, -2.0905e-03,  ...,  7.1716e-03,\n         -8.1787e-03,  6.5231e-04],\n        [-1.3962e-03, -3.5400e-03, -4.7302e-03,  ...,  7.8735e-03,\n         -4.2114e-03,  3.6011e-03],\n        [-7.0190e-03, -1.8463e-03, -4.3640e-03,  ..., -2.4109e-03,\n          1.0529e-03, -1.4648e-02]])\nmodel.layers.0.mlp.up_proj.lora_A.default.weight: tensor([[-0.0052, -0.0066,  0.0010,  ..., -0.0014,  0.0039,  0.0052],\n        [ 0.0010, -0.0019,  0.0048,  ..., -0.0200,  0.0187, -0.0017],\n        [ 0.0064,  0.0100,  0.0066,  ...,  0.0113, -0.0041, -0.0040],\n        ...,\n        [-0.0155,  0.0039, -0.0035,  ..., -0.0033, -0.0034, -0.0156],\n        [-0.0024, -0.0164,  0.0057,  ..., -0.0016, -0.0050,  0.0143],\n        [-0.0043, -0.0014,  0.0086,  ..., -0.0003,  0.0005, -0.0128]])\nmodel.layers.0.mlp.up_proj.lora_B.default.weight: tensor([[-1.5087e-03,  2.3222e-04,  1.9436e-03,  ..., -1.7843e-03,\n          4.4394e-04,  4.7159e-04],\n        [-2.2984e-03,  7.7963e-04,  1.4219e-03,  ..., -2.3861e-03,\n          1.7662e-03, -2.3422e-03],\n        [ 7.5626e-04, -3.3140e-04, -1.1444e-03,  ..., -7.2956e-04,\n          1.4858e-03, -5.0306e-05],\n        ...,\n        [-1.6975e-03,  2.3594e-03,  2.4271e-04,  ..., -3.6669e-04,\n          2.3785e-03, -5.6458e-04],\n        [-1.6403e-04, -1.5001e-03, -1.8215e-04,  ..., -1.0681e-04,\n         -5.3215e-04,  2.4719e-03],\n        [-4.3583e-04, -1.3933e-03,  7.6294e-05,  ...,  3.7861e-04,\n          1.9016e-03,  2.9488e-03]])\nmodel.layers.0.mlp.down_proj.base_layer.weight: tensor([[-4.1809e-03, -1.9409e-02,  1.2894e-03,  ..., -4.1580e-04,\n         -5.4932e-03,  8.1177e-03],\n        [ 5.7068e-03,  3.3569e-03, -7.6294e-03,  ..., -5.4932e-03,\n          2.0599e-03,  6.3477e-03],\n        [ 4.1199e-03,  6.7749e-03,  6.1417e-04,  ...,  1.6113e-02,\n          2.5787e-03, -1.4603e-05],\n        ...,\n        [-7.0190e-03, -9.0790e-04, -1.6861e-03,  ..., -1.1902e-02,\n         -1.2329e-02,  3.7842e-03],\n        [ 1.1658e-02,  1.0803e-02,  1.8477e-05,  ...,  9.3994e-03,\n          6.7444e-03,  4.7913e-03],\n        [-7.4463e-03, -9.4604e-03,  1.0681e-02,  ..., -3.7384e-04,\n          8.5831e-05, -3.1662e-04]])\nmodel.layers.0.mlp.down_proj.lora_A.default.weight: tensor([[-6.6757e-04,  3.5629e-03,  1.7042e-03,  ...,  6.4697e-03,\n         -3.3627e-03,  5.6458e-03],\n        [-6.0501e-03,  5.7259e-03, -5.1270e-03,  ..., -2.4529e-03,\n          2.1687e-03, -3.3417e-03],\n        [ 4.9829e-04,  3.3741e-03, -2.5177e-04,  ...,  1.7929e-04,\n         -3.5172e-03,  2.2316e-03],\n        ...,\n        [ 2.5787e-03,  2.1229e-03,  6.9046e-03,  ..., -3.0861e-03,\n          4.0627e-04, -4.0092e-03],\n        [ 3.8109e-03,  1.3371e-03,  2.6464e-04,  ..., -2.4843e-04,\n         -1.7929e-04, -4.0054e-03],\n        [ 8.5831e-06, -8.3847e-03,  6.9714e-04,  ..., -4.2763e-03,\n         -1.9646e-03, -1.8520e-03]])\nmodel.layers.0.mlp.down_proj.lora_B.default.weight: tensor([[ 2.5654e-04,  1.1826e-03,  7.1716e-04,  ...,  2.4223e-03,\n          2.8152e-03, -2.5520e-03],\n        [ 4.5419e-04, -8.8882e-04,  1.0481e-03,  ..., -4.6253e-05,\n          8.3685e-05,  1.3232e-04],\n        [ 4.9734e-04, -5.7554e-04, -1.8406e-03,  ...,  5.4216e-04,\n         -1.7719e-03,  1.0473e-04],\n        ...,\n        [ 2.5487e-04, -1.3390e-03, -2.0103e-03,  ..., -9.1648e-04,\n          1.2169e-03, -1.9474e-03],\n        [ 1.0757e-03, -1.7977e-04,  1.2612e-04,  ..., -1.3189e-03,\n          6.1941e-04,  1.5926e-03],\n        [ 1.0948e-03,  1.0538e-03,  4.8661e-04,  ..., -2.6298e-04,\n          7.2002e-04,  1.9875e-03]])\nmodel.layers.0.input_layernorm.weight: tensor([-1.0000,  3.1094,  0.6602,  ...,  2.4375,  2.6562,  2.7812])\nmodel.layers.0.post_attention_layernorm.weight: tensor([1.4453, 1.9141, 1.7266,  ..., 1.6328, 2.0156, 1.4844])\nmodel.layers.1.self_attn.q_proj.base_layer.weight: tensor([[-3.0756e-05,  5.1880e-04,  3.5400e-03,  ..., -5.3024e-04,\n         -2.5177e-03, -8.3542e-04],\n        [ 3.5477e-04,  3.2806e-03,  1.1520e-03,  ...,  5.8899e-03,\n         -2.7313e-03,  2.2125e-03],\n        [ 1.8082e-03, -1.1826e-03, -2.5177e-03,  ...,  3.3760e-04,\n          1.8311e-04,  4.4250e-03],\n        ...,\n        [ 7.2937e-03, -6.7444e-03,  6.7749e-03,  ...,  3.1128e-02,\n         -3.6469e-03, -4.3335e-03],\n        [ 1.9165e-02,  4.8523e-03,  6.0120e-03,  ...,  1.1353e-02,\n          2.3651e-03, -3.4668e-02],\n        [-8.6670e-03, -2.0386e-02, -5.7983e-03,  ..., -2.9663e-02,\n          1.7242e-03,  1.5991e-02]])\nmodel.layers.1.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0037,  0.0157, -0.0017,  ..., -0.0102,  0.0115, -0.0076],\n        [ 0.0070,  0.0006, -0.0063,  ..., -0.0111,  0.0060, -0.0021],\n        [-0.0151,  0.0086, -0.0059,  ...,  0.0033,  0.0080, -0.0012],\n        ...,\n        [ 0.0050, -0.0015,  0.0113,  ...,  0.0069,  0.0048, -0.0080],\n        [ 0.0019, -0.0027,  0.0002,  ..., -0.0013, -0.0088,  0.0149],\n        [-0.0169, -0.0012, -0.0184,  ...,  0.0051, -0.0044,  0.0085]])\nmodel.layers.1.self_attn.q_proj.lora_B.default.weight: tensor([[-2.2125e-03, -2.4548e-03,  5.6028e-04,  ..., -2.4281e-03,\n         -2.0103e-03, -5.0068e-04],\n        [-2.9640e-03, -2.7008e-03,  5.5742e-04,  ..., -9.6989e-04,\n         -2.2449e-03,  3.0842e-03],\n        [-7.9966e-04,  9.0313e-04, -1.5326e-03,  ..., -1.1740e-03,\n          4.4012e-04,  4.1723e-04],\n        ...,\n        [ 9.3079e-04,  1.8196e-03, -7.4863e-04,  ...,  1.4162e-03,\n          7.8392e-04, -9.8515e-04],\n        [-3.8290e-04, -5.9986e-04,  9.7132e-04,  ..., -1.0014e-05,\n         -8.8358e-04,  1.0242e-03],\n        [-9.5654e-04, -4.8923e-04,  1.4496e-03,  ..., -4.2915e-06,\n         -3.7289e-04, -1.0719e-03]])\nmodel.layers.1.self_attn.k_proj.base_layer.weight: tensor([[-0.0002, -0.0009, -0.0011,  ...,  0.0036, -0.0021,  0.0022],\n        [ 0.0041, -0.0012, -0.0006,  ..., -0.0002,  0.0037, -0.0032],\n        [ 0.0071,  0.0033, -0.0009,  ..., -0.0092,  0.0103,  0.0003],\n        ...,\n        [ 0.0038,  0.0014,  0.0128,  ..., -0.0020,  0.0179, -0.0049],\n        [ 0.0114, -0.0237,  0.0170,  ...,  0.0043, -0.0011, -0.0014],\n        [-0.0043,  0.0128, -0.0044,  ..., -0.0072, -0.0114,  0.0042]])\nmodel.layers.1.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0028,  0.0027,  0.0089,  ...,  0.0179, -0.0055,  0.0084],\n        [-0.0068,  0.0052,  0.0090,  ...,  0.0061,  0.0126,  0.0073],\n        [ 0.0059, -0.0047,  0.0117,  ..., -0.0160, -0.0053, -0.0059],\n        ...,\n        [ 0.0083,  0.0121, -0.0063,  ..., -0.0063,  0.0093, -0.0037],\n        [ 0.0092,  0.0062,  0.0071,  ...,  0.0009,  0.0109,  0.0068],\n        [-0.0037,  0.0045,  0.0043,  ..., -0.0129, -0.0031, -0.0042]])\nmodel.layers.1.self_attn.k_proj.lora_B.default.weight: tensor([[ 7.5054e-04, -1.4544e-04,  1.1015e-04,  ..., -1.6785e-03,\n         -7.8678e-04, -2.3603e-04],\n        [-1.1969e-03,  2.0866e-03,  1.6327e-03,  ...,  3.9721e-04,\n         -1.0443e-03, -2.2256e-04],\n        [ 2.8443e-04, -1.1663e-03, -5.7888e-04,  ...,  1.8692e-03,\n          5.7459e-04,  1.1444e-05],\n        ...,\n        [-1.8635e-03,  1.7118e-04,  2.7466e-04,  ..., -1.0643e-03,\n          3.6168e-04, -1.0228e-04],\n        [-8.3113e-04,  7.5054e-04, -1.2102e-03,  ...,  9.4318e-04,\n         -1.4067e-05, -1.7953e-04],\n        [-5.8508e-04, -1.7166e-04,  6.2048e-05,  ..., -6.2370e-04,\n         -5.4932e-04,  2.1183e-04]])\nmodel.layers.1.self_attn.v_proj.base_layer.weight: tensor([[-0.0069,  0.0142, -0.0135,  ...,  0.0048,  0.0043,  0.0151],\n        [-0.0029, -0.0013, -0.0097,  ..., -0.0089, -0.0187,  0.0116],\n        [-0.0037, -0.0031,  0.0006,  ..., -0.0048, -0.0239, -0.0391],\n        ...,\n        [ 0.0111, -0.0071, -0.0160,  ..., -0.0067, -0.0031,  0.0041],\n        [ 0.0082, -0.0124,  0.0287,  ...,  0.0043,  0.0011,  0.0062],\n        [ 0.0264,  0.0072,  0.0006,  ...,  0.0068,  0.0151,  0.0143]])\nmodel.layers.1.self_attn.v_proj.lora_A.default.weight: tensor([[-5.2948e-03, -3.5095e-03, -9.1324e-03,  ...,  5.2223e-03,\n         -1.0384e-02, -1.0818e-02],\n        [-8.1711e-03, -9.1095e-03,  3.6964e-03,  ...,  1.5472e-02,\n          5.6953e-03, -9.1553e-04],\n        [ 2.6741e-03, -1.6296e-02, -2.6703e-05,  ...,  2.8419e-03,\n         -1.5259e-03,  4.6043e-03],\n        ...,\n        [ 1.0689e-02, -1.6541e-02,  4.5300e-04,  ..., -9.9792e-03,\n         -9.8724e-03, -9.9182e-05],\n        [-4.7112e-03,  6.9885e-03,  3.0365e-03,  ...,  6.4392e-03,\n          1.0651e-02,  7.9956e-03],\n        [ 7.7820e-04, -1.4000e-03, -2.6741e-03,  ..., -5.1651e-03,\n         -1.2375e-02, -1.2230e-02]])\nmodel.layers.1.self_attn.v_proj.lora_B.default.weight: tensor([[-2.0676e-03,  2.2888e-05,  1.4067e-03,  ...,  9.2268e-04,\n         -2.2278e-03, -1.0548e-03],\n        [ 1.4257e-03, -2.1591e-03,  8.5402e-04,  ...,  3.9530e-04,\n          5.9652e-04,  1.4095e-03],\n        [ 8.6069e-04, -2.7633e-04, -3.4237e-04,  ..., -1.0185e-03,\n         -6.7711e-04,  6.9904e-04],\n        ...,\n        [ 7.8964e-04, -8.2874e-04,  6.1321e-04,  ...,  6.6853e-04,\n          1.2789e-03,  1.2331e-03],\n        [-1.1730e-04,  8.6355e-04, -5.4169e-04,  ...,  1.7862e-03,\n         -8.1015e-04, -1.3123e-03],\n        [-1.4954e-03,  1.5926e-03,  2.8038e-04,  ..., -1.1177e-03,\n         -1.5688e-03, -2.9087e-04]])\nmodel.layers.1.self_attn.o_proj.base_layer.weight: tensor([[ 0.0055, -0.0012, -0.0015,  ..., -0.0063, -0.0008,  0.0034],\n        [ 0.0024,  0.0007,  0.0086,  ..., -0.0081, -0.0107, -0.0078],\n        [ 0.0025, -0.0059,  0.0010,  ...,  0.0063, -0.0148, -0.0136],\n        ...,\n        [ 0.0081, -0.0048, -0.0043,  ..., -0.0109,  0.0005,  0.0091],\n        [-0.0082, -0.0043, -0.0067,  ..., -0.0140, -0.0014,  0.0105],\n        [-0.0079,  0.0005,  0.0053,  ...,  0.0075, -0.0011, -0.0034]])\nmodel.layers.1.self_attn.o_proj.lora_A.default.weight: tensor([[-4.4556e-03,  6.4850e-05, -6.2943e-03,  ...,  6.1569e-03,\n          1.7746e-02, -1.1337e-02],\n        [ 1.3466e-02,  3.1357e-03, -4.3716e-03,  ...,  1.9913e-02,\n          8.1711e-03,  1.9653e-02],\n        [ 5.9509e-03,  6.2637e-03,  3.3035e-03,  ...,  4.6463e-03,\n          5.7411e-03, -3.1509e-03],\n        ...,\n        [-8.4972e-04,  8.8501e-03, -1.4057e-03,  ...,  1.3687e-02,\n          8.4457e-03,  2.1851e-02],\n        [ 1.1124e-02,  1.3367e-02, -3.3474e-04,  ...,  2.3994e-03,\n          1.0614e-03,  2.3308e-03],\n        [-8.6060e-03,  1.2650e-02, -6.7291e-03,  ..., -9.1400e-03,\n         -5.1880e-04, -2.8152e-03]])\nmodel.layers.1.self_attn.o_proj.lora_B.default.weight: tensor([[ 1.0693e-04, -5.8889e-04, -9.2506e-05,  ..., -1.8940e-03,\n         -2.0180e-03,  5.6839e-04],\n        [ 7.6914e-04, -1.5373e-03, -3.0160e-04,  ..., -6.9046e-04,\n         -1.2779e-03,  2.6083e-04],\n        [-1.5955e-03, -2.5320e-04,  7.6580e-04,  ...,  4.4346e-04,\n          1.9302e-03,  1.2894e-03],\n        ...,\n        [ 4.0746e-04, -8.6117e-04,  2.0885e-04,  ..., -1.4305e-03,\n         -5.7650e-04, -6.3848e-04],\n        [-1.1959e-03,  1.5497e-04,  1.4267e-03,  ..., -1.9302e-03,\n          4.0817e-04, -5.1498e-05],\n        [ 2.9612e-04,  9.3508e-04, -1.5354e-03,  ..., -4.8447e-04,\n         -1.4639e-04, -2.4986e-04]])\nmodel.layers.1.mlp.gate_proj.base_layer.weight: tensor([[ 0.0060, -0.0010,  0.0010,  ..., -0.0036, -0.0056,  0.0072],\n        [ 0.0067,  0.0068, -0.0044,  ..., -0.0049, -0.0128,  0.0056],\n        [ 0.0052,  0.0047,  0.0085,  ..., -0.0044, -0.0009,  0.0051],\n        ...,\n        [-0.0115, -0.0135,  0.0039,  ..., -0.0045,  0.0089,  0.0087],\n        [ 0.0055, -0.0127, -0.0035,  ..., -0.0086, -0.0009, -0.0056],\n        [ 0.0006, -0.0001,  0.0063,  ...,  0.0045, -0.0004,  0.0035]])\nmodel.layers.1.mlp.gate_proj.lora_A.default.weight: tensor([[-2.2945e-03,  4.8027e-03, -1.1238e-02,  ...,  4.9553e-03,\n          5.1956e-03,  4.9515e-03],\n        [ 1.2329e-02,  8.8959e-03, -4.6082e-03,  ..., -1.0948e-02,\n         -1.5144e-03, -5.0163e-03],\n        [ 1.2543e-02,  7.4043e-03,  1.7014e-02,  ...,  5.7869e-03,\n          1.6663e-02,  2.0447e-03],\n        ...,\n        [-4.4785e-03, -1.2779e-04,  2.7695e-03,  ..., -1.1734e-02,\n          3.6049e-03,  1.1444e-05],\n        [-2.1095e-03, -3.7785e-03,  6.5498e-03,  ...,  1.1711e-02,\n          4.9286e-03,  2.0889e-02],\n        [-1.8864e-03,  1.2421e-02, -5.1079e-03,  ...,  5.1308e-03,\n         -3.7632e-03,  4.3488e-03]])\nmodel.layers.1.mlp.gate_proj.lora_B.default.weight: tensor([[-2.6913e-03,  7.5817e-04, -8.9645e-05,  ..., -4.8685e-04,\n          5.9032e-04, -1.2264e-03],\n        [-8.0156e-04, -1.2331e-03,  1.2608e-03,  ..., -2.6169e-03,\n         -7.8726e-04,  1.9836e-03],\n        [ 3.4332e-04,  3.6526e-04, -7.3338e-04,  ..., -8.9073e-04,\n         -1.5807e-04,  3.3522e-04],\n        ...,\n        [ 5.0068e-04,  8.1921e-04, -2.9135e-04,  ...,  1.1778e-03,\n         -1.5259e-05, -1.6232e-03],\n        [-2.1439e-03, -1.0338e-03,  1.5426e-04,  ...,  2.4414e-03,\n          1.0738e-03,  3.1662e-03],\n        [-5.2929e-04, -8.8310e-04,  7.3910e-04,  ..., -1.0185e-03,\n          7.9632e-04, -1.6928e-04]])\nmodel.layers.1.mlp.up_proj.base_layer.weight: tensor([[-1.2634e-02, -2.2095e-02,  6.6833e-03,  ...,  1.2634e-02,\n         -4.3640e-03,  8.3008e-03],\n        [-8.4839e-03,  1.4282e-02, -4.4823e-04,  ...,  3.1433e-03,\n          2.7061e-05, -1.0864e-02],\n        [ 1.6098e-03,  1.2436e-03,  1.1063e-03,  ..., -7.8201e-04,\n          1.1475e-02, -7.7057e-04],\n        ...,\n        [-1.1719e-02,  1.4038e-02,  3.3951e-04,  ..., -6.1035e-03,\n         -9.7656e-03,  1.7212e-02],\n        [ 1.1108e-02, -1.5163e-04, -5.0049e-03,  ..., -1.2054e-03,\n         -1.3504e-03,  2.2583e-03],\n        [-9.1934e-04,  2.4567e-03, -4.9744e-03,  ..., -3.7842e-03,\n          7.3853e-03, -5.4016e-03]])\nmodel.layers.1.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0066,  0.0040, -0.0006,  ...,  0.0019,  0.0111,  0.0011],\n        [ 0.0132, -0.0058, -0.0031,  ...,  0.0121,  0.0056,  0.0089],\n        [ 0.0087,  0.0016,  0.0127,  ...,  0.0006,  0.0123, -0.0190],\n        ...,\n        [ 0.0018,  0.0079,  0.0132,  ...,  0.0222,  0.0012,  0.0169],\n        [ 0.0009,  0.0136, -0.0132,  ..., -0.0169, -0.0174, -0.0126],\n        [-0.0202,  0.0048, -0.0123,  ...,  0.0024, -0.0134, -0.0021]])\nmodel.layers.1.mlp.up_proj.lora_B.default.weight: tensor([[-1.3046e-03,  2.0084e-03,  6.4945e-04,  ...,  8.5545e-04,\n          1.4095e-03, -3.9077e-04],\n        [ 1.2817e-03, -1.2541e-03, -5.5885e-04,  ..., -9.7513e-04,\n          1.7290e-03, -6.4850e-04],\n        [ 1.0700e-03, -8.2612e-05, -1.5926e-04,  ..., -6.7425e-04,\n         -1.3199e-03, -1.5354e-03],\n        ...,\n        [ 2.2888e-03, -5.7268e-04, -4.0169e-03,  ...,  9.2173e-04,\n          1.6623e-03,  1.6546e-04],\n        [ 7.9036e-05,  3.0470e-04, -5.3406e-05,  ...,  5.6171e-04,\n         -6.0320e-04,  1.1663e-03],\n        [-1.0481e-03,  1.4830e-04, -1.3695e-03,  ...,  7.0000e-04,\n         -4.5776e-04, -1.6098e-03]])\nmodel.layers.1.mlp.down_proj.base_layer.weight: tensor([[-0.0099,  0.0112, -0.0137,  ..., -0.0004,  0.0013,  0.0014],\n        [-0.0276,  0.0056,  0.0002,  ..., -0.0006,  0.0090, -0.0038],\n        [ 0.0015,  0.0021, -0.0024,  ..., -0.0042, -0.0051,  0.0042],\n        ...,\n        [ 0.0049,  0.0010, -0.0010,  ...,  0.0006, -0.0074,  0.0028],\n        [ 0.0061,  0.0025,  0.0014,  ..., -0.0121, -0.0052, -0.0097],\n        [-0.0041, -0.0060, -0.0020,  ...,  0.0030, -0.0055,  0.0050]])\nmodel.layers.1.mlp.down_proj.lora_A.default.weight: tensor([[-1.0757e-03,  1.4648e-03,  4.1466e-03,  ..., -5.7564e-03,\n         -1.2627e-03, -4.2267e-03],\n        [-2.8515e-03, -7.6437e-04,  2.1305e-03,  ..., -9.1553e-04,\n         -5.8823e-03, -6.5231e-04],\n        [-4.2458e-03, -2.6169e-03, -5.8842e-04,  ..., -8.5602e-03,\n          3.1548e-03, -5.4970e-03],\n        ...,\n        [ 2.0218e-04, -4.3335e-03, -1.0929e-03,  ...,  1.9474e-03,\n          2.5177e-04,  2.4414e-04],\n        [ 2.5005e-03, -2.6131e-03, -2.0275e-03,  ...,  2.1324e-03,\n         -6.2294e-03, -3.2177e-03],\n        [-5.7030e-04,  2.2755e-03, -2.4796e-03,  ...,  1.2550e-03,\n          1.8291e-03, -6.4850e-05]])\nmodel.layers.1.mlp.down_proj.lora_B.default.weight: tensor([[ 3.1166e-03, -1.0719e-03, -2.3994e-03,  ...,  1.5306e-03,\n         -2.2459e-04,  8.3351e-04],\n        [-8.1158e-04,  2.1381e-03, -1.3256e-03,  ...,  9.2459e-04,\n          1.2302e-03, -5.9223e-04],\n        [-4.4727e-04, -4.2915e-04,  2.0142e-03,  ...,  1.1444e-03,\n         -6.2847e-04,  1.1787e-03],\n        ...,\n        [-2.0385e-05,  4.5395e-04, -2.1625e-04,  ...,  4.2295e-04,\n          6.5804e-04,  6.8665e-04],\n        [-1.2732e-03,  8.6308e-04,  9.3079e-04,  ...,  2.0390e-03,\n         -2.2488e-03,  1.4400e-03],\n        [-1.1644e-03, -1.2560e-03, -9.8038e-04,  ...,  1.3685e-03,\n         -1.5736e-03,  1.6346e-03]])\nmodel.layers.1.input_layernorm.weight: tensor([1.1250, 0.8086, 1.1172,  ..., 1.0781, 0.9766, 0.8047])\nmodel.layers.1.post_attention_layernorm.weight: tensor([1.7266, 1.4766, 1.7266,  ..., 1.4062, 1.7422, 1.2031])\nmodel.layers.2.self_attn.q_proj.base_layer.weight: tensor([[-2.8381e-03, -3.2959e-03, -4.5166e-03,  ...,  8.6212e-04,\n         -3.8147e-03,  2.3346e-03],\n        [-3.1891e-03,  4.7913e-03, -4.9744e-03,  ..., -3.8757e-03,\n          1.7776e-03, -6.4392e-03],\n        [ 1.2512e-03, -6.1340e-03, -2.0905e-03,  ...,  1.8555e-02,\n          4.3030e-03, -3.3264e-03],\n        ...,\n        [ 1.2741e-03, -6.4468e-04, -2.1973e-03,  ...,  2.5482e-03,\n          8.8501e-03, -6.5308e-03],\n        [ 3.7193e-04,  1.3275e-03, -6.8665e-03,  ..., -1.6327e-03,\n         -2.6550e-03, -1.1063e-03],\n        [ 6.2561e-03,  3.1738e-03, -6.0425e-03,  ...,  7.0632e-06,\n         -3.2806e-03, -3.8757e-03]])\nmodel.layers.2.self_attn.q_proj.lora_A.default.weight: tensor([[-0.0052,  0.0030, -0.0021,  ..., -0.0005,  0.0098, -0.0068],\n        [ 0.0105, -0.0091,  0.0093,  ...,  0.0018, -0.0132,  0.0134],\n        [-0.0136,  0.0147,  0.0008,  ..., -0.0041,  0.0137,  0.0026],\n        ...,\n        [-0.0028, -0.0061, -0.0043,  ...,  0.0060, -0.0157, -0.0002],\n        [-0.0022, -0.0135,  0.0015,  ..., -0.0074,  0.0020,  0.0057],\n        [-0.0026,  0.0106, -0.0086,  ...,  0.0098,  0.0089, -0.0054]])\nmodel.layers.2.self_attn.q_proj.lora_B.default.weight: tensor([[-4.7207e-04, -1.5144e-03,  4.5776e-05,  ..., -7.3338e-04,\n         -3.1567e-03, -8.5354e-04],\n        [-1.4143e-03,  2.0294e-03,  5.4836e-04,  ..., -5.8413e-04,\n          2.4719e-03, -4.0722e-04],\n        [ 2.0466e-03,  1.1492e-04, -2.0275e-03,  ..., -5.8937e-04,\n         -1.2646e-03, -2.0256e-03],\n        ...,\n        [ 3.0327e-04, -2.9039e-04, -5.3215e-04,  ...,  7.3385e-04,\n         -8.5449e-04,  9.2125e-04],\n        [-1.3285e-03, -5.0545e-05, -7.2336e-04,  ..., -2.0256e-03,\n          7.1239e-04,  1.6184e-03],\n        [ 2.6131e-03,  2.7428e-03, -2.1696e-04,  ...,  2.2888e-04,\n          1.1902e-03, -2.5101e-03]])\nmodel.layers.2.self_attn.k_proj.base_layer.weight: tensor([[ 0.0014, -0.0049,  0.0007,  ..., -0.0002,  0.0035,  0.0060],\n        [ 0.0017, -0.0146,  0.0027,  ..., -0.0035,  0.0129, -0.0090],\n        [-0.0130,  0.0171, -0.0053,  ...,  0.0051, -0.0114,  0.0079],\n        ...,\n        [-0.0220,  0.0019, -0.0320,  ..., -0.0084, -0.0096, -0.0148],\n        [-0.0032, -0.0026,  0.0003,  ..., -0.0042,  0.0079, -0.0029],\n        [ 0.0072, -0.0145,  0.0025,  ...,  0.0106, -0.0113,  0.0073]])\nmodel.layers.2.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0130, -0.0063, -0.0029,  ...,  0.0056,  0.0181,  0.0025],\n        [ 0.0073, -0.0015, -0.0005,  ..., -0.0061,  0.0062,  0.0106],\n        [ 0.0220,  0.0118,  0.0041,  ...,  0.0064,  0.0028,  0.0105],\n        ...,\n        [-0.0008,  0.0075,  0.0139,  ..., -0.0036,  0.0097, -0.0055],\n        [ 0.0118,  0.0046,  0.0041,  ..., -0.0025,  0.0030,  0.0027],\n        [-0.0022, -0.0067, -0.0037,  ..., -0.0136,  0.0076, -0.0190]])\nmodel.layers.2.self_attn.k_proj.lora_B.default.weight: tensor([[-1.5230e-03, -9.7609e-04,  2.0027e-05,  ...,  9.3269e-04,\n         -1.3485e-03, -7.6389e-04],\n        [ 2.5826e-03, -7.5340e-04,  1.7071e-03,  ..., -3.0518e-03,\n         -5.5075e-04, -1.1549e-03],\n        [ 4.3368e-04,  5.9938e-04, -4.5013e-04,  ...,  4.8876e-04,\n          2.8133e-03, -1.0109e-04],\n        ...,\n        [-1.4019e-04, -1.2875e-05,  1.6403e-04,  ...,  1.1225e-03,\n          6.9237e-04, -1.4877e-03],\n        [ 9.7179e-04,  1.3256e-04, -7.0858e-04,  ...,  8.4877e-05,\n         -2.7323e-04,  1.3847e-03],\n        [ 1.5926e-03,  2.1029e-04, -1.4057e-03,  ...,  3.1614e-04,\n          9.3174e-04,  1.7052e-03]])\nmodel.layers.2.self_attn.v_proj.base_layer.weight: tensor([[-0.0024, -0.0008, -0.0142,  ...,  0.0119,  0.0014, -0.0101],\n        [-0.0015, -0.0020,  0.0016,  ..., -0.0120, -0.0186,  0.0018],\n        [-0.0116,  0.0070,  0.0091,  ...,  0.0056,  0.0036, -0.0159],\n        ...,\n        [ 0.0048, -0.0063,  0.0010,  ..., -0.0077,  0.0074,  0.0099],\n        [-0.0074,  0.0017,  0.0056,  ..., -0.0221, -0.0134, -0.0021],\n        [-0.0251,  0.0028,  0.0137,  ...,  0.0091, -0.0204,  0.0091]])\nmodel.layers.2.self_attn.v_proj.lora_A.default.weight: tensor([[ 4.9591e-05, -1.4816e-02,  7.9041e-03,  ..., -7.9727e-03,\n         -1.9806e-02,  3.2330e-03],\n        [-4.4327e-03,  1.7899e-02,  1.2222e-02,  ..., -8.8120e-03,\n          2.5043e-03, -8.4610e-03],\n        [-4.7874e-03,  5.7449e-03, -6.2485e-03,  ..., -1.7761e-02,\n         -2.9793e-03, -3.7594e-03],\n        ...,\n        [-9.9335e-03, -1.3344e-02, -1.4801e-02,  ..., -3.0594e-03,\n         -5.0125e-03, -4.6692e-03],\n        [-9.4604e-04,  1.2817e-02, -5.9662e-03,  ...,  2.1629e-03,\n          9.8228e-04, -1.4954e-03],\n        [-6.9847e-03, -7.9880e-03, -1.1436e-02,  ..., -1.0056e-02,\n          7.6675e-04, -1.4019e-03]])\nmodel.layers.2.self_attn.v_proj.lora_B.default.weight: tensor([[ 4.7684e-06, -8.2016e-04,  3.2759e-04,  ...,  2.8801e-04,\n         -4.2915e-04, -1.1902e-03],\n        [-1.9798e-03, -5.1403e-04,  3.2473e-04,  ...,  1.7624e-03,\n         -1.6594e-03,  1.6146e-03],\n        [ 2.7122e-03, -1.0347e-03, -1.0014e-04,  ..., -2.1591e-03,\n          2.3613e-03, -1.1444e-03],\n        ...,\n        [ 5.8746e-04,  8.7166e-04,  9.8801e-04,  ..., -3.9601e-04,\n          1.7376e-03, -8.8882e-04],\n        [ 2.1000e-03, -1.2693e-03, -7.7057e-04,  ..., -2.7442e-04,\n         -2.4033e-03, -6.6328e-04],\n        [-3.3998e-04,  2.5902e-03,  2.2354e-03,  ..., -9.7656e-04,\n         -3.5334e-04,  2.0752e-03]])\nmodel.layers.2.self_attn.o_proj.base_layer.weight: tensor([[ 0.0107,  0.0173,  0.0099,  ...,  0.0117,  0.0043,  0.0055],\n        [ 0.0020, -0.0011, -0.0018,  ..., -0.0019,  0.0066,  0.0058],\n        [ 0.0110, -0.0114,  0.0033,  ...,  0.0069,  0.0077, -0.0012],\n        ...,\n        [-0.0024,  0.0016, -0.0019,  ..., -0.0129, -0.0082,  0.0098],\n        [-0.0095,  0.0079,  0.0061,  ...,  0.0029,  0.0007, -0.0050],\n        [-0.0023,  0.0027,  0.0031,  ...,  0.0175, -0.0071,  0.0142]])\nmodel.layers.2.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0150, -0.0086,  0.0028,  ...,  0.0144, -0.0085,  0.0082],\n        [-0.0060,  0.0020,  0.0108,  ...,  0.0065, -0.0001,  0.0125],\n        [-0.0012,  0.0118,  0.0045,  ...,  0.0006,  0.0141,  0.0056],\n        ...,\n        [ 0.0014, -0.0155, -0.0154,  ...,  0.0012,  0.0089,  0.0044],\n        [ 0.0003,  0.0021, -0.0010,  ..., -0.0178,  0.0017,  0.0019],\n        [ 0.0008,  0.0140,  0.0003,  ..., -0.0093,  0.0045, -0.0141]])\nmodel.layers.2.self_attn.o_proj.lora_B.default.weight: tensor([[ 4.1008e-05, -5.1928e-04,  2.2364e-04,  ...,  1.5717e-03,\n         -5.9903e-05,  1.1787e-03],\n        [-1.1816e-03,  8.0013e-04,  4.1795e-04,  ..., -3.6383e-04,\n         -1.0605e-03,  3.0613e-04],\n        [-1.0347e-03, -1.3046e-03,  1.2436e-03,  ..., -1.5736e-05,\n         -5.0926e-04, -6.4564e-04],\n        ...,\n        [-1.5473e-04,  4.7278e-04, -1.9341e-03,  ..., -9.4509e-04,\n          4.2915e-04, -9.6512e-04],\n        [ 1.3590e-04, -2.4867e-04,  1.0548e-03,  ..., -5.3453e-04,\n         -3.7146e-04, -4.3869e-05],\n        [ 1.1826e-03,  2.1133e-03, -2.0027e-04,  ..., -3.5048e-04,\n          1.0185e-03, -8.2970e-05]])\nmodel.layers.2.mlp.gate_proj.base_layer.weight: tensor([[-5.1880e-03, -1.4343e-03,  6.4697e-03,  ..., -9.0942e-03,\n         -1.8082e-03, -5.1270e-03],\n        [ 1.4954e-03,  1.7929e-03, -2.6550e-03,  ..., -3.6774e-03,\n         -8.3542e-04, -2.3651e-03],\n        [ 4.8218e-03, -4.7607e-03, -1.4343e-03,  ..., -2.1210e-03,\n          5.1880e-03,  4.5471e-03],\n        ...,\n        [-5.5237e-03,  1.4404e-02,  2.8687e-03,  ..., -5.4321e-03,\n         -1.8005e-03, -1.0498e-02],\n        [-3.2234e-04, -6.5308e-03,  6.1035e-03,  ..., -5.4321e-03,\n          2.0752e-03,  1.8239e-05],\n        [ 4.1199e-03,  5.0964e-03, -1.4801e-03,  ...,  1.0376e-03,\n         -1.4832e-02, -2.7466e-03]])\nmodel.layers.2.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0056,  0.0031, -0.0175,  ...,  0.0098, -0.0039,  0.0163],\n        [ 0.0036,  0.0164, -0.0024,  ..., -0.0059, -0.0107, -0.0064],\n        [-0.0070, -0.0038, -0.0120,  ...,  0.0075, -0.0040,  0.0035],\n        ...,\n        [ 0.0046,  0.0080,  0.0125,  ..., -0.0032, -0.0013,  0.0070],\n        [-0.0059,  0.0095,  0.0048,  ..., -0.0168,  0.0068, -0.0155],\n        [ 0.0003, -0.0168, -0.0081,  ...,  0.0131, -0.0105,  0.0041]])\nmodel.layers.2.mlp.gate_proj.lora_B.default.weight: tensor([[-0.0023,  0.0008,  0.0014,  ...,  0.0012,  0.0003,  0.0019],\n        [ 0.0003,  0.0017,  0.0015,  ...,  0.0002,  0.0001,  0.0008],\n        [ 0.0008,  0.0008, -0.0001,  ...,  0.0004, -0.0004, -0.0003],\n        ...,\n        [-0.0026,  0.0011,  0.0019,  ...,  0.0007,  0.0014,  0.0003],\n        [-0.0010, -0.0017, -0.0013,  ..., -0.0009, -0.0014, -0.0025],\n        [-0.0010,  0.0018,  0.0012,  ..., -0.0016,  0.0013,  0.0008]])\nmodel.layers.2.mlp.up_proj.base_layer.weight: tensor([[ 0.0085, -0.0038, -0.0027,  ..., -0.0045,  0.0070,  0.0093],\n        [ 0.0012, -0.0134,  0.0018,  ...,  0.0069,  0.0060, -0.0017],\n        [-0.0043,  0.0012,  0.0039,  ..., -0.0006,  0.0022, -0.0018],\n        ...,\n        [-0.0022,  0.0036,  0.0069,  ..., -0.0046, -0.0051,  0.0010],\n        [ 0.0011, -0.0053, -0.0049,  ..., -0.0135,  0.0056, -0.0087],\n        [-0.0007,  0.0012,  0.0003,  ..., -0.0013, -0.0076,  0.0027]])\nmodel.layers.2.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0069,  0.0003, -0.0165,  ..., -0.0141, -0.0002,  0.0011],\n        [ 0.0050,  0.0196,  0.0181,  ...,  0.0047, -0.0036,  0.0134],\n        [-0.0075, -0.0052,  0.0032,  ...,  0.0095,  0.0145, -0.0127],\n        ...,\n        [-0.0009, -0.0013, -0.0001,  ..., -0.0188,  0.0001, -0.0097],\n        [ 0.0035,  0.0117,  0.0031,  ...,  0.0103,  0.0093,  0.0041],\n        [-0.0059, -0.0004,  0.0055,  ..., -0.0030, -0.0094, -0.0138]])\nmodel.layers.2.mlp.up_proj.lora_B.default.weight: tensor([[ 8.0347e-04, -1.9264e-03, -9.5177e-04,  ...,  9.0027e-04,\n         -2.9678e-03, -1.3790e-03],\n        [ 2.5845e-04, -7.5388e-04,  1.4896e-03,  ...,  2.2316e-04,\n          3.4165e-04,  8.4543e-04],\n        [ 1.0338e-03, -7.1049e-04, -5.4693e-04,  ...,  3.0327e-04,\n         -9.2506e-04, -3.1424e-04],\n        ...,\n        [-2.0123e-03,  1.8415e-03,  2.4109e-03,  ..., -2.0123e-03,\n         -8.0299e-04,  1.8444e-03],\n        [ 2.3994e-03, -1.7452e-03,  9.4175e-05,  ...,  5.1308e-04,\n          1.1597e-03, -2.0618e-03],\n        [ 1.6699e-03,  4.5109e-04, -1.0118e-03,  ...,  2.0676e-03,\n         -4.3917e-04,  4.6921e-04]])\nmodel.layers.2.mlp.down_proj.base_layer.weight: tensor([[ 0.0075,  0.0015,  0.0042,  ..., -0.0034,  0.0010,  0.0074],\n        [-0.0078, -0.0070, -0.0023,  ...,  0.0016,  0.0059,  0.0053],\n        [ 0.0011,  0.0102, -0.0070,  ...,  0.0120, -0.0097, -0.0048],\n        ...,\n        [ 0.0024, -0.0008, -0.0001,  ..., -0.0005, -0.0132, -0.0082],\n        [ 0.0034,  0.0060,  0.0005,  ..., -0.0068,  0.0096, -0.0097],\n        [-0.0045, -0.0061, -0.0013,  ..., -0.0078, -0.0028,  0.0036]])\nmodel.layers.2.mlp.down_proj.lora_A.default.weight: tensor([[ 9.0408e-04, -3.1013e-03,  2.6016e-03,  ..., -6.0806e-03,\n         -3.6888e-03, -3.5954e-04],\n        [-1.6890e-03,  7.5760e-03, -5.0545e-05,  ..., -4.9400e-03,\n         -1.7147e-03, -1.2665e-03],\n        [-1.0538e-03, -6.4278e-04, -1.1358e-03,  ...,  2.8610e-04,\n         -7.3433e-04,  1.7805e-03],\n        ...,\n        [ 2.3460e-03,  3.2043e-03, -4.5967e-04,  ...,  2.1439e-03,\n          5.3139e-03,  7.0477e-04],\n        [ 4.5967e-03,  2.3346e-03,  1.2054e-03,  ..., -1.9588e-03,\n          4.4289e-03,  9.2506e-04],\n        [-3.4523e-04,  3.9558e-03, -7.5531e-03,  ...,  9.6798e-04,\n         -2.2125e-03,  2.4395e-03]])\nmodel.layers.2.mlp.down_proj.lora_B.default.weight: tensor([[-1.4992e-03, -1.5583e-03,  8.3828e-04,  ...,  2.1439e-03,\n          5.9319e-04,  1.3018e-03],\n        [-1.1101e-03, -3.2005e-03,  2.3258e-04,  ...,  1.1644e-03,\n          1.0643e-03,  1.9550e-03],\n        [-4.7493e-04, -2.7962e-03, -1.9550e-05,  ...,  2.7161e-03,\n          7.9632e-04,  5.6744e-05],\n        ...,\n        [ 1.1463e-03, -2.7180e-04, -4.8685e-04,  ...,  2.4533e-04,\n         -9.5654e-04,  2.0599e-03],\n        [ 5.4979e-04,  1.0195e-03, -2.4338e-03,  ..., -1.2789e-03,\n         -3.7718e-04,  1.0424e-03],\n        [ 2.7323e-04,  1.9398e-03, -3.3712e-04,  ..., -2.2831e-03,\n         -1.8311e-03,  2.2812e-03]])\nmodel.layers.2.input_layernorm.weight: tensor([1.3906, 1.1484, 1.4141,  ..., 1.3047, 1.4453, 1.0000])\nmodel.layers.2.post_attention_layernorm.weight: tensor([2.5938, 1.8047, 2.5781,  ..., 1.9531, 2.7031, 1.4844])\nmodel.layers.3.self_attn.q_proj.base_layer.weight: tensor([[ 0.0006,  0.0016, -0.0005,  ..., -0.0018,  0.0013,  0.0019],\n        [ 0.0038,  0.0053,  0.0039,  ...,  0.0020,  0.0010,  0.0039],\n        [ 0.0003, -0.0109, -0.0050,  ..., -0.0067,  0.0058,  0.0090],\n        ...,\n        [-0.0101, -0.0143,  0.0028,  ..., -0.0099,  0.0033, -0.0106],\n        [ 0.0030,  0.0029,  0.0034,  ...,  0.0023,  0.0042,  0.0043],\n        [ 0.0045,  0.0167, -0.0020,  ...,  0.0110, -0.0111, -0.0222]])\nmodel.layers.3.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0097, -0.0011, -0.0084,  ..., -0.0176,  0.0071, -0.0110],\n        [ 0.0054,  0.0005, -0.0017,  ...,  0.0140, -0.0018,  0.0152],\n        [ 0.0068, -0.0062,  0.0028,  ...,  0.0110,  0.0034,  0.0050],\n        ...,\n        [-0.0143,  0.0084, -0.0158,  ...,  0.0020, -0.0086, -0.0033],\n        [-0.0066, -0.0060,  0.0003,  ..., -0.0046,  0.0018, -0.0063],\n        [-0.0012,  0.0045,  0.0006,  ...,  0.0097, -0.0023,  0.0035]])\nmodel.layers.3.self_attn.q_proj.lora_B.default.weight: tensor([[ 1.7452e-03,  1.4067e-03,  4.8065e-04,  ...,  4.0960e-04,\n          1.0796e-03,  1.2827e-04],\n        [-1.0147e-03, -8.1396e-04,  1.2827e-03,  ...,  1.6441e-03,\n         -9.4557e-04, -6.1750e-04],\n        [ 1.1845e-03,  7.0000e-04, -9.1314e-04,  ..., -5.5313e-04,\n          2.1601e-04, -1.5581e-04],\n        ...,\n        [-3.1471e-04, -2.3251e-03, -1.4496e-04,  ..., -8.0395e-04,\n          9.6560e-04,  2.9850e-04],\n        [-1.9569e-03,  5.3453e-04,  2.4529e-03,  ..., -1.0605e-03,\n         -8.6975e-04, -1.0891e-03],\n        [-3.7956e-04, -1.6098e-03, -1.4019e-03,  ...,  3.7861e-04,\n         -7.9155e-05,  4.0817e-04]])\nmodel.layers.3.self_attn.k_proj.base_layer.weight: tensor([[-0.0007,  0.0005, -0.0038,  ..., -0.0032,  0.0063, -0.0140],\n        [-0.0004,  0.0044, -0.0159,  ...,  0.0038, -0.0153,  0.0041],\n        [ 0.0014,  0.0025, -0.0135,  ...,  0.0058, -0.0083,  0.0040],\n        ...,\n        [-0.0029, -0.0151,  0.0069,  ..., -0.0039,  0.0014, -0.0141],\n        [ 0.0008,  0.0131, -0.0021,  ..., -0.0032,  0.0035,  0.0107],\n        [ 0.0041,  0.0245,  0.0030,  ..., -0.0006, -0.0026, -0.0234]])\nmodel.layers.3.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0189,  0.0084,  0.0004,  ..., -0.0075, -0.0031, -0.0110],\n        [-0.0146, -0.0183, -0.0138,  ...,  0.0062, -0.0110, -0.0018],\n        [-0.0130, -0.0103,  0.0048,  ..., -0.0066, -0.0162, -0.0126],\n        ...,\n        [-0.0168,  0.0016, -0.0061,  ...,  0.0045, -0.0070,  0.0057],\n        [-0.0054,  0.0122, -0.0173,  ..., -0.0166, -0.0158, -0.0092],\n        [ 0.0045, -0.0019,  0.0172,  ...,  0.0015, -0.0033, -0.0068]])\nmodel.layers.3.self_attn.k_proj.lora_B.default.weight: tensor([[-1.1940e-03, -1.1101e-03,  6.4325e-04,  ...,  5.0640e-04,\n         -1.7624e-03, -2.3670e-03],\n        [-5.5552e-04, -1.4467e-03,  3.2425e-04,  ...,  1.1806e-03,\n         -1.1444e-05, -8.2493e-04],\n        [-1.7881e-03, -1.0595e-03, -1.3351e-03,  ..., -3.1757e-04,\n         -2.6245e-03, -2.5463e-04],\n        ...,\n        [ 1.5926e-04, -1.5688e-04, -9.9468e-04,  ..., -1.8692e-03,\n         -2.6741e-03, -9.7942e-04],\n        [ 1.3494e-03,  1.1921e-03, -1.3361e-03,  ...,  5.8556e-04,\n          4.4060e-04,  9.8705e-05],\n        [ 1.1692e-03,  4.4680e-04,  1.8826e-03,  ..., -1.3361e-03,\n         -1.2646e-03,  2.1720e-04]])\nmodel.layers.3.self_attn.v_proj.base_layer.weight: tensor([[-0.0006, -0.0046,  0.0006,  ..., -0.0064, -0.0012,  0.0035],\n        [-0.0121, -0.0010, -0.0086,  ..., -0.0051,  0.0021,  0.0038],\n        [-0.0087, -0.0031, -0.0101,  ...,  0.0023,  0.0015, -0.0005],\n        ...,\n        [-0.0198, -0.0010, -0.0106,  ...,  0.0071, -0.0041,  0.0069],\n        [ 0.0151, -0.0130, -0.0115,  ..., -0.0019, -0.0020, -0.0004],\n        [-0.0005, -0.0145,  0.0025,  ...,  0.0087,  0.0060,  0.0120]])\nmodel.layers.3.self_attn.v_proj.lora_A.default.weight: tensor([[-0.0034,  0.0042, -0.0050,  ..., -0.0033, -0.0067,  0.0022],\n        [ 0.0059,  0.0113,  0.0045,  ...,  0.0017,  0.0064,  0.0016],\n        [ 0.0024, -0.0017,  0.0083,  ...,  0.0074, -0.0043, -0.0004],\n        ...,\n        [ 0.0058, -0.0036,  0.0048,  ...,  0.0021, -0.0168, -0.0038],\n        [-0.0125,  0.0144, -0.0054,  ..., -0.0036, -0.0015, -0.0047],\n        [ 0.0059, -0.0010, -0.0046,  ..., -0.0083,  0.0078, -0.0028]])\nmodel.layers.3.self_attn.v_proj.lora_B.default.weight: tensor([[ 1.5888e-03, -7.1526e-05,  2.3174e-03,  ..., -3.1352e-04,\n         -3.3331e-04, -1.5354e-04],\n        [-8.5068e-04,  1.2589e-03, -9.4795e-04,  ...,  3.1471e-04,\n          2.1338e-05,  1.5354e-03],\n        [ 9.7179e-04,  3.7432e-04, -1.8001e-05,  ...,  9.6607e-04,\n         -4.1103e-04,  7.6294e-05],\n        ...,\n        [ 3.2496e-04,  1.3447e-03,  5.9700e-04,  ..., -1.4782e-03,\n          4.5729e-04, -2.1172e-04],\n        [-1.2970e-03,  1.8454e-03,  3.8624e-05,  ..., -1.6775e-03,\n         -5.4884e-04, -3.5667e-04],\n        [-2.9087e-04,  1.3218e-03, -1.4992e-03,  ...,  1.0624e-03,\n          7.4100e-04, -1.8597e-04]])\nmodel.layers.3.self_attn.o_proj.base_layer.weight: tensor([[-0.0044,  0.0142, -0.0079,  ...,  0.0038, -0.0032,  0.0025],\n        [ 0.0060, -0.0261,  0.0145,  ..., -0.0038,  0.0059,  0.0024],\n        [ 0.0061,  0.0019,  0.0060,  ...,  0.0028, -0.0002, -0.0006],\n        ...,\n        [-0.0119, -0.0082, -0.0083,  ..., -0.0014,  0.0035, -0.0051],\n        [-0.0004,  0.0047,  0.0052,  ...,  0.0068,  0.0002, -0.0032],\n        [ 0.0024,  0.0194,  0.0052,  ..., -0.0038,  0.0013,  0.0010]])\nmodel.layers.3.self_attn.o_proj.lora_A.default.weight: tensor([[ 0.0117, -0.0004, -0.0095,  ..., -0.0072,  0.0200, -0.0106],\n        [ 0.0053, -0.0061, -0.0020,  ...,  0.0147, -0.0149,  0.0113],\n        [-0.0043,  0.0087,  0.0147,  ..., -0.0186, -0.0016, -0.0007],\n        ...,\n        [ 0.0020, -0.0038, -0.0080,  ..., -0.0019,  0.0050,  0.0041],\n        [-0.0049,  0.0020,  0.0003,  ...,  0.0062,  0.0150,  0.0095],\n        [-0.0027,  0.0057, -0.0197,  ...,  0.0145, -0.0003, -0.0012]])\nmodel.layers.3.self_attn.o_proj.lora_B.default.weight: tensor([[ 2.3830e-04, -5.0545e-04, -1.3752e-03,  ..., -3.2806e-04,\n         -1.8301e-03, -3.5095e-04],\n        [ 9.9754e-04,  7.2145e-04, -1.8597e-03,  ...,  1.4782e-04,\n         -1.4114e-03, -4.3154e-04],\n        [-1.9193e-04, -7.7438e-04,  6.9523e-04,  ..., -2.1019e-03,\n         -3.5477e-04, -1.3981e-03],\n        ...,\n        [ 8.9979e-04,  1.7748e-03, -3.0708e-03,  ...,  3.4142e-04,\n         -2.1458e-06,  8.4734e-04],\n        [-4.5705e-04, -1.4782e-04,  7.4816e-04,  ...,  2.0390e-03,\n         -1.3475e-03, -3.1233e-04],\n        [-1.3752e-03,  1.5712e-04,  4.3845e-04,  ...,  1.9798e-03,\n         -4.6301e-04,  1.7929e-03]])\nmodel.layers.3.mlp.gate_proj.base_layer.weight: tensor([[ 0.0234, -0.0012,  0.0057,  ..., -0.0011,  0.0067, -0.0066],\n        [ 0.0125, -0.0021, -0.0004,  ...,  0.0037,  0.0064,  0.0022],\n        [ 0.0032,  0.0034,  0.0055,  ..., -0.0027, -0.0032,  0.0025],\n        ...,\n        [-0.0018,  0.0117, -0.0007,  ...,  0.0065,  0.0002, -0.0120],\n        [ 0.0021,  0.0067,  0.0097,  ..., -0.0042,  0.0010,  0.0057],\n        [ 0.0037, -0.0021, -0.0104,  ..., -0.0001, -0.0124, -0.0111]])\nmodel.layers.3.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0123, -0.0119,  0.0036,  ..., -0.0113, -0.0053,  0.0007],\n        [ 0.0037,  0.0058,  0.0108,  ...,  0.0189,  0.0070, -0.0022],\n        [ 0.0027, -0.0061, -0.0057,  ...,  0.0044,  0.0078, -0.0034],\n        ...,\n        [-0.0049, -0.0081, -0.0155,  ..., -0.0185, -0.0084, -0.0052],\n        [-0.0091, -0.0015, -0.0039,  ..., -0.0029, -0.0021, -0.0081],\n        [-0.0099, -0.0109, -0.0035,  ...,  0.0041,  0.0041,  0.0098]])\nmodel.layers.3.mlp.gate_proj.lora_B.default.weight: tensor([[ 2.3079e-04,  6.8426e-04,  9.0504e-04,  ..., -9.1743e-04,\n          2.4681e-03,  2.0657e-03],\n        [ 7.1192e-04, -2.8305e-03, -1.1444e-03,  ..., -1.0519e-03,\n          7.1669e-04,  7.2718e-04],\n        [-6.1655e-04, -8.2970e-04,  1.6842e-03,  ...,  3.0947e-04,\n         -1.0118e-03, -2.2926e-03],\n        ...,\n        [-1.7433e-03,  1.2751e-03, -2.9325e-05,  ..., -3.2425e-04,\n          2.9039e-04,  2.2411e-04],\n        [ 7.1430e-04,  1.6356e-03,  3.3283e-04,  ..., -1.6403e-04,\n         -2.0599e-03, -6.8092e-04],\n        [ 2.7390e-03, -6.1560e-04, -2.7237e-03,  ..., -1.7061e-03,\n          7.1430e-04,  7.9632e-05]])\nmodel.layers.3.mlp.up_proj.base_layer.weight: tensor([[ 0.0034,  0.0106, -0.0069,  ...,  0.0114,  0.0056,  0.0016],\n        [-0.0084,  0.0002, -0.0049,  ...,  0.0043, -0.0104,  0.0042],\n        [-0.0063,  0.0050, -0.0104,  ..., -0.0028, -0.0027,  0.0043],\n        ...,\n        [-0.0017,  0.0013, -0.0106,  ..., -0.0092, -0.0184,  0.0031],\n        [ 0.0006,  0.0039, -0.0007,  ...,  0.0156,  0.0085, -0.0134],\n        [ 0.0106, -0.0034,  0.0084,  ...,  0.0038, -0.0002,  0.0030]])\nmodel.layers.3.mlp.up_proj.lora_A.default.weight: tensor([[-0.0184,  0.0039, -0.0105,  ...,  0.0169,  0.0010,  0.0198],\n        [-0.0057,  0.0044, -0.0068,  ..., -0.0051, -0.0120, -0.0028],\n        [-0.0173, -0.0116, -0.0027,  ..., -0.0020,  0.0042, -0.0042],\n        ...,\n        [ 0.0002,  0.0141,  0.0025,  ...,  0.0087,  0.0056, -0.0043],\n        [ 0.0096,  0.0053,  0.0051,  ...,  0.0100,  0.0189,  0.0036],\n        [ 0.0090, -0.0112, -0.0036,  ..., -0.0066,  0.0139,  0.0052]])\nmodel.layers.3.mlp.up_proj.lora_B.default.weight: tensor([[-8.1062e-04,  1.9388e-03,  5.4550e-04,  ...,  2.5635e-03,\n         -2.4338e-03, -1.0147e-03],\n        [ 2.2621e-03,  2.2125e-04, -4.3869e-05,  ...,  5.9986e-04,\n          1.7328e-03,  1.6212e-03],\n        [ 4.5991e-04,  7.0000e-04, -7.6723e-04,  ..., -1.4830e-04,\n          1.6155e-03, -1.3008e-03],\n        ...,\n        [-2.1057e-03,  5.5122e-04, -2.5392e-05,  ...,  1.9255e-03,\n         -2.5749e-05, -3.2377e-04],\n        [-5.4598e-05, -5.8556e-04, -1.8368e-03,  ...,  1.3475e-03,\n         -7.1478e-04, -9.7275e-04],\n        [-6.0081e-04,  7.3910e-05,  4.2915e-05,  ..., -1.4763e-03,\n         -7.7248e-04,  2.6588e-03]])\nmodel.layers.3.mlp.down_proj.base_layer.weight: tensor([[-0.0038, -0.0069, -0.0067,  ..., -0.0022, -0.0047, -0.0026],\n        [-0.0070, -0.0063,  0.0015,  ...,  0.0016, -0.0027, -0.0060],\n        [-0.0018,  0.0060, -0.0093,  ..., -0.0046,  0.0007,  0.0135],\n        ...,\n        [-0.0100,  0.0053, -0.0020,  ..., -0.0095,  0.0061,  0.0067],\n        [-0.0070,  0.0082, -0.0004,  ..., -0.0138, -0.0054,  0.0023],\n        [-0.0017, -0.0043, -0.0046,  ...,  0.0035, -0.0053, -0.0071]])\nmodel.layers.3.mlp.down_proj.lora_A.default.weight: tensor([[ 2.2507e-04, -7.6294e-04,  1.1740e-03,  ...,  3.4580e-03,\n          5.0831e-04,  5.9967e-03],\n        [ 5.2795e-03, -5.1498e-05,  6.5880e-03,  ..., -2.8801e-03,\n         -1.9760e-03,  8.4734e-04],\n        [-8.4686e-04,  2.8419e-04,  2.7103e-03,  ...,  1.5640e-04,\n          6.9695e-03, -6.3324e-03],\n        ...,\n        [-4.9057e-03,  1.3866e-03, -7.5912e-04,  ..., -2.6989e-04,\n         -3.9062e-03,  2.1553e-04],\n        [-6.4850e-05, -7.8392e-04,  3.4866e-03,  ..., -4.9667e-03,\n         -3.2940e-03, -4.7150e-03],\n        [-2.2373e-03, -2.7962e-03, -6.9809e-04,  ..., -1.5602e-03,\n         -3.3207e-03,  8.0967e-04]])\nmodel.layers.3.mlp.down_proj.lora_B.default.weight: tensor([[-1.1864e-03,  2.9874e-04,  5.9652e-04,  ...,  1.0672e-03,\n          1.0705e-04,  4.9591e-04],\n        [ 2.7847e-03, -1.1263e-03, -2.0027e-03,  ...,  1.2655e-03,\n          1.9741e-04, -2.4395e-03],\n        [-5.6362e-04,  1.0433e-03,  1.0653e-03,  ..., -7.6389e-04,\n         -1.6823e-03,  8.4400e-05],\n        ...,\n        [-5.5313e-05, -3.9673e-04, -5.1880e-04,  ..., -7.8106e-04,\n          1.2894e-03, -9.3460e-05],\n        [ 1.5869e-03,  2.8706e-04, -1.1368e-03,  ..., -2.6655e-04,\n          4.3774e-04,  8.1110e-04],\n        [ 6.5327e-04, -7.0333e-04, -1.5297e-03,  ...,  3.8362e-04,\n          1.7004e-03, -7.8154e-04]])\nmodel.layers.3.input_layernorm.weight: tensor([1.6016, 0.7969, 1.0391,  ..., 0.6016, 1.0000, 0.4082])\nmodel.layers.3.post_attention_layernorm.weight: tensor([2.8906, 1.6406, 2.6719,  ..., 1.8359, 2.7969, 1.4531])\nmodel.layers.4.self_attn.q_proj.base_layer.weight: tensor([[ 0.0033, -0.0127, -0.0189,  ..., -0.0167, -0.0070,  0.0048],\n        [-0.0008,  0.0089,  0.0020,  ..., -0.0038, -0.0035, -0.0144],\n        [ 0.0064,  0.0075, -0.0058,  ..., -0.0177, -0.0099, -0.0007],\n        ...,\n        [-0.0081,  0.0041, -0.0208,  ...,  0.0156, -0.0013, -0.0015],\n        [ 0.0104, -0.0122, -0.0041,  ...,  0.0006,  0.0074, -0.0022],\n        [ 0.0023, -0.0150,  0.0026,  ...,  0.0233, -0.0045, -0.0110]])\nmodel.layers.4.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0085,  0.0040,  0.0117,  ..., -0.0119,  0.0081, -0.0065],\n        [-0.0054, -0.0033, -0.0157,  ...,  0.0177,  0.0008,  0.0194],\n        [ 0.0072,  0.0097,  0.0005,  ..., -0.0012, -0.0093, -0.0007],\n        ...,\n        [-0.0094,  0.0013,  0.0026,  ...,  0.0029, -0.0038,  0.0083],\n        [ 0.0138,  0.0092,  0.0185,  ...,  0.0163,  0.0032,  0.0030],\n        [-0.0038, -0.0067, -0.0023,  ..., -0.0010,  0.0139, -0.0039]])\nmodel.layers.4.self_attn.q_proj.lora_B.default.weight: tensor([[-2.0409e-03,  4.3249e-04, -8.9741e-04,  ...,  3.9387e-04,\n         -9.0408e-04,  1.7242e-03],\n        [-1.8988e-03,  3.8624e-05, -1.0872e-03,  ...,  8.1968e-04,\n         -3.2663e-04,  1.7929e-03],\n        [-1.9703e-03,  1.5984e-03,  1.4973e-03,  ..., -3.5119e-04,\n         -2.1877e-03,  3.9768e-04],\n        ...,\n        [-5.4836e-04, -1.5974e-03, -4.3321e-04,  ...,  7.4482e-04,\n         -2.3031e-04, -6.3896e-04],\n        [-6.2275e-04, -1.8988e-03, -2.6894e-04,  ...,  1.2875e-04,\n          1.0891e-03,  1.9569e-03],\n        [-1.8110e-03,  4.2939e-04, -1.2398e-05,  ...,  3.2234e-04,\n         -7.7200e-04, -3.6097e-04]])\nmodel.layers.4.self_attn.k_proj.base_layer.weight: tensor([[ 0.0039,  0.0003,  0.0013,  ...,  0.0075,  0.0021, -0.0001],\n        [-0.0011, -0.0041, -0.0056,  ..., -0.0109, -0.0012,  0.0079],\n        [ 0.0045,  0.0074, -0.0015,  ..., -0.0008,  0.0001,  0.0036],\n        ...,\n        [-0.0001,  0.0008,  0.0013,  ..., -0.0016, -0.0068,  0.0097],\n        [ 0.0069,  0.0082, -0.0042,  ...,  0.0039, -0.0030,  0.0164],\n        [-0.0111,  0.0038,  0.0052,  ..., -0.0120,  0.0048,  0.0204]])\nmodel.layers.4.self_attn.k_proj.lora_A.default.weight: tensor([[-0.0036, -0.0032, -0.0082,  ...,  0.0167,  0.0090,  0.0120],\n        [-0.0083, -0.0025, -0.0094,  ...,  0.0062, -0.0111,  0.0088],\n        [-0.0171,  0.0144, -0.0004,  ..., -0.0043,  0.0032, -0.0125],\n        ...,\n        [ 0.0067,  0.0022,  0.0029,  ...,  0.0096,  0.0077,  0.0031],\n        [-0.0013, -0.0030, -0.0191,  ...,  0.0012, -0.0102, -0.0041],\n        [-0.0101, -0.0002,  0.0006,  ...,  0.0044, -0.0067, -0.0048]])\nmodel.layers.4.self_attn.k_proj.lora_B.default.weight: tensor([[-1.8911e-03,  3.2120e-03, -4.9400e-04,  ...,  1.9526e-04,\n          2.6855e-03,  2.5311e-03],\n        [-2.2736e-03,  1.9150e-03,  1.3638e-03,  ..., -3.0041e-05,\n          3.2768e-03,  1.6766e-03],\n        [ 2.1400e-03, -1.6804e-03, -1.2703e-03,  ..., -1.5364e-03,\n         -1.1158e-04,  2.7466e-04],\n        ...,\n        [ 1.9522e-03, -1.3371e-03,  2.3484e-04,  ...,  2.5892e-04,\n         -2.5826e-03,  1.1282e-03],\n        [ 1.5678e-03, -2.6436e-03,  1.1253e-03,  ...,  7.7105e-04,\n         -5.1594e-04, -2.2144e-03],\n        [ 1.7262e-03,  8.8072e-04, -6.3658e-05,  ...,  9.9087e-04,\n         -1.1444e-03,  1.0586e-04]])\nmodel.layers.4.self_attn.v_proj.base_layer.weight: tensor([[-0.0046, -0.0063,  0.0154,  ...,  0.0063, -0.0125,  0.0040],\n        [-0.0024,  0.0015, -0.0074,  ..., -0.0023,  0.0084, -0.0047],\n        [ 0.0118, -0.0065,  0.0028,  ..., -0.0154,  0.0019,  0.0131],\n        ...,\n        [-0.0062, -0.0016,  0.0040,  ..., -0.0044,  0.0043, -0.0037],\n        [ 0.0080,  0.0238, -0.0072,  ...,  0.0094, -0.0071, -0.0092],\n        [ 0.0060, -0.0074,  0.0090,  ...,  0.0121, -0.0070,  0.0171]])\nmodel.layers.4.self_attn.v_proj.lora_A.default.weight: tensor([[-1.0651e-02,  9.7198e-03,  7.0000e-03,  ...,  6.8970e-03,\n          5.0201e-03,  1.1589e-02],\n        [-1.4458e-02,  6.3095e-03, -1.7334e-02,  ..., -1.0620e-02,\n          1.2573e-02, -2.7199e-03],\n        [ 3.3264e-03,  7.2250e-03, -4.5013e-04,  ..., -1.1749e-02,\n         -1.5266e-02, -5.3444e-03],\n        ...,\n        [-5.4054e-03,  3.1700e-03,  1.1459e-02,  ..., -8.5068e-03,\n          3.6964e-03,  9.1553e-05],\n        [-4.4556e-03, -7.5607e-03, -5.7220e-04,  ..., -1.1063e-02,\n          6.3591e-03, -1.4244e-02],\n        [ 4.4212e-03, -5.6458e-03,  8.0967e-04,  ...,  1.4931e-02,\n         -1.1536e-02,  4.5776e-05]])\nmodel.layers.4.self_attn.v_proj.lora_B.default.weight: tensor([[-7.0572e-04,  6.1989e-05, -9.2840e-04,  ..., -4.1485e-04,\n         -1.1206e-04, -1.9002e-04],\n        [-4.4060e-04, -1.3590e-04,  3.1948e-04,  ...,  1.1921e-03,\n          2.1458e-05, -1.1034e-03],\n        [-5.4836e-04,  7.1526e-07,  1.1978e-03,  ..., -1.3180e-03,\n         -6.5136e-04,  1.3409e-03],\n        ...,\n        [ 8.1635e-04, -6.5517e-04,  5.5075e-05,  ...,  4.7922e-04,\n         -1.2589e-03,  1.4057e-03],\n        [-2.9778e-04,  2.2049e-03, -9.9850e-04,  ..., -1.6003e-03,\n         -1.9455e-03, -6.8665e-05],\n        [-1.5211e-03, -1.9550e-03,  1.6441e-03,  ..., -2.3484e-04,\n         -1.1444e-05, -2.1935e-03]])\nmodel.layers.4.self_attn.o_proj.base_layer.weight: tensor([[ 0.0225,  0.0120,  0.0002,  ..., -0.0210,  0.0029,  0.0090],\n        [ 0.0135, -0.0117, -0.0054,  ...,  0.0061, -0.0031,  0.0025],\n        [ 0.0170, -0.0076,  0.0010,  ...,  0.0012, -0.0066, -0.0025],\n        ...,\n        [-0.0009,  0.0082, -0.0170,  ..., -0.0051, -0.0104,  0.0021],\n        [ 0.0055, -0.0079,  0.0031,  ...,  0.0090,  0.0061, -0.0040],\n        [-0.0017,  0.0060,  0.0041,  ...,  0.0007,  0.0189, -0.0061]])\nmodel.layers.4.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0020,  0.0133,  0.0137,  ...,  0.0014,  0.0096,  0.0038],\n        [ 0.0095, -0.0008,  0.0074,  ..., -0.0016, -0.0211, -0.0071],\n        [-0.0134, -0.0135,  0.0050,  ...,  0.0016, -0.0052,  0.0081],\n        ...,\n        [ 0.0113, -0.0037,  0.0084,  ...,  0.0016,  0.0133,  0.0005],\n        [ 0.0128,  0.0071,  0.0047,  ..., -0.0091,  0.0045, -0.0114],\n        [-0.0093,  0.0051, -0.0148,  ...,  0.0049,  0.0039,  0.0114]])\nmodel.layers.4.self_attn.o_proj.lora_B.default.weight: tensor([[-6.6948e-04, -1.3161e-04,  9.2268e-05,  ..., -5.4216e-04,\n         -9.7561e-04,  8.9455e-04],\n        [ 7.5340e-04,  2.3842e-06,  1.4079e-04,  ..., -4.6730e-04,\n          1.1921e-04, -2.2411e-04],\n        [-5.8985e-04,  5.0306e-04, -1.5478e-03,  ..., -1.2760e-03,\n         -1.6499e-04,  2.2221e-04],\n        ...,\n        [ 1.0710e-03, -7.6580e-04, -3.6335e-04,  ..., -1.0862e-03,\n         -1.6527e-03,  7.0286e-04],\n        [-2.8181e-04,  1.1463e-03,  2.1458e-05,  ..., -9.4318e-04,\n         -1.5860e-03,  9.1124e-04],\n        [ 6.1607e-04, -9.1839e-04,  1.2655e-03,  ...,  8.7619e-05,\n         -1.2922e-04,  9.3842e-04]])\nmodel.layers.4.mlp.gate_proj.base_layer.weight: tensor([[ 6.7139e-04,  3.9864e-04, -2.2888e-03,  ..., -7.9346e-03,\n         -7.1716e-04, -6.8970e-03],\n        [ 8.0109e-04, -4.5471e-03, -6.1951e-03,  ...,  5.1260e-05,\n          9.4414e-05, -5.4932e-03],\n        [-8.4229e-03, -9.3994e-03,  6.1951e-03,  ...,  3.2654e-03,\n          5.1575e-03,  2.2736e-03],\n        ...,\n        [-5.2185e-03, -1.2268e-02, -6.9275e-03,  ..., -9.9945e-04,\n          2.0447e-03,  7.2632e-03],\n        [-4.5776e-03,  1.6556e-03,  8.4839e-03,  ..., -1.9150e-03,\n          9.2773e-03, -1.2024e-02],\n        [-6.5918e-03,  9.4604e-03,  1.5488e-03,  ...,  6.7749e-03,\n         -5.3101e-03, -5.0964e-03]])\nmodel.layers.4.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0047, -0.0058, -0.0040,  ..., -0.0144,  0.0010,  0.0016],\n        [-0.0195,  0.0054, -0.0116,  ..., -0.0033, -0.0009, -0.0053],\n        [-0.0131, -0.0086, -0.0002,  ..., -0.0117,  0.0022, -0.0092],\n        ...,\n        [ 0.0025, -0.0033, -0.0165,  ..., -0.0044,  0.0075, -0.0025],\n        [ 0.0020, -0.0094, -0.0130,  ...,  0.0164, -0.0155,  0.0111],\n        [-0.0078,  0.0117,  0.0024,  ...,  0.0153, -0.0083,  0.0126]])\nmodel.layers.4.mlp.gate_proj.lora_B.default.weight: tensor([[-2.3518e-03, -1.4186e-04,  2.9707e-04,  ..., -3.0518e-05,\n          1.0567e-03,  5.7554e-04],\n        [-1.4048e-03, -1.5831e-03, -7.8201e-04,  ..., -2.7370e-04,\n          1.4982e-03, -3.9768e-04],\n        [ 9.3555e-04, -2.8682e-04,  3.1233e-05,  ..., -1.0300e-04,\n          1.5717e-03,  8.3876e-04],\n        ...,\n        [ 1.7834e-04, -3.4714e-04, -3.0446e-04,  ..., -5.7411e-04,\n         -4.2486e-04, -1.3962e-03],\n        [-1.4935e-03, -2.9945e-03,  9.4414e-04,  ..., -3.2544e-04,\n         -2.9030e-03, -1.0872e-03],\n        [-7.3099e-04,  1.7977e-03,  1.2112e-03,  ...,  1.1387e-03,\n          7.9823e-04,  5.6553e-04]])\nmodel.layers.4.mlp.up_proj.base_layer.weight: tensor([[-0.0070,  0.0030, -0.0020,  ..., -0.0062, -0.0042, -0.0084],\n        [ 0.0029,  0.0007,  0.0023,  ..., -0.0012,  0.0053,  0.0015],\n        [-0.0026, -0.0122,  0.0173,  ...,  0.0010, -0.0143, -0.0061],\n        ...,\n        [-0.0025, -0.0056, -0.0004,  ..., -0.0060, -0.0027,  0.0107],\n        [ 0.0042, -0.0004, -0.0055,  ..., -0.0019,  0.0029, -0.0073],\n        [-0.0115,  0.0112, -0.0007,  ..., -0.0106, -0.0022, -0.0058]])\nmodel.layers.4.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0042, -0.0068,  0.0014,  ...,  0.0020,  0.0197, -0.0127],\n        [ 0.0064, -0.0135, -0.0127,  ...,  0.0032,  0.0022,  0.0027],\n        [-0.0045,  0.0052, -0.0074,  ..., -0.0060,  0.0024, -0.0167],\n        ...,\n        [-0.0027,  0.0020, -0.0099,  ...,  0.0056,  0.0101,  0.0111],\n        [ 0.0047,  0.0017, -0.0001,  ..., -0.0198,  0.0047, -0.0059],\n        [ 0.0168,  0.0057,  0.0036,  ...,  0.0029, -0.0114,  0.0136]])\nmodel.layers.4.mlp.up_proj.lora_B.default.weight: tensor([[ 1.5354e-04, -3.3641e-04, -1.3084e-03,  ..., -1.4048e-03,\n         -2.5826e-03, -1.7166e-03],\n        [-3.3045e-04,  3.0231e-04,  2.7351e-03,  ...,  1.7519e-03,\n          2.5139e-03,  8.6164e-04],\n        [-1.1597e-03, -1.1263e-03,  3.9148e-04,  ...,  3.9387e-04,\n          1.2789e-03, -2.3708e-03],\n        ...,\n        [-1.1711e-03,  3.3379e-05, -2.6560e-04,  ..., -8.6546e-04,\n          4.1151e-04, -1.2627e-03],\n        [ 7.8201e-04, -1.5545e-04,  9.9277e-04,  ...,  1.6785e-03,\n         -2.4843e-04,  1.4162e-04],\n        [ 2.3479e-03, -8.9693e-04,  3.9005e-04,  ..., -1.5616e-04,\n         -1.7710e-03,  8.8024e-04]])\nmodel.layers.4.mlp.down_proj.base_layer.weight: tensor([[ 0.0014,  0.0066, -0.0041,  ..., -0.0014,  0.0006, -0.0107],\n        [-0.0031,  0.0009, -0.0009,  ..., -0.0103, -0.0040,  0.0112],\n        [ 0.0028, -0.0004,  0.0111,  ..., -0.0009,  0.0022,  0.0064],\n        ...,\n        [ 0.0073,  0.0001,  0.0017,  ..., -0.0004, -0.0034, -0.0101],\n        [ 0.0035,  0.0110, -0.0090,  ..., -0.0022,  0.0024, -0.0019],\n        [ 0.0100,  0.0070, -0.0033,  ..., -0.0045, -0.0074, -0.0002]])\nmodel.layers.4.mlp.down_proj.lora_A.default.weight: tensor([[-0.0026,  0.0023, -0.0033,  ..., -0.0004,  0.0032, -0.0033],\n        [ 0.0026, -0.0056,  0.0064,  ..., -0.0006,  0.0003,  0.0051],\n        [-0.0007,  0.0079,  0.0015,  ...,  0.0068,  0.0025,  0.0064],\n        ...,\n        [-0.0005,  0.0036, -0.0067,  ...,  0.0004, -0.0010, -0.0006],\n        [ 0.0046, -0.0011,  0.0037,  ..., -0.0064,  0.0017, -0.0016],\n        [-0.0005,  0.0010, -0.0031,  ...,  0.0009, -0.0022, -0.0023]])\nmodel.layers.4.mlp.down_proj.lora_B.default.weight: tensor([[-1.1206e-03, -1.5669e-03, -4.7946e-04,  ...,  1.3475e-03,\n          5.4359e-04, -5.7650e-04],\n        [-2.3460e-03, -1.4400e-03,  1.7166e-05,  ...,  1.1406e-03,\n         -1.0757e-03, -1.1339e-03],\n        [-3.8862e-04,  1.9779e-03, -3.6478e-04,  ...,  1.3657e-03,\n          3.9577e-04,  1.0471e-03],\n        ...,\n        [-2.8305e-03,  1.2064e-04,  2.3117e-03,  ..., -3.1972e-04,\n          9.5558e-04, -3.1891e-03],\n        [ 2.5654e-04,  2.3842e-06,  1.3428e-03,  ..., -2.2292e-04,\n         -4.2534e-04, -5.6744e-04],\n        [ 6.9427e-04,  9.0694e-04,  2.4548e-03,  ..., -7.3624e-04,\n          1.1978e-03,  8.1968e-04]])\nmodel.layers.4.input_layernorm.weight: tensor([1.1797, 0.9102, 1.0156,  ..., 0.6562, 1.4531, 0.3574])\nmodel.layers.4.post_attention_layernorm.weight: tensor([2.8125, 1.4844, 2.6719,  ..., 1.7500, 3.0469, 1.2578])\nmodel.layers.5.self_attn.q_proj.base_layer.weight: tensor([[-6.1035e-03,  2.3041e-03,  8.9722e-03,  ..., -3.7231e-03,\n         -1.3428e-03,  1.2390e-02],\n        [-2.3315e-02,  5.9204e-03, -3.9673e-03,  ...,  7.7438e-04,\n          6.6528e-03,  1.2390e-02],\n        [-5.9509e-03, -8.6594e-04, -7.5684e-03,  ..., -1.6479e-02,\n          4.7607e-03, -8.9111e-03],\n        ...,\n        [-5.6458e-03, -2.0218e-04,  1.6251e-03,  ...,  9.6512e-04,\n         -1.6937e-03,  3.0823e-03],\n        [ 1.5564e-03, -2.5024e-03, -4.1962e-05,  ...,  9.3842e-04,\n         -1.0300e-03,  3.2043e-03],\n        [-5.4932e-03, -1.1780e-02,  4.6082e-03,  ..., -4.5471e-03,\n          8.5831e-04,  1.9379e-03]])\nmodel.layers.5.self_attn.q_proj.lora_A.default.weight: tensor([[-0.0013, -0.0054, -0.0188,  ..., -0.0002, -0.0029, -0.0047],\n        [-0.0043,  0.0025,  0.0130,  ...,  0.0053,  0.0154, -0.0004],\n        [ 0.0179,  0.0010, -0.0024,  ..., -0.0039,  0.0069,  0.0075],\n        ...,\n        [ 0.0034, -0.0078,  0.0219,  ..., -0.0064, -0.0151,  0.0057],\n        [-0.0071,  0.0059,  0.0076,  ..., -0.0109,  0.0047, -0.0112],\n        [-0.0130, -0.0015,  0.0024,  ..., -0.0020,  0.0048, -0.0098]])\nmodel.layers.5.self_attn.q_proj.lora_B.default.weight: tensor([[-1.1787e-03, -5.5695e-04,  5.2357e-04,  ...,  1.4763e-03,\n         -2.2774e-03,  3.8624e-04],\n        [-6.7091e-04, -2.2793e-04,  3.4237e-04,  ...,  8.2397e-04,\n          2.4033e-04,  7.0667e-04],\n        [-1.3065e-03, -4.1986e-04, -1.5221e-03,  ...,  1.0691e-03,\n         -1.8239e-04,  6.6996e-04],\n        ...,\n        [ 3.5763e-05, -8.7738e-04, -1.2913e-03,  ..., -9.0456e-04,\n          5.6982e-04, -1.7395e-03],\n        [-5.3406e-04,  6.2990e-04, -2.1057e-03,  ...,  1.0910e-03,\n         -1.3170e-03,  8.5783e-04],\n        [ 5.9128e-05,  3.1161e-04,  2.5368e-03,  ..., -2.2373e-03,\n          8.4591e-04, -7.0286e-04]])\nmodel.layers.5.self_attn.k_proj.base_layer.weight: tensor([[-0.0010,  0.0074,  0.0034,  ..., -0.0073, -0.0049,  0.0042],\n        [ 0.0085, -0.0056, -0.0063,  ..., -0.0045, -0.0044,  0.0050],\n        [ 0.0078, -0.0035, -0.0053,  ..., -0.0014,  0.0035, -0.0003],\n        ...,\n        [ 0.0112,  0.0123, -0.0254,  ...,  0.0104,  0.0028, -0.0255],\n        [-0.0211, -0.0101, -0.0143,  ...,  0.0223,  0.0103,  0.0228],\n        [ 0.0054,  0.0079,  0.0232,  ..., -0.0096, -0.0029,  0.0223]])\nmodel.layers.5.self_attn.k_proj.lora_A.default.weight: tensor([[-0.0111,  0.0085,  0.0058,  ..., -0.0053, -0.0073, -0.0014],\n        [-0.0137,  0.0004, -0.0020,  ...,  0.0016, -0.0049, -0.0015],\n        [ 0.0007, -0.0157, -0.0023,  ..., -0.0143,  0.0152, -0.0044],\n        ...,\n        [-0.0093, -0.0070, -0.0065,  ...,  0.0112,  0.0136,  0.0021],\n        [ 0.0079, -0.0070,  0.0009,  ...,  0.0108,  0.0063,  0.0069],\n        [-0.0120,  0.0073, -0.0018,  ...,  0.0069, -0.0064,  0.0130]])\nmodel.layers.5.self_attn.k_proj.lora_B.default.weight: tensor([[ 1.6823e-03, -6.1417e-04, -1.1182e-04,  ..., -2.0008e-03,\n         -1.8053e-03,  4.0221e-04],\n        [ 8.6594e-04,  1.2150e-03, -7.5769e-04,  ..., -3.5524e-04,\n          8.1539e-04, -2.8896e-04],\n        [ 4.1866e-04,  1.3323e-03, -1.6093e-04,  ...,  3.4189e-04,\n          4.0221e-04, -2.0256e-03],\n        ...,\n        [-6.3133e-04, -7.1430e-04,  1.3199e-03,  ...,  1.0653e-03,\n          4.0102e-04,  6.4278e-04],\n        [ 1.9979e-04, -8.2493e-04,  1.3428e-03,  ..., -5.1546e-04,\n         -6.1989e-06,  1.1110e-03],\n        [ 5.1880e-04,  2.9278e-04, -3.5286e-04,  ...,  2.5749e-05,\n          1.0691e-03, -2.6474e-03]])\nmodel.layers.5.self_attn.v_proj.base_layer.weight: tensor([[-0.0030, -0.0059,  0.0059,  ...,  0.0006,  0.0031,  0.0038],\n        [ 0.0023, -0.0014, -0.0071,  ...,  0.0058,  0.0089,  0.0004],\n        [-0.0101,  0.0033,  0.0036,  ..., -0.0052,  0.0016, -0.0018],\n        ...,\n        [-0.0058,  0.0119, -0.0031,  ...,  0.0006,  0.0036, -0.0018],\n        [-0.0089,  0.0015,  0.0038,  ..., -0.0127, -0.0073,  0.0092],\n        [ 0.0111,  0.0013,  0.0024,  ...,  0.0048, -0.0050, -0.0027]])\nmodel.layers.5.self_attn.v_proj.lora_A.default.weight: tensor([[ 0.0027,  0.0027,  0.0005,  ...,  0.0038, -0.0158,  0.0037],\n        [-0.0064,  0.0031,  0.0121,  ..., -0.0182,  0.0076, -0.0162],\n        [ 0.0107, -0.0104, -0.0029,  ...,  0.0002, -0.0091, -0.0008],\n        ...,\n        [-0.0007, -0.0047, -0.0042,  ..., -0.0135, -0.0194,  0.0004],\n        [-0.0154,  0.0092, -0.0130,  ...,  0.0120, -0.0053, -0.0014],\n        [ 0.0065, -0.0137,  0.0170,  ...,  0.0020, -0.0048,  0.0111]])\nmodel.layers.5.self_attn.v_proj.lora_B.default.weight: tensor([[ 7.8678e-04, -1.0548e-03,  1.5793e-03,  ..., -4.7922e-04,\n          1.3666e-03,  1.8749e-03],\n        [-1.0366e-03, -1.4534e-03,  7.7772e-04,  ...,  4.9686e-04,\n          1.1864e-03,  4.3559e-04],\n        [-2.4452e-03,  2.2945e-03, -1.4353e-03,  ..., -1.9360e-04,\n         -1.2875e-03, -1.1902e-03],\n        ...,\n        [ 1.7071e-03, -4.6349e-04,  1.0281e-03,  ..., -2.7204e-04,\n          6.2180e-04, -7.9870e-05],\n        [-1.4868e-03,  1.7948e-03, -4.3082e-04,  ...,  3.8195e-04,\n         -1.4925e-03, -7.9155e-04],\n        [ 6.4993e-04, -1.5602e-03,  1.0843e-03,  ...,  4.6635e-04,\n          3.5429e-04,  1.1225e-03]])\nmodel.layers.5.self_attn.o_proj.base_layer.weight: tensor([[ 0.0101, -0.0103,  0.0139,  ...,  0.0194,  0.0070,  0.0060],\n        [ 0.0049,  0.0056, -0.0082,  ...,  0.0019,  0.0013,  0.0056],\n        [-0.0037,  0.0023, -0.0097,  ..., -0.0054, -0.0008, -0.0006],\n        ...,\n        [ 0.0010, -0.0150,  0.0092,  ..., -0.0005, -0.0142,  0.0063],\n        [-0.0016, -0.0006, -0.0038,  ...,  0.0021, -0.0041, -0.0178],\n        [-0.0091,  0.0026,  0.0051,  ...,  0.0063, -0.0075, -0.0050]])\nmodel.layers.5.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0185, -0.0172, -0.0069,  ..., -0.0024, -0.0166, -0.0011],\n        [ 0.0018,  0.0162,  0.0068,  ..., -0.0161, -0.0102, -0.0142],\n        [ 0.0083, -0.0103,  0.0017,  ..., -0.0087, -0.0118, -0.0092],\n        ...,\n        [ 0.0190,  0.0013,  0.0109,  ..., -0.0064, -0.0034, -0.0047],\n        [-0.0174,  0.0015, -0.0148,  ...,  0.0022,  0.0088,  0.0067],\n        [-0.0040, -0.0047, -0.0037,  ..., -0.0150,  0.0086, -0.0117]])\nmodel.layers.5.self_attn.o_proj.lora_B.default.weight: tensor([[ 1.6165e-03,  1.2827e-03, -9.5367e-07,  ...,  5.9128e-04,\n          1.0624e-03,  3.5071e-04],\n        [ 1.7500e-04, -1.4222e-04,  6.1846e-04,  ..., -8.1825e-04,\n         -1.9813e-04, -5.2989e-05],\n        [ 1.2770e-03,  5.9748e-04, -1.6613e-03,  ..., -1.2436e-03,\n          2.2812e-03,  1.5373e-03],\n        ...,\n        [-2.7323e-04, -2.9635e-04, -2.0289e-04,  ...,  5.1880e-04,\n         -7.9775e-04, -5.1880e-04],\n        [-7.7391e-04, -9.3651e-04,  1.0204e-03,  ...,  5.8794e-04,\n         -1.5984e-03, -5.5790e-04],\n        [ 1.0471e-03,  8.5831e-06, -2.3842e-05,  ...,  8.5735e-04,\n         -9.7179e-04,  6.4182e-04]])\nmodel.layers.5.mlp.gate_proj.base_layer.weight: tensor([[ 7.2327e-03,  4.4250e-03,  2.6703e-03,  ...,  5.7068e-03,\n         -6.5918e-03,  2.8381e-03],\n        [ 6.5308e-03,  1.0620e-02, -7.5912e-04,  ..., -1.0910e-03,\n         -2.4109e-03,  7.1106e-03],\n        [ 3.7079e-03,  7.2479e-04, -6.6528e-03,  ..., -8.0566e-03,\n         -4.9744e-03,  4.1504e-03],\n        ...,\n        [-5.4626e-03,  6.6223e-03, -4.6387e-03,  ..., -1.0729e-04,\n         -2.6398e-03,  4.3640e-03],\n        [ 1.2398e-05,  5.0049e-03, -7.5073e-03,  ..., -6.8359e-03,\n         -1.7395e-03,  3.6163e-03],\n        [ 6.6757e-05, -1.4801e-03, -8.3008e-03,  ...,  8.1635e-04,\n         -5.8899e-03,  1.3504e-03]])\nmodel.layers.5.mlp.gate_proj.lora_A.default.weight: tensor([[ 0.0034,  0.0018,  0.0068,  ...,  0.0003, -0.0013, -0.0092],\n        [-0.0064, -0.0002, -0.0026,  ...,  0.0042,  0.0036, -0.0039],\n        [ 0.0048, -0.0151,  0.0148,  ...,  0.0070, -0.0055,  0.0103],\n        ...,\n        [ 0.0051,  0.0141, -0.0044,  ..., -0.0020, -0.0052,  0.0054],\n        [-0.0116, -0.0020, -0.0136,  ...,  0.0045, -0.0073, -0.0033],\n        [-0.0145,  0.0081, -0.0207,  ..., -0.0041,  0.0037, -0.0134]])\nmodel.layers.5.mlp.gate_proj.lora_B.default.weight: tensor([[-0.0005, -0.0007, -0.0009,  ..., -0.0006, -0.0002, -0.0003],\n        [ 0.0004,  0.0001, -0.0031,  ...,  0.0017, -0.0024, -0.0002],\n        [ 0.0003, -0.0005, -0.0009,  ...,  0.0008,  0.0015,  0.0014],\n        ...,\n        [ 0.0011,  0.0004,  0.0027,  ..., -0.0017,  0.0009,  0.0018],\n        [-0.0013,  0.0004, -0.0010,  ...,  0.0004,  0.0005, -0.0008],\n        [ 0.0008, -0.0005, -0.0010,  ...,  0.0006,  0.0027,  0.0015]])\nmodel.layers.5.mlp.up_proj.base_layer.weight: tensor([[ 2.4109e-03, -9.5215e-03, -5.0964e-03,  ..., -3.0975e-03,\n         -1.0132e-02,  8.3618e-03],\n        [ 7.3853e-03,  6.1035e-03,  5.4626e-03,  ...,  4.1504e-03,\n         -1.4343e-03, -1.8005e-03],\n        [-1.7090e-03, -6.4697e-03,  6.1951e-03,  ..., -7.4005e-04,\n          9.2773e-03, -9.2163e-03],\n        ...,\n        [-5.5542e-03, -7.5684e-03, -8.3618e-03,  ...,  4.9210e-04,\n         -6.3171e-03,  7.9956e-03],\n        [ 1.8539e-03, -6.9141e-05,  4.0588e-03,  ...,  9.9487e-03,\n         -1.4801e-03,  3.6469e-03],\n        [ 5.8889e-05, -1.2085e-02, -3.8910e-03,  ...,  4.4556e-03,\n         -9.0942e-03, -3.6163e-03]])\nmodel.layers.5.mlp.up_proj.lora_A.default.weight: tensor([[-0.0068, -0.0004, -0.0104,  ..., -0.0070, -0.0024, -0.0019],\n        [-0.0066,  0.0171, -0.0100,  ..., -0.0137,  0.0085, -0.0047],\n        [ 0.0117, -0.0046,  0.0168,  ...,  0.0021, -0.0080, -0.0066],\n        ...,\n        [ 0.0039,  0.0056,  0.0196,  ...,  0.0076, -0.0013, -0.0017],\n        [-0.0002, -0.0175, -0.0016,  ..., -0.0139,  0.0137, -0.0119],\n        [ 0.0114, -0.0115, -0.0010,  ...,  0.0100,  0.0115,  0.0028]])\nmodel.layers.5.mlp.up_proj.lora_B.default.weight: tensor([[ 4.3201e-04,  1.9312e-04, -1.0824e-03,  ...,  5.1594e-04,\n          5.3978e-04,  7.5436e-04],\n        [ 8.9407e-04,  3.0842e-03,  1.6899e-03,  ..., -7.4768e-04,\n         -1.1635e-04,  1.7452e-04],\n        [ 6.1989e-04,  7.3433e-04,  1.5316e-03,  ...,  2.1210e-03,\n         -7.0810e-05,  1.4963e-03],\n        ...,\n        [-9.9754e-04, -2.7275e-04, -8.9264e-04,  ..., -1.4877e-04,\n         -5.3406e-04,  2.5558e-04],\n        [-1.5430e-03,  8.3208e-04, -4.8518e-04,  ..., -4.8280e-05,\n          1.0176e-03, -1.0729e-05],\n        [-5.2357e-04,  1.3752e-03,  5.5170e-04,  ..., -2.4319e-05,\n          7.5817e-04,  3.4714e-04]])\nmodel.layers.5.mlp.down_proj.base_layer.weight: tensor([[-1.9150e-03,  1.9360e-04,  3.3264e-03,  ...,  5.1575e-03,\n         -6.7444e-03, -1.0133e-05],\n        [-3.0365e-03,  2.5787e-03, -7.6599e-03,  ...,  2.7161e-03,\n          4.5166e-03, -7.5073e-03],\n        [-1.0986e-02,  8.6060e-03,  3.1891e-03,  ..., -2.1820e-03,\n          1.7166e-03,  8.3008e-03],\n        ...,\n        [ 9.7275e-04,  2.7161e-03,  1.0193e-02,  ...,  3.4943e-03,\n          7.8201e-05, -3.6774e-03],\n        [-1.2451e-02,  5.2261e-04,  3.6621e-04,  ..., -4.7913e-03,\n         -2.3193e-03, -4.8218e-03],\n        [ 9.0942e-03,  3.2654e-03,  6.8054e-03,  ...,  1.5137e-02,\n          7.0801e-03, -8.7280e-03]])\nmodel.layers.5.mlp.down_proj.lora_A.default.weight: tensor([[-0.0015,  0.0016,  0.0033,  ..., -0.0020,  0.0066, -0.0018],\n        [ 0.0070, -0.0028,  0.0054,  ..., -0.0021, -0.0018,  0.0022],\n        [-0.0005, -0.0019,  0.0011,  ...,  0.0043,  0.0011,  0.0008],\n        ...,\n        [-0.0048, -0.0021, -0.0050,  ..., -0.0017, -0.0009, -0.0089],\n        [-0.0045,  0.0003, -0.0022,  ...,  0.0021,  0.0001, -0.0065],\n        [-0.0041, -0.0056, -0.0015,  ..., -0.0007, -0.0002,  0.0014]])\nmodel.layers.5.mlp.down_proj.lora_B.default.weight: tensor([[-1.4105e-03,  5.9652e-04, -1.7490e-03,  ..., -1.1730e-03,\n         -2.1248e-03, -3.6860e-04],\n        [ 6.1035e-04, -6.0225e-04,  9.8228e-04,  ...,  2.4509e-04,\n          1.1911e-03, -2.5272e-05],\n        [-4.1246e-05,  1.2789e-03,  2.7275e-04,  ...,  2.2430e-03,\n          2.2526e-03,  1.2589e-03],\n        ...,\n        [-1.5230e-03,  2.5291e-03, -1.4629e-03,  ..., -7.6056e-04,\n         -8.7833e-04, -1.4706e-03],\n        [ 8.9264e-04,  8.8215e-05,  5.1260e-04,  ...,  5.2929e-04,\n         -3.4714e-04,  4.2868e-04],\n        [ 2.0409e-03, -1.8854e-03, -6.7854e-04,  ..., -1.9455e-03,\n         -1.9245e-03, -1.8129e-03]])\nmodel.layers.5.input_layernorm.weight: tensor([1.6172, 0.9258, 1.4531,  ..., 0.7109, 1.7500, 0.3789])\nmodel.layers.5.post_attention_layernorm.weight: tensor([3.0312, 1.3594, 3.0938,  ..., 1.8281, 3.4844, 1.3438])\nmodel.layers.6.self_attn.q_proj.base_layer.weight: tensor([[ 5.5237e-03, -2.9907e-03,  9.1553e-04,  ...,  3.5553e-03,\n          8.0585e-05,  3.0060e-03],\n        [-1.3657e-03,  1.1597e-03,  3.1433e-03,  ...,  1.3306e-02,\n          3.1281e-03,  1.6113e-02],\n        [ 8.4686e-04, -2.8839e-03,  6.5613e-03,  ..., -2.8534e-03,\n         -2.6093e-03,  2.5024e-03],\n        ...,\n        [-6.5918e-03,  5.2002e-02,  2.8229e-03,  ..., -3.0884e-02,\n         -1.1230e-02,  1.0437e-02],\n        [-1.1230e-02, -1.2283e-03,  5.7068e-03,  ..., -2.7466e-02,\n         -8.3008e-03, -1.4404e-02],\n        [ 5.5542e-03, -3.5645e-02, -1.5747e-02,  ..., -1.4267e-03,\n         -6.3782e-03,  7.2327e-03]])\nmodel.layers.6.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0177,  0.0107, -0.0041,  ..., -0.0018, -0.0176, -0.0145],\n        [-0.0091, -0.0058,  0.0099,  ..., -0.0130,  0.0011, -0.0050],\n        [ 0.0025,  0.0087, -0.0105,  ...,  0.0056, -0.0123, -0.0015],\n        ...,\n        [-0.0051,  0.0011, -0.0073,  ...,  0.0077, -0.0059,  0.0123],\n        [-0.0226,  0.0149, -0.0067,  ...,  0.0073,  0.0079,  0.0009],\n        [ 0.0099, -0.0039,  0.0017,  ..., -0.0071,  0.0052, -0.0097]])\nmodel.layers.6.self_attn.q_proj.lora_B.default.weight: tensor([[ 1.9026e-03,  7.7057e-04, -2.4319e-04,  ...,  7.7343e-04,\n         -2.2812e-03,  8.1444e-04],\n        [ 6.0654e-04, -3.1567e-04, -2.3956e-03,  ..., -1.0061e-03,\n         -1.7967e-03,  2.6169e-03],\n        [-1.2093e-03, -2.4471e-03,  1.5676e-05,  ..., -2.2888e-03,\n          1.1292e-03, -2.1000e-03],\n        ...,\n        [-1.1559e-03,  9.3985e-04, -2.4490e-03,  ...,  1.4725e-03,\n          9.6321e-04, -2.5678e-04],\n        [-1.5993e-03,  1.3947e-04, -4.8661e-04,  ...,  9.2602e-04,\n         -1.1148e-03, -1.1482e-03],\n        [-1.3351e-05,  4.8637e-05,  7.1239e-04,  ..., -4.4346e-04,\n         -1.4877e-03, -1.1349e-03]])\nmodel.layers.6.self_attn.k_proj.base_layer.weight: tensor([[ 2.8534e-03, -1.4282e-02,  2.3499e-03,  ...,  9.8267e-03,\n          3.4027e-03,  7.0953e-04],\n        [ 1.2970e-03, -7.0190e-03,  2.9144e-03,  ...,  7.0095e-05,\n          3.2959e-03, -8.8120e-04],\n        [-1.9789e-05,  2.5787e-03, -2.5940e-04,  ...,  3.6011e-03,\n         -1.3638e-04, -6.5918e-03],\n        ...,\n        [ 1.1292e-02,  2.4902e-02,  8.1177e-03,  ..., -1.5747e-02,\n         -6.2561e-03,  6.9885e-03],\n        [-9.7656e-03, -1.9775e-02, -1.7700e-02,  ..., -1.0010e-02,\n          3.7384e-03, -1.2817e-02],\n        [ 9.1553e-03,  4.8218e-03,  4.1962e-05,  ..., -7.5684e-03,\n          5.4932e-03,  8.9111e-03]])\nmodel.layers.6.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0102, -0.0037,  0.0078,  ...,  0.0049,  0.0072,  0.0011],\n        [ 0.0090,  0.0057,  0.0028,  ...,  0.0143, -0.0019,  0.0102],\n        [ 0.0128, -0.0087,  0.0182,  ..., -0.0010, -0.0075,  0.0017],\n        ...,\n        [ 0.0052,  0.0175,  0.0025,  ..., -0.0047, -0.0054, -0.0075],\n        [ 0.0048, -0.0176,  0.0164,  ...,  0.0015,  0.0101, -0.0012],\n        [ 0.0059,  0.0008,  0.0017,  ..., -0.0025,  0.0089, -0.0027]])\nmodel.layers.6.self_attn.k_proj.lora_B.default.weight: tensor([[-3.3379e-03,  5.8413e-04,  1.2684e-03,  ..., -2.3746e-04,\n         -2.5826e-03,  1.7691e-03],\n        [ 1.2608e-03, -2.0361e-04, -5.9128e-05,  ...,  8.2111e-04,\n         -6.7282e-04, -1.4410e-03],\n        [-2.1667e-03, -7.5912e-04,  3.2520e-04,  ...,  1.5163e-04,\n         -5.0068e-06,  1.4019e-04],\n        ...,\n        [ 1.5163e-04,  2.3232e-03,  1.2627e-03,  ..., -9.6416e-04,\n          1.4067e-05,  1.8096e-04],\n        [ 3.1948e-05,  2.0027e-05, -8.8406e-04,  ...,  7.7295e-04,\n         -1.5182e-03, -8.7881e-04],\n        [ 6.1989e-05, -2.7347e-04, -2.5845e-04,  ...,  1.1435e-03,\n         -6.5565e-04,  1.7414e-03]])\nmodel.layers.6.self_attn.v_proj.base_layer.weight: tensor([[ 0.0021, -0.0007, -0.0016,  ..., -0.0074, -0.0031,  0.0008],\n        [ 0.0091,  0.0016, -0.0070,  ...,  0.0012,  0.0053,  0.0031],\n        [-0.0141, -0.0069,  0.0049,  ...,  0.0037,  0.0021, -0.0101],\n        ...,\n        [ 0.0034,  0.0079, -0.0095,  ...,  0.0216, -0.0186, -0.0058],\n        [-0.0084,  0.0037,  0.0009,  ...,  0.0073,  0.0016,  0.0114],\n        [-0.0117, -0.0033,  0.0075,  ..., -0.0125,  0.0148, -0.0103]])\nmodel.layers.6.self_attn.v_proj.lora_A.default.weight: tensor([[-6.7139e-04, -7.2021e-03,  3.1776e-03,  ..., -9.1782e-03,\n         -6.7329e-03, -9.6741e-03],\n        [ 7.4005e-03, -2.4929e-03,  1.5762e-02,  ..., -5.2414e-03,\n         -1.0475e-02, -7.7591e-03],\n        [-1.2779e-02, -1.8585e-02, -7.3395e-03,  ..., -3.6049e-03,\n          2.8763e-03,  3.4275e-03],\n        ...,\n        [-8.1940e-03,  3.1242e-03, -1.6205e-02,  ..., -7.0496e-03,\n         -4.8714e-03, -8.4152e-03],\n        [ 1.5427e-02, -4.7684e-04, -2.1515e-03,  ...,  1.2268e-02,\n          1.9073e-03,  1.8784e-02],\n        [ 9.0485e-03,  1.8417e-02,  1.3103e-03,  ...,  2.6665e-03,\n          7.4043e-03, -8.3923e-05]])\nmodel.layers.6.self_attn.v_proj.lora_B.default.weight: tensor([[ 0.0006, -0.0023, -0.0007,  ..., -0.0022,  0.0016, -0.0013],\n        [ 0.0006,  0.0019,  0.0022,  ..., -0.0006, -0.0002, -0.0012],\n        [-0.0010,  0.0014,  0.0017,  ..., -0.0020,  0.0013, -0.0014],\n        ...,\n        [ 0.0027,  0.0020, -0.0011,  ..., -0.0003, -0.0017, -0.0006],\n        [ 0.0021, -0.0023, -0.0021,  ..., -0.0025,  0.0020, -0.0005],\n        [-0.0005,  0.0010, -0.0008,  ...,  0.0004,  0.0009, -0.0002]])\nmodel.layers.6.self_attn.o_proj.base_layer.weight: tensor([[ 0.0052,  0.0095, -0.0099,  ...,  0.0135,  0.0031, -0.0008],\n        [ 0.0028, -0.0098, -0.0114,  ...,  0.0103,  0.0022, -0.0014],\n        [ 0.0003, -0.0128,  0.0039,  ..., -0.0117,  0.0031, -0.0153],\n        ...,\n        [ 0.0021, -0.0075, -0.0022,  ...,  0.0148,  0.0091,  0.0002],\n        [ 0.0071,  0.0029, -0.0051,  ..., -0.0077, -0.0003,  0.0057],\n        [ 0.0010, -0.0024,  0.0050,  ...,  0.0045,  0.0045,  0.0079]])\nmodel.layers.6.self_attn.o_proj.lora_A.default.weight: tensor([[ 2.4242e-03,  1.5388e-02, -1.2428e-02,  ...,  1.3062e-02,\n         -2.1744e-02, -3.1700e-03],\n        [-1.2329e-02, -5.5199e-03, -2.2316e-03,  ..., -6.5308e-03,\n         -4.2305e-03, -3.0327e-03],\n        [-1.6312e-02,  1.0300e-03, -4.9171e-03,  ..., -1.7075e-02,\n          7.8011e-03, -7.7057e-03],\n        ...,\n        [ 1.8425e-03,  9.9182e-03, -1.3687e-02,  ..., -8.3084e-03,\n          8.8959e-03,  5.3406e-05],\n        [ 1.0849e-02,  4.4289e-03, -2.5177e-03,  ..., -4.0970e-03,\n         -4.8447e-03, -1.5945e-02],\n        [ 1.1185e-02,  1.5869e-03,  1.3748e-02,  ..., -8.5297e-03,\n         -6.4507e-03, -8.3313e-03]])\nmodel.layers.6.self_attn.o_proj.lora_B.default.weight: tensor([[ 0.0026, -0.0001,  0.0006,  ...,  0.0017,  0.0007, -0.0010],\n        [-0.0023, -0.0012, -0.0005,  ..., -0.0009, -0.0012,  0.0019],\n        [ 0.0004,  0.0001,  0.0003,  ..., -0.0007, -0.0017,  0.0003],\n        ...,\n        [-0.0010,  0.0012,  0.0006,  ...,  0.0021, -0.0006,  0.0013],\n        [-0.0006, -0.0018, -0.0014,  ..., -0.0012, -0.0008,  0.0016],\n        [ 0.0022, -0.0022, -0.0015,  ...,  0.0010,  0.0004,  0.0015]])\nmodel.layers.6.mlp.gate_proj.base_layer.weight: tensor([[-0.0042,  0.0067,  0.0107,  ...,  0.0039,  0.0020, -0.0018],\n        [-0.0072, -0.0022,  0.0061,  ..., -0.0053,  0.0029, -0.0097],\n        [ 0.0049,  0.0014,  0.0006,  ..., -0.0002,  0.0079,  0.0044],\n        ...,\n        [ 0.0018, -0.0009, -0.0023,  ..., -0.0009, -0.0052,  0.0036],\n        [-0.0056, -0.0103, -0.0007,  ...,  0.0026, -0.0042, -0.0002],\n        [-0.0001,  0.0002, -0.0043,  ..., -0.0035, -0.0006, -0.0017]])\nmodel.layers.6.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0101, -0.0095, -0.0098,  ..., -0.0010,  0.0151, -0.0017],\n        [ 0.0059,  0.0096,  0.0077,  ...,  0.0057,  0.0105,  0.0029],\n        [-0.0038, -0.0004, -0.0086,  ...,  0.0091, -0.0081,  0.0204],\n        ...,\n        [-0.0017,  0.0181, -0.0039,  ...,  0.0076, -0.0187,  0.0033],\n        [-0.0150,  0.0073, -0.0150,  ..., -0.0085, -0.0023, -0.0070],\n        [-0.0030, -0.0007, -0.0044,  ..., -0.0111, -0.0010, -0.0088]])\nmodel.layers.6.mlp.gate_proj.lora_B.default.weight: tensor([[-1.0891e-03,  2.8896e-04, -2.7466e-03,  ...,  2.2182e-03,\n          4.2486e-04,  1.7548e-04],\n        [ 6.7043e-04,  1.3981e-03,  1.8778e-03,  ..., -3.6931e-04,\n          2.1040e-04, -1.2779e-03],\n        [ 9.4795e-04, -7.8773e-04,  4.6825e-04,  ..., -5.4836e-04,\n          3.1433e-03, -7.7343e-04],\n        ...,\n        [ 5.7268e-04,  1.8692e-04,  6.4850e-05,  ..., -8.8692e-05,\n         -3.0365e-03, -7.5006e-04],\n        [-2.6894e-04,  2.8858e-03,  1.3447e-04,  ..., -5.8651e-04,\n         -2.2659e-03, -1.5478e-03],\n        [ 1.9350e-03,  1.4067e-03,  1.5545e-04,  ..., -1.3428e-03,\n          1.2088e-04, -2.1210e-03]])\nmodel.layers.6.mlp.up_proj.base_layer.weight: tensor([[-0.0063,  0.0088, -0.0037,  ..., -0.0041,  0.0068, -0.0070],\n        [ 0.0020, -0.0067, -0.0065,  ...,  0.0052, -0.0083, -0.0075],\n        [ 0.0002,  0.0007,  0.0043,  ..., -0.0087, -0.0105,  0.0056],\n        ...,\n        [ 0.0048,  0.0039,  0.0064,  ..., -0.0031, -0.0089,  0.0089],\n        [ 0.0038, -0.0005, -0.0032,  ..., -0.0071,  0.0068,  0.0060],\n        [ 0.0013, -0.0017,  0.0010,  ..., -0.0105,  0.0030,  0.0034]])\nmodel.layers.6.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0026, -0.0085,  0.0037,  ...,  0.0005,  0.0032,  0.0051],\n        [ 0.0010, -0.0115, -0.0083,  ...,  0.0126,  0.0155,  0.0156],\n        [ 0.0053,  0.0076, -0.0091,  ...,  0.0133, -0.0203,  0.0096],\n        ...,\n        [-0.0174, -0.0007, -0.0113,  ..., -0.0175,  0.0010, -0.0088],\n        [ 0.0018, -0.0052,  0.0027,  ...,  0.0153,  0.0018,  0.0020],\n        [ 0.0014, -0.0021, -0.0057,  ..., -0.0052,  0.0048, -0.0058]])\nmodel.layers.6.mlp.up_proj.lora_B.default.weight: tensor([[ 6.1846e-04, -1.9255e-03,  1.6041e-03,  ...,  7.2098e-04,\n          1.6336e-03, -1.4019e-04],\n        [ 2.3603e-04,  9.3460e-05, -2.3746e-04,  ...,  2.3308e-03,\n         -2.6855e-03,  2.5234e-03],\n        [-1.3418e-03,  2.3861e-03, -9.2936e-04,  ...,  7.0858e-04,\n         -1.1778e-03, -4.9496e-04],\n        ...,\n        [ 4.3583e-04,  3.5024e-04, -6.8951e-04,  ...,  4.0531e-05,\n          8.0967e-04,  1.7571e-04],\n        [-9.0981e-04,  6.4659e-04,  1.3256e-04,  ...,  2.4662e-03,\n         -1.7605e-03,  1.9951e-03],\n        [-1.1024e-03, -1.3733e-03,  6.9094e-04,  ...,  9.1934e-04,\n         -6.6757e-06, -5.5790e-05]])\nmodel.layers.6.mlp.down_proj.base_layer.weight: tensor([[-0.0102,  0.0066, -0.0054,  ..., -0.0031,  0.0012, -0.0020],\n        [-0.0024,  0.0015,  0.0032,  ...,  0.0067, -0.0004,  0.0022],\n        [-0.0022, -0.0107, -0.0081,  ..., -0.0052,  0.0089,  0.0045],\n        ...,\n        [-0.0024, -0.0073, -0.0021,  ..., -0.0056,  0.0005, -0.0005],\n        [-0.0024, -0.0069, -0.0069,  ..., -0.0061,  0.0048,  0.0006],\n        [ 0.0040, -0.0084, -0.0053,  ...,  0.0020, -0.0005,  0.0043]])\nmodel.layers.6.mlp.down_proj.lora_A.default.weight: tensor([[-0.0018,  0.0059, -0.0068,  ...,  0.0002, -0.0017, -0.0058],\n        [-0.0042, -0.0025, -0.0021,  ..., -0.0002,  0.0036, -0.0096],\n        [-0.0050,  0.0016, -0.0007,  ...,  0.0003,  0.0051, -0.0025],\n        ...,\n        [-0.0031, -0.0005, -0.0050,  ...,  0.0015,  0.0009, -0.0045],\n        [-0.0015, -0.0008, -0.0031,  ...,  0.0004, -0.0026,  0.0020],\n        [-0.0029,  0.0032, -0.0006,  ...,  0.0020, -0.0016, -0.0012]])\nmodel.layers.6.mlp.down_proj.lora_B.default.weight: tensor([[-4.6301e-04, -1.4763e-03,  5.8746e-04,  ..., -1.3561e-03,\n          4.1628e-04,  1.0109e-03],\n        [ 1.4591e-03,  1.3809e-03, -1.9741e-03,  ...,  1.8263e-03,\n          4.4775e-04, -4.1604e-05],\n        [ 1.3332e-03,  6.6662e-04, -6.7234e-04,  ..., -3.9649e-04,\n          8.6308e-04, -6.0177e-04],\n        ...,\n        [ 2.0123e-03, -2.6965e-04, -1.1339e-03,  ...,  5.0354e-04,\n         -1.0672e-03,  2.2459e-04],\n        [-6.0415e-04,  1.0738e-03,  1.2493e-03,  ..., -5.8889e-04,\n         -9.9468e-04, -4.2439e-04],\n        [-9.7847e-04, -9.3937e-04, -8.2636e-04,  ..., -1.4343e-03,\n         -2.4128e-04,  2.3117e-03]])\nmodel.layers.6.input_layernorm.weight: tensor([1.9922, 0.9961, 2.2188,  ..., 0.9062, 3.1875, 0.3066])\nmodel.layers.6.post_attention_layernorm.weight: tensor([3.0000, 1.2656, 3.0625,  ..., 1.7812, 3.9688, 1.4219])\nmodel.layers.7.self_attn.q_proj.base_layer.weight: tensor([[-0.0120,  0.0232, -0.0019,  ..., -0.0064,  0.0098, -0.0049],\n        [ 0.0025,  0.0198,  0.0109,  ...,  0.0007, -0.0087,  0.0030],\n        [-0.0016,  0.0293,  0.0054,  ..., -0.0110,  0.0098, -0.0104],\n        ...,\n        [-0.0123,  0.0133, -0.0067,  ...,  0.0089,  0.0070,  0.0033],\n        [ 0.0056,  0.0080, -0.0012,  ..., -0.0178, -0.0082, -0.0109],\n        [ 0.0089, -0.0036, -0.0049,  ...,  0.0023, -0.0388, -0.0137]])\nmodel.layers.7.self_attn.q_proj.lora_A.default.weight: tensor([[ 2.0325e-02, -5.2834e-04,  1.0300e-02,  ..., -1.6815e-02,\n          2.1225e-02, -1.6617e-02],\n        [ 1.7441e-02,  2.7237e-03,  5.3406e-05,  ...,  5.9509e-04,\n          5.1346e-03, -2.8915e-03],\n        [ 7.5073e-03, -1.5259e-04,  1.3702e-02,  ...,  2.9526e-03,\n         -3.4695e-03,  1.4183e-02],\n        ...,\n        [ 7.3242e-03, -1.0925e-02,  2.1439e-03,  ...,  9.5062e-03,\n          3.7117e-03,  3.0708e-03],\n        [ 2.3994e-03, -1.4465e-02,  7.5264e-03,  ..., -1.0101e-02,\n          1.2390e-02, -1.3855e-02],\n        [ 1.5289e-02, -1.8600e-02, -3.5610e-03,  ...,  7.6103e-03,\n          2.6245e-03,  7.4005e-04]])\nmodel.layers.7.self_attn.q_proj.lora_B.default.weight: tensor([[-2.6703e-04, -1.8215e-04, -1.6365e-03,  ..., -8.9645e-05,\n          1.5354e-03,  1.0891e-03],\n        [ 9.6130e-04, -9.6750e-04,  2.4719e-03,  ...,  2.4080e-04,\n          1.4477e-03, -2.9411e-03],\n        [ 6.2275e-04,  7.8201e-05, -1.8644e-03,  ..., -1.3399e-03,\n         -1.0033e-03, -6.8092e-04],\n        ...,\n        [-3.3855e-05, -1.9693e-04,  1.9627e-03,  ..., -1.0853e-03,\n         -5.1355e-04, -1.4448e-03],\n        [-2.9984e-03, -2.1458e-03,  1.9245e-03,  ..., -3.0556e-03,\n         -2.7828e-03, -2.4052e-03],\n        [-2.3327e-03, -3.3913e-03,  1.3638e-03,  ..., -9.9087e-04,\n         -2.5883e-03, -1.7281e-03]])\nmodel.layers.7.self_attn.k_proj.base_layer.weight: tensor([[-0.0046,  0.0070, -0.0168,  ...,  0.0053, -0.0016,  0.0121],\n        [ 0.0011, -0.0029,  0.0040,  ...,  0.0095,  0.0073, -0.0054],\n        [-0.0095,  0.0056, -0.0151,  ...,  0.0044, -0.0044,  0.0098],\n        ...,\n        [ 0.0003,  0.0305, -0.0156,  ...,  0.0045, -0.0004, -0.0044],\n        [ 0.0123,  0.0025, -0.0027,  ...,  0.0041,  0.0006, -0.0077],\n        [ 0.0178,  0.0148, -0.0111,  ..., -0.0070, -0.0182, -0.0571]])\nmodel.layers.7.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0153,  0.0027,  0.0159,  ..., -0.0136,  0.0008,  0.0054],\n        [-0.0108,  0.0059, -0.0045,  ..., -0.0154, -0.0007,  0.0006],\n        [-0.0089, -0.0061,  0.0002,  ..., -0.0041, -0.0019, -0.0016],\n        ...,\n        [ 0.0022,  0.0091, -0.0095,  ...,  0.0036,  0.0036,  0.0153],\n        [ 0.0013,  0.0080, -0.0089,  ...,  0.0016,  0.0035,  0.0096],\n        [ 0.0182, -0.0086,  0.0030,  ...,  0.0016, -0.0164, -0.0010]])\nmodel.layers.7.self_attn.k_proj.lora_B.default.weight: tensor([[ 6.2418e-04,  1.9417e-03, -2.4147e-03,  ..., -1.2646e-03,\n          1.2331e-03, -8.6498e-04],\n        [ 6.0177e-04,  5.5790e-04,  1.8835e-05,  ...,  6.6757e-05,\n          9.4891e-04,  1.7242e-03],\n        [-1.7905e-04,  2.2278e-03, -9.3460e-05,  ...,  3.6240e-05,\n         -3.2783e-04, -1.9875e-03],\n        ...,\n        [ 7.5674e-04,  2.1095e-03,  4.4966e-04,  ..., -1.4639e-03,\n         -1.6136e-03, -7.0620e-04],\n        [ 2.1744e-03,  2.1172e-04,  1.6308e-04,  ..., -1.4296e-03,\n          2.1343e-03,  1.0109e-04],\n        [-2.6207e-03,  5.1022e-04,  1.1492e-03,  ..., -6.5804e-05,\n         -1.2264e-03, -1.9608e-03]])\nmodel.layers.7.self_attn.v_proj.base_layer.weight: tensor([[-0.0076, -0.0028,  0.0104,  ...,  0.0081,  0.0114, -0.0053],\n        [-0.0221,  0.0051,  0.0087,  ...,  0.0023, -0.0051,  0.0087],\n        [ 0.0003,  0.0109, -0.0031,  ...,  0.0039, -0.0050, -0.0032],\n        ...,\n        [-0.0130,  0.0026, -0.0144,  ...,  0.0110, -0.0086, -0.0056],\n        [-0.0164,  0.0037,  0.0135,  ...,  0.0125,  0.0157, -0.0142],\n        [ 0.0066,  0.0095,  0.0052,  ...,  0.0057,  0.0081, -0.0037]])\nmodel.layers.7.self_attn.v_proj.lora_A.default.weight: tensor([[ 0.0179, -0.0180,  0.0142,  ..., -0.0117, -0.0009, -0.0091],\n        [-0.0113,  0.0023, -0.0012,  ...,  0.0051,  0.0080,  0.0008],\n        [-0.0108, -0.0031, -0.0109,  ..., -0.0078, -0.0037, -0.0001],\n        ...,\n        [ 0.0139,  0.0053, -0.0010,  ..., -0.0158,  0.0044, -0.0058],\n        [-0.0029,  0.0040, -0.0135,  ...,  0.0001, -0.0100, -0.0111],\n        [-0.0082, -0.0016, -0.0117,  ..., -0.0085, -0.0067, -0.0162]])\nmodel.layers.7.self_attn.v_proj.lora_B.default.weight: tensor([[-2.1420e-03, -1.4009e-03,  1.8654e-03,  ..., -1.1292e-03,\n         -6.4898e-04, -7.9966e-04],\n        [ 3.4666e-04, -1.4172e-03,  8.5163e-04,  ..., -8.5640e-04,\n          1.0967e-04,  8.7976e-04],\n        [-5.9271e-04, -1.7967e-03,  3.8147e-04,  ..., -5.1165e-04,\n         -3.0637e-04, -4.3535e-04],\n        ...,\n        [-4.4394e-04, -1.6212e-03,  5.8079e-04,  ..., -2.7895e-04,\n         -1.3638e-03,  8.4877e-05],\n        [ 2.0981e-04,  3.2520e-04, -5.9080e-04,  ...,  2.8496e-03,\n          5.4073e-04,  2.5406e-03],\n        [-3.1924e-04, -5.6267e-04,  1.2970e-03,  ...,  1.6079e-03,\n         -9.1076e-04,  1.8787e-03]])\nmodel.layers.7.self_attn.o_proj.base_layer.weight: tensor([[ 0.0008, -0.0025,  0.0052,  ...,  0.0048,  0.0013, -0.0015],\n        [-0.0092,  0.0011, -0.0098,  ..., -0.0049, -0.0099,  0.0008],\n        [-0.0116, -0.0087, -0.0040,  ..., -0.0058,  0.0090,  0.0016],\n        ...,\n        [-0.0098,  0.0066,  0.0004,  ...,  0.0001,  0.0039, -0.0049],\n        [ 0.0040,  0.0023, -0.0012,  ..., -0.0074,  0.0023, -0.0009],\n        [-0.0003, -0.0025, -0.0023,  ...,  0.0108, -0.0070,  0.0099]])\nmodel.layers.7.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0103,  0.0029, -0.0092,  ...,  0.0189, -0.0071,  0.0114],\n        [-0.0010, -0.0039, -0.0083,  ..., -0.0035,  0.0022, -0.0083],\n        [ 0.0037, -0.0094,  0.0094,  ...,  0.0100, -0.0069,  0.0205],\n        ...,\n        [-0.0003,  0.0090, -0.0127,  ...,  0.0132,  0.0045,  0.0104],\n        [ 0.0095, -0.0038,  0.0191,  ..., -0.0185,  0.0036, -0.0065],\n        [ 0.0032,  0.0101,  0.0032,  ..., -0.0050,  0.0049,  0.0056]])\nmodel.layers.7.self_attn.o_proj.lora_B.default.weight: tensor([[ 1.0719e-03, -6.3610e-04, -1.4830e-03,  ..., -1.3409e-03,\n         -1.6479e-03, -2.2602e-03],\n        [ 1.7166e-03, -4.3106e-04,  2.6054e-03,  ...,  1.1549e-03,\n          8.2016e-04,  4.6349e-04],\n        [ 2.3913e-04, -1.2474e-03,  4.8041e-04,  ...,  9.8705e-05,\n          1.8673e-03,  9.7561e-04],\n        ...,\n        [ 3.7956e-04, -2.6436e-03,  1.0281e-03,  ...,  1.2875e-03,\n         -1.7319e-03, -2.1315e-04],\n        [ 1.2856e-03,  1.1177e-03,  6.4182e-04,  ...,  6.8378e-04,\n         -7.2289e-04,  1.2398e-04],\n        [ 2.5225e-04,  9.1219e-04, -1.3008e-03,  ..., -1.2436e-03,\n         -4.5586e-04,  6.7854e-04]])\nmodel.layers.7.mlp.gate_proj.base_layer.weight: tensor([[-4.4346e-05, -3.8757e-03,  3.3264e-03,  ...,  1.0864e-02,\n         -3.6621e-03, -7.9956e-03],\n        [-4.5166e-03,  4.1504e-03,  7.4768e-04,  ..., -4.6387e-03,\n          2.4567e-03, -6.7749e-03],\n        [-4.4441e-04,  6.5613e-03, -8.9722e-03,  ..., -1.4877e-03,\n          1.0147e-03,  5.5847e-03],\n        ...,\n        [-1.0986e-03,  1.6327e-03, -6.4087e-03,  ...,  3.0212e-03,\n          7.6294e-04,  5.0964e-03],\n        [-3.0060e-03,  2.2430e-03,  1.7456e-02,  ...,  8.9722e-03,\n         -2.9297e-03, -5.3406e-03],\n        [-1.1414e-02,  3.6621e-03,  5.8289e-03,  ..., -4.4250e-03,\n         -7.0572e-05, -3.9978e-03]])\nmodel.layers.7.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0056, -0.0154,  0.0016,  ..., -0.0074, -0.0183, -0.0057],\n        [-0.0171,  0.0070, -0.0045,  ...,  0.0052, -0.0049, -0.0087],\n        [-0.0095, -0.0053, -0.0095,  ...,  0.0003, -0.0128,  0.0002],\n        ...,\n        [-0.0129, -0.0043, -0.0142,  ..., -0.0043, -0.0009,  0.0029],\n        [-0.0016,  0.0058,  0.0063,  ...,  0.0033, -0.0073, -0.0026],\n        [-0.0076,  0.0076, -0.0012,  ..., -0.0195,  0.0043,  0.0003]])\nmodel.layers.7.mlp.gate_proj.lora_B.default.weight: tensor([[ 8.5306e-04,  1.5335e-03, -4.3869e-05,  ..., -6.6996e-04,\n          4.3535e-04,  5.6362e-04],\n        [-2.4176e-04, -1.3924e-04,  1.3790e-03,  ...,  1.0414e-03,\n          5.5695e-04, -9.1743e-04],\n        [-3.5572e-04,  1.6003e-03,  1.9178e-03,  ...,  2.0027e-03,\n         -1.8597e-04, -2.8038e-04],\n        ...,\n        [-1.5402e-03,  1.7691e-03, -5.3692e-04,  ...,  8.7452e-04,\n          3.7241e-04, -2.1019e-03],\n        [ 3.7384e-04, -9.1839e-04,  1.7128e-03,  ..., -9.9754e-04,\n          9.3317e-04, -2.9659e-04],\n        [ 5.1594e-04, -1.6499e-04,  2.8133e-05,  ..., -7.8154e-04,\n          1.5841e-03, -1.4305e-03]])\nmodel.layers.7.mlp.up_proj.base_layer.weight: tensor([[-9.8419e-04, -5.7983e-03, -3.2196e-03,  ..., -1.0223e-03,\n          9.1553e-03,  4.6997e-03],\n        [ 2.8381e-03,  6.7139e-03,  6.4087e-03,  ...,  7.7057e-04,\n         -2.3956e-03,  1.6724e-02],\n        [-4.1809e-03, -8.8692e-05,  3.0975e-03,  ..., -7.0190e-03,\n          2.2583e-03,  7.5684e-03],\n        ...,\n        [ 2.6398e-03,  4.3640e-03, -4.1199e-03,  ..., -3.1128e-03,\n         -4.1504e-03,  8.4229e-03],\n        [ 1.8215e-04,  2.0020e-02, -9.9945e-04,  ..., -2.9602e-03,\n          6.6223e-03, -1.0742e-02],\n        [ 2.1973e-03, -3.0975e-03, -6.6833e-03,  ..., -9.4986e-04,\n         -8.2779e-04,  6.7139e-03]])\nmodel.layers.7.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0061, -0.0085,  0.0055,  ..., -0.0073, -0.0120, -0.0088],\n        [-0.0130, -0.0133, -0.0018,  ...,  0.0084, -0.0071,  0.0083],\n        [-0.0102,  0.0213, -0.0100,  ...,  0.0050, -0.0194,  0.0206],\n        ...,\n        [ 0.0082, -0.0085, -0.0026,  ..., -0.0047, -0.0096,  0.0027],\n        [-0.0186, -0.0111, -0.0044,  ...,  0.0119,  0.0077,  0.0008],\n        [-0.0100,  0.0007, -0.0172,  ..., -0.0115,  0.0005,  0.0008]])\nmodel.layers.7.mlp.up_proj.lora_B.default.weight: tensor([[ 1.4267e-03, -1.2817e-03,  3.3092e-04,  ..., -1.6146e-03,\n         -1.6594e-03, -2.0409e-03],\n        [-2.6169e-03, -9.5844e-04,  5.2452e-06,  ...,  3.7956e-04,\n          2.4390e-04,  4.5776e-05],\n        [ 4.1771e-04, -1.4668e-03,  1.8482e-03,  ...,  2.4986e-03,\n         -1.8368e-03, -5.8699e-04],\n        ...,\n        [-2.5177e-04,  3.7146e-04, -5.7983e-04,  ..., -1.3084e-03,\n          2.6584e-04, -1.5106e-03],\n        [-2.4796e-04, -1.0014e-03, -1.3351e-04,  ..., -1.0786e-03,\n         -1.5507e-03, -1.6632e-03],\n        [ 2.6941e-04, -6.6423e-04, -1.8024e-03,  ...,  2.3212e-03,\n          8.3327e-05,  1.2417e-03]])\nmodel.layers.7.mlp.down_proj.base_layer.weight: tensor([[-6.5613e-03,  2.6398e-03,  2.9373e-04,  ...,  4.7112e-04,\n         -4.6539e-04,  1.0071e-02],\n        [-6.3171e-03,  4.3030e-03,  1.0559e-02,  ..., -7.2002e-05,\n         -1.0010e-02,  3.1433e-03],\n        [-3.3722e-03,  6.1035e-03, -8.8501e-03,  ...,  1.1826e-04,\n         -7.6904e-03, -8.5449e-03],\n        ...,\n        [-8.9722e-03, -2.6398e-03, -8.1177e-03,  ...,  2.8253e-05,\n         -2.9755e-03,  2.3174e-04],\n        [ 5.6458e-03,  9.4986e-04,  7.5150e-04,  ..., -1.0010e-02,\n          1.7166e-03,  1.7929e-04],\n        [ 9.3384e-03,  2.7618e-03,  5.7983e-03,  ...,  3.9062e-03,\n          1.4099e-02,  2.1515e-03]])\nmodel.layers.7.mlp.down_proj.lora_A.default.weight: tensor([[ 0.0054, -0.0035, -0.0007,  ...,  0.0067, -0.0036,  0.0029],\n        [-0.0056,  0.0039, -0.0059,  ..., -0.0010,  0.0015, -0.0059],\n        [-0.0067, -0.0006, -0.0035,  ..., -0.0007,  0.0055, -0.0058],\n        ...,\n        [ 0.0015, -0.0054,  0.0024,  ...,  0.0008,  0.0036,  0.0037],\n        [ 0.0005, -0.0018, -0.0035,  ..., -0.0006, -0.0022,  0.0030],\n        [ 0.0029,  0.0002,  0.0010,  ...,  0.0048, -0.0017,  0.0021]])\nmodel.layers.7.mlp.down_proj.lora_B.default.weight: tensor([[-0.0012,  0.0007,  0.0003,  ..., -0.0015, -0.0012, -0.0012],\n        [ 0.0006, -0.0013, -0.0006,  ..., -0.0007,  0.0008,  0.0022],\n        [-0.0008, -0.0007,  0.0008,  ..., -0.0008, -0.0007, -0.0007],\n        ...,\n        [-0.0008,  0.0022,  0.0002,  ..., -0.0012, -0.0003, -0.0012],\n        [ 0.0004, -0.0002, -0.0009,  ..., -0.0012,  0.0015,  0.0011],\n        [-0.0004, -0.0019,  0.0005,  ..., -0.0008, -0.0005,  0.0003]])\nmodel.layers.7.input_layernorm.weight: tensor([1.3984, 0.8633, 1.4141,  ..., 1.0156, 1.9297, 0.5156])\nmodel.layers.7.post_attention_layernorm.weight: tensor([2.8594, 1.1328, 2.9375,  ..., 1.6094, 3.6250, 1.2109])\nmodel.layers.8.self_attn.q_proj.base_layer.weight: tensor([[-0.0021,  0.0107, -0.0042,  ...,  0.0102,  0.0013, -0.0111],\n        [ 0.0069, -0.0129, -0.0006,  ...,  0.0036,  0.0005,  0.0040],\n        [ 0.0033, -0.0187, -0.0077,  ..., -0.0197,  0.0107, -0.0032],\n        ...,\n        [ 0.0034, -0.0013, -0.0276,  ...,  0.0011, -0.0181, -0.0022],\n        [-0.0005, -0.0188,  0.0058,  ..., -0.0251, -0.0068, -0.0261],\n        [ 0.0018, -0.0096,  0.0125,  ...,  0.0153,  0.0142,  0.0022]])\nmodel.layers.8.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0105, -0.0008, -0.0030,  ..., -0.0192, -0.0167, -0.0187],\n        [-0.0164, -0.0093, -0.0118,  ...,  0.0005, -0.0004, -0.0107],\n        [ 0.0103, -0.0076,  0.0109,  ..., -0.0021,  0.0175, -0.0101],\n        ...,\n        [-0.0049, -0.0032, -0.0033,  ..., -0.0099,  0.0129, -0.0077],\n        [ 0.0146, -0.0040,  0.0213,  ...,  0.0168,  0.0031,  0.0196],\n        [ 0.0125,  0.0156,  0.0019,  ..., -0.0097,  0.0087, -0.0091]])\nmodel.layers.8.self_attn.q_proj.lora_B.default.weight: tensor([[-0.0021,  0.0011, -0.0001,  ...,  0.0008,  0.0015, -0.0010],\n        [-0.0020, -0.0012,  0.0003,  ..., -0.0026, -0.0016,  0.0027],\n        [ 0.0020,  0.0006, -0.0004,  ...,  0.0007, -0.0002, -0.0025],\n        ...,\n        [ 0.0004, -0.0007, -0.0004,  ...,  0.0010,  0.0004, -0.0010],\n        [ 0.0004, -0.0003, -0.0017,  ...,  0.0004,  0.0011,  0.0006],\n        [ 0.0006, -0.0003, -0.0012,  ..., -0.0005, -0.0008,  0.0013]])\nmodel.layers.8.self_attn.k_proj.base_layer.weight: tensor([[-0.0042,  0.0077, -0.0002,  ..., -0.0040, -0.0077, -0.0005],\n        [ 0.0014, -0.0092,  0.0033,  ...,  0.0015,  0.0013,  0.0016],\n        [-0.0017,  0.0057, -0.0055,  ...,  0.0029, -0.0029, -0.0031],\n        ...,\n        [ 0.0181,  0.0043, -0.0265,  ..., -0.0144,  0.0006, -0.0498],\n        [-0.0039, -0.0393, -0.0006,  ..., -0.0212, -0.0004, -0.0228],\n        [-0.0009,  0.0101, -0.0217,  ...,  0.0194,  0.0182,  0.0146]])\nmodel.layers.8.self_attn.k_proj.lora_A.default.weight: tensor([[-0.0059,  0.0075, -0.0072,  ..., -0.0042, -0.0028,  0.0061],\n        [-0.0107,  0.0176, -0.0175,  ..., -0.0103,  0.0009,  0.0034],\n        [-0.0083,  0.0207,  0.0005,  ..., -0.0028, -0.0057,  0.0098],\n        ...,\n        [-0.0028, -0.0166,  0.0057,  ...,  0.0054,  0.0163, -0.0047],\n        [ 0.0198,  0.0021,  0.0096,  ...,  0.0207,  0.0177,  0.0123],\n        [ 0.0075,  0.0052,  0.0016,  ..., -0.0013,  0.0051,  0.0139]])\nmodel.layers.8.self_attn.k_proj.lora_B.default.weight: tensor([[-3.2520e-04, -2.3699e-04, -8.5545e-04,  ...,  1.6060e-03,\n         -2.1362e-03, -1.5507e-03],\n        [ 1.1082e-03, -2.7008e-03,  2.7542e-03,  ..., -8.0013e-04,\n          2.9659e-03,  1.0014e-04],\n        [ 2.3842e-03,  4.1008e-05,  1.3905e-03,  ...,  2.8610e-04,\n          5.7602e-04,  1.9989e-03],\n        ...,\n        [-3.9268e-04, -1.3971e-03, -1.7166e-03,  ..., -6.8426e-04,\n         -1.7929e-04,  5.2977e-04],\n        [-2.1992e-03,  1.2064e-03, -2.7580e-03,  ...,  1.0033e-03,\n         -1.9665e-03,  3.8147e-05],\n        [-1.4429e-03,  1.7529e-03, -2.4433e-03,  ...,  1.5497e-04,\n         -4.6420e-04, -1.1482e-03]])\nmodel.layers.8.self_attn.v_proj.base_layer.weight: tensor([[-0.0060, -0.0006, -0.0034,  ...,  0.0002, -0.0018,  0.0034],\n        [ 0.0120, -0.0073, -0.0044,  ..., -0.0035,  0.0094,  0.0093],\n        [-0.0066, -0.0016, -0.0042,  ..., -0.0028, -0.0076, -0.0012],\n        ...,\n        [ 0.0112,  0.0077, -0.0050,  ...,  0.0051, -0.0014,  0.0019],\n        [-0.0037, -0.0043,  0.0038,  ...,  0.0045, -0.0005,  0.0052],\n        [ 0.0129,  0.0006,  0.0090,  ..., -0.0012, -0.0005,  0.0078]])\nmodel.layers.8.self_attn.v_proj.lora_A.default.weight: tensor([[ 0.0179, -0.0021,  0.0054,  ..., -0.0048, -0.0112, -0.0106],\n        [-0.0074,  0.0003, -0.0091,  ...,  0.0025,  0.0097,  0.0058],\n        [-0.0041, -0.0110, -0.0123,  ..., -0.0093,  0.0011, -0.0204],\n        ...,\n        [ 0.0186, -0.0098,  0.0106,  ...,  0.0017, -0.0092, -0.0082],\n        [ 0.0049,  0.0083,  0.0005,  ...,  0.0195, -0.0002,  0.0029],\n        [-0.0016, -0.0084, -0.0041,  ...,  0.0173,  0.0092,  0.0169]])\nmodel.layers.8.self_attn.v_proj.lora_B.default.weight: tensor([[ 1.5163e-04, -1.6966e-03, -2.7037e-04,  ...,  2.1362e-03,\n         -7.4816e-04, -1.0109e-03],\n        [-5.7316e-04, -2.1152e-03,  4.8828e-04,  ...,  1.8425e-03,\n          1.4019e-04, -5.9128e-05],\n        [-8.5068e-04,  3.3140e-04,  2.9612e-04,  ..., -8.3733e-04,\n          1.6298e-03,  2.1219e-05],\n        ...,\n        [ 1.6975e-03, -7.8154e-04, -1.4133e-03,  ...,  1.5926e-04,\n         -3.2854e-04, -1.6880e-04],\n        [-3.8195e-04, -2.7633e-04,  7.6151e-04,  ..., -1.7548e-04,\n          1.2112e-03,  1.9112e-03],\n        [-1.7929e-03,  4.3964e-04, -4.0913e-04,  ..., -1.6422e-03,\n          1.4038e-03, -9.5367e-07]])\nmodel.layers.8.self_attn.o_proj.base_layer.weight: tensor([[ 2.1820e-03,  6.7139e-03, -8.0566e-03,  ...,  1.6632e-03,\n         -6.7749e-03, -7.1411e-03],\n        [ 1.5488e-03,  5.3346e-06, -4.1809e-03,  ..., -6.9580e-03,\n          9.9487e-03,  1.0742e-02],\n        [-4.6082e-03,  4.4250e-03,  2.8229e-03,  ...,  2.3193e-03,\n          8.9111e-03, -3.3264e-03],\n        ...,\n        [-1.0376e-02,  1.8387e-03,  2.3041e-03,  ..., -4.6921e-04,\n          4.6997e-03,  1.0498e-02],\n        [-2.2095e-02,  3.8300e-03, -4.9973e-04,  ...,  5.4932e-03,\n          5.3883e-05, -2.2125e-03],\n        [-4.1771e-04,  1.0010e-02,  3.3569e-03,  ..., -1.1444e-03,\n          3.6011e-03,  5.7983e-03]])\nmodel.layers.8.self_attn.o_proj.lora_A.default.weight: tensor([[-3.5820e-03, -1.1902e-02, -9.5291e-03,  ...,  6.2370e-03,\n         -2.7962e-03,  1.2489e-02],\n        [ 4.5242e-03, -4.8332e-03,  9.1705e-03,  ..., -1.6113e-02,\n         -5.8441e-03, -1.0201e-02],\n        [-1.2527e-02, -1.0933e-02,  5.7297e-03,  ...,  7.2861e-03,\n          3.2845e-03,  7.6180e-03],\n        ...,\n        [ 9.2163e-03, -1.2848e-02,  1.7365e-02,  ...,  2.3079e-03,\n          8.8501e-04,  9.5367e-03],\n        [-1.9073e-05,  1.6998e-02,  7.3547e-03,  ..., -6.2828e-03,\n         -8.3160e-03,  2.7180e-03],\n        [ 3.5439e-03,  6.5088e-04,  9.2850e-03,  ...,  8.3084e-03,\n          1.3443e-02,  2.2507e-03]])\nmodel.layers.8.self_attn.o_proj.lora_B.default.weight: tensor([[ 1.6994e-03,  1.4391e-03, -2.1267e-03,  ...,  2.6107e-04,\n          4.7040e-04, -3.3355e-04],\n        [-9.6321e-05,  3.3855e-04,  2.3403e-03,  ...,  2.3098e-03,\n         -8.1396e-04,  1.3981e-03],\n        [ 6.5088e-04, -2.6321e-04, -1.6193e-03,  ..., -1.6022e-03,\n          2.4452e-03, -3.1710e-04],\n        ...,\n        [-7.9775e-04, -1.6994e-03,  1.0881e-03,  ...,  9.3269e-04,\n         -4.1819e-04, -1.6499e-04],\n        [-1.3885e-03,  2.0218e-04,  2.0809e-03,  ...,  8.9979e-04,\n         -2.2926e-03,  1.3237e-03],\n        [ 6.8045e-04,  1.6403e-04, -2.2621e-03,  ..., -2.8477e-03,\n          1.6651e-03, -2.1744e-03]])\nmodel.layers.8.mlp.gate_proj.base_layer.weight: tensor([[ 0.0015,  0.0045,  0.0002,  ...,  0.0009, -0.0002, -0.0027],\n        [ 0.0046,  0.0071, -0.0017,  ..., -0.0011, -0.0093, -0.0012],\n        [-0.0056,  0.0064, -0.0010,  ..., -0.0128,  0.0057, -0.0019],\n        ...,\n        [ 0.0005,  0.0013,  0.0073,  ...,  0.0018,  0.0041, -0.0023],\n        [ 0.0096, -0.0039,  0.0023,  ..., -0.0018, -0.0007, -0.0010],\n        [ 0.0010, -0.0054, -0.0033,  ..., -0.0040, -0.0048, -0.0042]])\nmodel.layers.8.mlp.gate_proj.lora_A.default.weight: tensor([[ 2.1744e-04,  2.7008e-03, -8.0566e-03,  ...,  1.5244e-02,\n         -7.0572e-03, -1.0300e-03],\n        [-4.2725e-03, -1.5320e-02,  1.2825e-02,  ...,  4.2076e-03,\n         -4.6768e-03, -7.3204e-03],\n        [-6.4583e-03, -1.0513e-02, -1.8234e-02,  ..., -7.8049e-03,\n         -1.8814e-02,  2.0905e-03],\n        ...,\n        [ 4.8790e-03,  2.2736e-03, -4.4098e-03,  ..., -3.8738e-03,\n          1.2108e-02, -3.8910e-04],\n        [ 3.7766e-03,  6.1073e-03,  1.2321e-02,  ..., -4.4785e-03,\n          5.4932e-03, -3.9482e-03],\n        [ 3.6240e-05, -9.7847e-04, -5.6992e-03,  ..., -6.7139e-04,\n          2.2507e-04,  5.5790e-05]])\nmodel.layers.8.mlp.gate_proj.lora_B.default.weight: tensor([[ 6.3419e-04, -1.0920e-03,  1.9894e-03,  ...,  5.5027e-04,\n          1.7853e-03, -5.2929e-04],\n        [-2.6274e-04, -3.7789e-04,  1.2951e-03,  ..., -7.2479e-05,\n         -3.1424e-04, -1.3428e-03],\n        [ 5.2309e-04, -2.7180e-05,  2.6207e-03,  ...,  3.4833e-04,\n          1.3199e-03, -2.5439e-04],\n        ...,\n        [-1.2255e-03,  1.3790e-03, -1.0023e-03,  ...,  6.3086e-04,\n         -2.5253e-03, -4.2534e-04],\n        [ 2.7370e-03, -1.3132e-03, -2.8706e-04,  ..., -4.9496e-04,\n         -7.8106e-04, -8.4782e-04],\n        [ 1.0461e-04,  1.4114e-03,  1.9407e-04,  ..., -1.6651e-03,\n          2.6169e-03,  2.8496e-03]])\nmodel.layers.8.mlp.up_proj.base_layer.weight: tensor([[ 0.0167,  0.0110, -0.0056,  ..., -0.0112,  0.0031, -0.0035],\n        [ 0.0052,  0.0045, -0.0043,  ...,  0.0024, -0.0003, -0.0012],\n        [ 0.0087, -0.0078,  0.0023,  ..., -0.0116,  0.0030,  0.0114],\n        ...,\n        [-0.0022,  0.0054,  0.0056,  ...,  0.0018,  0.0051, -0.0002],\n        [-0.0013,  0.0099,  0.0040,  ...,  0.0035,  0.0011, -0.0046],\n        [-0.0048, -0.0030, -0.0037,  ..., -0.0065,  0.0119, -0.0046]])\nmodel.layers.8.mlp.up_proj.lora_A.default.weight: tensor([[ 2.7294e-03, -2.6588e-03,  8.7738e-03,  ...,  1.0147e-03,\n         -8.3389e-03,  7.5531e-03],\n        [-1.6647e-02, -1.1932e-02, -1.0773e-02,  ..., -4.0436e-03,\n          1.2970e-04, -9.7504e-03],\n        [-5.9471e-03,  3.1013e-03,  1.0433e-03,  ..., -1.7593e-02,\n         -2.3270e-04, -1.1154e-02],\n        ...,\n        [ 9.9182e-05, -1.9016e-03,  1.5091e-02,  ..., -8.6517e-03,\n          5.1384e-03, -1.6754e-02],\n        [ 1.6693e-02, -7.8583e-03,  8.8272e-03,  ...,  2.6703e-03,\n          3.7956e-03, -1.1032e-02],\n        [-2.7313e-03,  3.5553e-03, -2.0103e-03,  ..., -1.4236e-02,\n          1.4542e-02, -1.6113e-02]])\nmodel.layers.8.mlp.up_proj.lora_B.default.weight: tensor([[-8.7738e-04,  7.5531e-04,  1.1835e-03,  ..., -1.2150e-03,\n          2.1896e-03,  6.1989e-04],\n        [ 2.4223e-03, -2.6093e-03,  5.4359e-04,  ..., -2.0123e-04,\n         -6.9094e-04, -7.9393e-04],\n        [ 8.0109e-05, -7.0763e-04, -1.5545e-03,  ...,  1.7166e-04,\n          3.5286e-05, -2.3117e-03],\n        ...,\n        [-2.1553e-03,  7.2718e-05,  3.7813e-04,  ..., -1.0338e-03,\n          4.3964e-04,  1.2045e-03],\n        [-2.1877e-03, -1.5078e-03, -4.5466e-04,  ...,  7.3433e-05,\n         -5.6553e-04,  7.6246e-04],\n        [ 3.1242e-03,  8.3828e-04,  2.3441e-03,  ..., -1.2197e-03,\n          1.0204e-04,  1.1277e-04]])\nmodel.layers.8.mlp.down_proj.base_layer.weight: tensor([[ 1.0376e-02,  2.0142e-03,  5.4016e-03,  ..., -9.1553e-04,\n         -4.1809e-03, -3.0823e-03],\n        [ 6.4697e-03,  3.1281e-03, -2.9755e-03,  ..., -1.0315e-02,\n          1.7456e-02,  8.6060e-03],\n        [-9.0942e-03, -6.6528e-03,  1.4648e-03,  ..., -2.7466e-04,\n         -1.1749e-03,  1.8082e-03],\n        ...,\n        [-7.6294e-03,  4.4823e-04, -9.7656e-03,  ..., -1.8692e-03,\n         -8.5831e-04, -2.1820e-03],\n        [-2.4261e-03, -1.4526e-02,  6.0320e-05,  ...,  3.0670e-03,\n          7.2327e-03, -1.9989e-03],\n        [ 4.3640e-03,  1.5198e-02, -5.2185e-03,  ...,  1.7624e-03,\n          4.9133e-03, -1.7578e-02]])\nmodel.layers.8.mlp.down_proj.lora_A.default.weight: tensor([[-0.0018,  0.0022,  0.0015,  ...,  0.0074, -0.0015, -0.0007],\n        [-0.0047, -0.0060, -0.0026,  ..., -0.0026,  0.0025, -0.0055],\n        [ 0.0024, -0.0014, -0.0026,  ..., -0.0048, -0.0027, -0.0003],\n        ...,\n        [-0.0050,  0.0064, -0.0017,  ...,  0.0004,  0.0044, -0.0057],\n        [ 0.0024, -0.0037,  0.0028,  ...,  0.0038, -0.0035,  0.0057],\n        [-0.0031,  0.0059, -0.0055,  ..., -0.0005,  0.0030,  0.0009]])\nmodel.layers.8.mlp.down_proj.lora_B.default.weight: tensor([[-1.7118e-04,  5.1498e-04, -6.3300e-05,  ...,  3.0565e-04,\n          2.5630e-04, -7.7820e-04],\n        [-5.2691e-04,  1.3103e-03, -2.4376e-03,  ..., -4.2057e-04,\n          1.3371e-03, -2.2163e-03],\n        [-1.0414e-03, -6.2704e-04,  1.0920e-03,  ...,  1.8382e-04,\n         -5.1117e-04,  1.7462e-03],\n        ...,\n        [-1.5173e-03,  1.7986e-03, -1.5039e-03,  ..., -1.5230e-03,\n         -4.2248e-04, -1.1759e-03],\n        [ 1.1826e-03, -2.9755e-04, -1.2150e-03,  ...,  2.5291e-03,\n          1.6985e-03,  1.0309e-03],\n        [ 1.8549e-04, -1.4229e-03,  2.1286e-03,  ...,  3.8075e-04,\n         -8.9312e-04,  6.2561e-04]])\nmodel.layers.8.input_layernorm.weight: tensor([1.5078, 0.7891, 1.7578,  ..., 1.0156, 2.7188, 0.5664])\nmodel.layers.8.post_attention_layernorm.weight: tensor([2.7812, 1.0234, 2.8125,  ..., 1.4297, 3.5156, 1.1250])\nmodel.layers.9.self_attn.q_proj.base_layer.weight: tensor([[-2.4109e-03,  3.7384e-03,  4.5013e-04,  ..., -3.8452e-03,\n         -7.1335e-04,  5.7220e-04],\n        [ 4.3869e-04,  3.4904e-04,  5.0049e-03,  ...,  5.4626e-03,\n         -1.5793e-03, -7.0190e-04],\n        [ 1.5030e-03,  4.4861e-03, -1.8921e-03,  ...,  8.9722e-03,\n          3.9291e-04,  2.0508e-02],\n        ...,\n        [ 5.2490e-03,  5.9509e-03, -2.5940e-03,  ...,  1.3184e-02,\n          6.0425e-03, -8.5449e-03],\n        [-9.1553e-03,  6.6280e-05, -5.4016e-03,  ..., -2.5787e-03,\n          1.3977e-02,  2.0630e-02],\n        [-6.5308e-03,  1.1475e-02, -3.6011e-03,  ..., -5.2185e-03,\n          7.7248e-05,  6.1989e-05]])\nmodel.layers.9.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0223, -0.0169,  0.0039,  ...,  0.0039,  0.0074, -0.0014],\n        [ 0.0029,  0.0034, -0.0042,  ...,  0.0005, -0.0025,  0.0074],\n        [-0.0043, -0.0037, -0.0023,  ..., -0.0042, -0.0090, -0.0084],\n        ...,\n        [-0.0113,  0.0062, -0.0044,  ..., -0.0025,  0.0002, -0.0113],\n        [ 0.0064, -0.0169,  0.0093,  ...,  0.0077, -0.0018,  0.0072],\n        [-0.0129,  0.0052, -0.0094,  ..., -0.0026,  0.0062, -0.0085]])\nmodel.layers.9.self_attn.q_proj.lora_B.default.weight: tensor([[ 2.7199e-03,  4.0627e-04,  2.8915e-03,  ...,  1.5993e-03,\n         -3.0556e-03, -1.3065e-03],\n        [-1.8291e-03,  5.7888e-04,  2.5635e-03,  ...,  7.0763e-04,\n         -7.0572e-04, -1.1578e-03],\n        [ 9.2220e-04,  3.5934e-03, -3.7193e-05,  ...,  2.6779e-03,\n         -2.5177e-04,  2.7351e-03],\n        ...,\n        [ 1.6899e-03, -2.1172e-03, -7.8869e-04,  ..., -7.1335e-04,\n          8.4305e-04, -2.3441e-03],\n        [-1.4362e-03, -4.1962e-05,  6.6566e-04,  ...,  4.7326e-04,\n         -7.3242e-04,  7.5388e-04],\n        [-1.2684e-03,  3.3665e-04, -5.2452e-04,  ...,  8.8882e-04,\n         -9.0694e-04, -2.0218e-03]])\nmodel.layers.9.self_attn.k_proj.base_layer.weight: tensor([[ 0.0057,  0.0066,  0.0075,  ...,  0.0057,  0.0020, -0.0010],\n        [ 0.0090, -0.0010,  0.0133,  ...,  0.0087,  0.0090,  0.0003],\n        [ 0.0050, -0.0099,  0.0066,  ..., -0.0023,  0.0050, -0.0006],\n        ...,\n        [-0.0078, -0.0018,  0.0271,  ..., -0.0062, -0.0135, -0.0194],\n        [ 0.0086,  0.0172, -0.0131,  ..., -0.0013,  0.0039, -0.0114],\n        [-0.0018, -0.0056,  0.0157,  ..., -0.0325, -0.0243,  0.0275]])\nmodel.layers.9.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0186,  0.0104,  0.0085,  ..., -0.0087,  0.0094, -0.0018],\n        [ 0.0182, -0.0060,  0.0051,  ...,  0.0052, -0.0067, -0.0059],\n        [ 0.0040, -0.0141,  0.0180,  ..., -0.0170, -0.0030,  0.0020],\n        ...,\n        [-0.0046, -0.0053,  0.0161,  ..., -0.0011, -0.0063,  0.0003],\n        [-0.0051,  0.0049,  0.0045,  ..., -0.0004,  0.0056,  0.0112],\n        [ 0.0030,  0.0004,  0.0110,  ..., -0.0169,  0.0025,  0.0019]])\nmodel.layers.9.self_attn.k_proj.lora_B.default.weight: tensor([[ 1.4238e-03, -1.6499e-03,  2.9635e-04,  ..., -4.0531e-05,\n          1.6203e-03,  9.9182e-05],\n        [ 3.1891e-03,  7.1812e-04, -1.7509e-03,  ...,  9.1600e-04,\n          3.0212e-03, -2.0123e-04],\n        [ 1.7462e-03,  9.9945e-04,  8.1539e-05,  ...,  3.6097e-04,\n          3.7098e-04, -1.2779e-04],\n        ...,\n        [ 3.1233e-04, -2.4533e-04,  2.7580e-03,  ..., -1.0033e-03,\n         -3.0098e-03, -3.3283e-04],\n        [ 1.1568e-03, -1.4839e-03, -2.8419e-04,  ...,  9.0694e-04,\n          1.9836e-03, -6.8760e-04],\n        [ 1.2369e-03,  1.6060e-03, -5.0354e-04,  ...,  2.1076e-03,\n          3.0136e-04, -1.5640e-04]])\nmodel.layers.9.self_attn.v_proj.base_layer.weight: tensor([[ 0.0156, -0.0127, -0.0123,  ..., -0.0064,  0.0021,  0.0073],\n        [-0.0013,  0.0079,  0.0063,  ..., -0.0084, -0.0041, -0.0022],\n        [-0.0170, -0.0044,  0.0047,  ...,  0.0008, -0.0076, -0.0055],\n        ...,\n        [-0.0033, -0.0015,  0.0002,  ..., -0.0050, -0.0193,  0.0166],\n        [ 0.0021, -0.0054,  0.0043,  ..., -0.0013,  0.0087, -0.0060],\n        [ 0.0002, -0.0022,  0.0003,  ..., -0.0001, -0.0020,  0.0078]])\nmodel.layers.9.self_attn.v_proj.lora_A.default.weight: tensor([[-0.0023, -0.0068, -0.0024,  ..., -0.0027,  0.0108,  0.0019],\n        [ 0.0056,  0.0040,  0.0016,  ...,  0.0110,  0.0083,  0.0001],\n        [ 0.0041, -0.0076,  0.0096,  ...,  0.0089,  0.0122,  0.0137],\n        ...,\n        [-0.0002, -0.0124,  0.0124,  ..., -0.0011,  0.0113,  0.0035],\n        [-0.0018,  0.0103, -0.0141,  ...,  0.0011, -0.0005,  0.0041],\n        [-0.0025,  0.0135, -0.0002,  ...,  0.0068,  0.0054, -0.0008]])\nmodel.layers.9.self_attn.v_proj.lora_B.default.weight: tensor([[-1.6594e-03, -1.4534e-03,  6.0797e-04,  ..., -1.3037e-03,\n          1.7300e-03,  1.0748e-03],\n        [ 1.8549e-03,  1.1768e-03,  1.3084e-03,  ...,  2.6474e-03,\n          8.8120e-04,  8.6784e-05],\n        [ 1.5640e-03,  6.5184e-04, -4.3869e-04,  ...,  8.7690e-04,\n         -7.5626e-04, -1.8177e-03],\n        ...,\n        [ 1.0405e-03,  1.8196e-03, -5.3787e-04,  ...,  7.8321e-05,\n         -5.1308e-04,  1.4439e-03],\n        [-4.9400e-04,  6.2943e-04,  1.7443e-03,  ..., -2.6703e-04,\n          7.7915e-04,  1.4791e-03],\n        [-8.4972e-04,  1.4544e-04, -1.7242e-03,  ...,  1.3571e-03,\n         -1.6270e-03,  2.3155e-03]])\nmodel.layers.9.self_attn.o_proj.base_layer.weight: tensor([[ 0.0032, -0.0042, -0.0253,  ...,  0.0010, -0.0004, -0.0043],\n        [-0.0080,  0.0007, -0.0146,  ...,  0.0081,  0.0039,  0.0053],\n        [ 0.0008,  0.0027,  0.0105,  ...,  0.0067,  0.0048,  0.0188],\n        ...,\n        [ 0.0044, -0.0036,  0.0061,  ..., -0.0018,  0.0071,  0.0025],\n        [-0.0037, -0.0145, -0.0126,  ..., -0.0053,  0.0078,  0.0031],\n        [-0.0039, -0.0027, -0.0035,  ...,  0.0005,  0.0047, -0.0004]])\nmodel.layers.9.self_attn.o_proj.lora_A.default.weight: tensor([[ 0.0039, -0.0088, -0.0033,  ..., -0.0033, -0.0163, -0.0099],\n        [-0.0109, -0.0178, -0.0094,  ...,  0.0062,  0.0082,  0.0057],\n        [ 0.0083,  0.0054,  0.0067,  ...,  0.0204,  0.0101,  0.0193],\n        ...,\n        [ 0.0041,  0.0129,  0.0024,  ..., -0.0047, -0.0010,  0.0091],\n        [-0.0117,  0.0011, -0.0041,  ...,  0.0085,  0.0011,  0.0107],\n        [ 0.0068,  0.0104,  0.0024,  ...,  0.0097,  0.0016,  0.0023]])\nmodel.layers.9.self_attn.o_proj.lora_B.default.weight: tensor([[-9.3603e-04, -9.3031e-04,  1.3628e-03,  ...,  3.8576e-04,\n          1.4830e-04, -8.6975e-04],\n        [-4.5717e-05,  1.6289e-03, -6.5613e-04,  ..., -5.0259e-04,\n         -2.2736e-03,  5.9414e-04],\n        [ 6.8474e-04, -6.7329e-04,  2.5463e-04,  ...,  1.5297e-03,\n          1.7815e-03,  6.9952e-04],\n        ...,\n        [-3.0022e-03,  2.0790e-04, -1.4830e-03,  ...,  1.6851e-03,\n         -1.1148e-03, -1.2560e-03],\n        [ 2.0199e-03, -4.1008e-05,  1.0357e-03,  ..., -6.9618e-05,\n          2.7962e-03,  2.5024e-03],\n        [ 3.5763e-05, -1.9245e-03, -7.6675e-04,  ...,  2.3365e-04,\n          6.1798e-04,  2.9831e-03]])\nmodel.layers.9.mlp.gate_proj.base_layer.weight: tensor([[-0.0028, -0.0061,  0.0012,  ...,  0.0003, -0.0002,  0.0111],\n        [ 0.0033,  0.0154,  0.0178,  ..., -0.0205,  0.0077, -0.0023],\n        [ 0.0006,  0.0005, -0.0060,  ..., -0.0058, -0.0057, -0.0034],\n        ...,\n        [ 0.0032, -0.0052,  0.0079,  ..., -0.0012, -0.0115,  0.0043],\n        [ 0.0068,  0.0006, -0.0064,  ...,  0.0046,  0.0040,  0.0097],\n        [ 0.0018, -0.0090, -0.0030,  ...,  0.0005,  0.0042, -0.0033]])\nmodel.layers.9.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0081,  0.0181, -0.0109,  ..., -0.0074, -0.0007,  0.0122],\n        [ 0.0074,  0.0208,  0.0195,  ...,  0.0102,  0.0140,  0.0162],\n        [ 0.0021,  0.0043, -0.0092,  ...,  0.0149,  0.0147,  0.0187],\n        ...,\n        [ 0.0050, -0.0130,  0.0096,  ...,  0.0044, -0.0098,  0.0056],\n        [ 0.0068,  0.0070,  0.0151,  ...,  0.0073,  0.0031,  0.0133],\n        [ 0.0057,  0.0110, -0.0008,  ..., -0.0069, -0.0088,  0.0005]])\nmodel.layers.9.mlp.gate_proj.lora_B.default.weight: tensor([[-1.7405e-03, -1.7214e-04,  1.7548e-04,  ...,  1.8635e-03,\n         -7.1526e-07, -6.1274e-04],\n        [-8.9645e-05, -2.8610e-06,  7.2289e-04,  ...,  9.2936e-04,\n         -1.6661e-03,  2.1255e-04],\n        [ 2.8076e-03,  7.4148e-04,  1.3876e-03,  ..., -1.0281e-03,\n         -1.0834e-03, -9.7322e-04],\n        ...,\n        [-2.1744e-04, -5.0068e-05, -1.1959e-03,  ...,  1.4067e-04,\n          3.8719e-04,  9.5844e-04],\n        [-3.3379e-04, -1.3599e-03,  3.1519e-04,  ...,  2.5215e-03,\n          1.2922e-03, -2.6441e-04],\n        [-2.7924e-03, -1.6775e-03,  2.3651e-03,  ...,  4.0531e-04,\n          5.2357e-04,  1.5345e-03]])\nmodel.layers.9.mlp.up_proj.base_layer.weight: tensor([[ 1.1063e-03,  8.5449e-03, -9.7046e-03,  ...,  3.5248e-03,\n          7.0801e-03,  4.2200e-05],\n        [ 5.1117e-04,  4.3030e-03,  4.0588e-03,  ...,  9.4604e-03,\n          3.2196e-03,  5.5237e-03],\n        [ 3.8719e-04,  1.4114e-03,  1.0925e-02,  ...,  1.4496e-03,\n         -5.4932e-03,  6.4468e-04],\n        ...,\n        [ 1.9150e-03,  3.7231e-03,  9.4604e-03,  ..., -1.4221e-02,\n          5.4626e-03,  1.9741e-04],\n        [-8.9111e-03, -3.0060e-03,  5.7983e-03,  ...,  9.0332e-03,\n          5.8365e-04,  3.3112e-03],\n        [ 4.3640e-03,  7.2327e-03, -6.7139e-03,  ..., -9.5825e-03,\n         -6.9275e-03, -8.3008e-03]])\nmodel.layers.9.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0113,  0.0067, -0.0042,  ...,  0.0100,  0.0023,  0.0072],\n        [ 0.0105, -0.0093, -0.0046,  ..., -0.0023, -0.0122, -0.0071],\n        [-0.0170,  0.0073, -0.0211,  ...,  0.0055, -0.0095, -0.0073],\n        ...,\n        [-0.0043, -0.0009, -0.0090,  ...,  0.0019, -0.0016,  0.0011],\n        [-0.0100, -0.0003, -0.0034,  ...,  0.0144, -0.0038,  0.0066],\n        [ 0.0126, -0.0044,  0.0167,  ..., -0.0009,  0.0049, -0.0048]])\nmodel.layers.9.mlp.up_proj.lora_B.default.weight: tensor([[-2.9182e-03, -1.5602e-03, -1.6651e-03,  ...,  2.6035e-04,\n          1.8549e-04,  1.3695e-03],\n        [ 1.1444e-05,  2.2984e-03, -9.4700e-04,  ...,  8.3733e-04,\n          4.8447e-04, -6.1321e-04],\n        [ 1.4019e-03,  1.8787e-03, -6.0368e-04,  ..., -7.1383e-04,\n         -1.4758e-04, -8.6164e-04],\n        ...,\n        [-1.4820e-03, -2.5139e-03,  7.8821e-04,  ..., -1.7977e-03,\n          9.6798e-04, -1.2770e-03],\n        [-1.0195e-03,  2.6393e-04, -1.2951e-03,  ...,  9.0981e-04,\n          1.2493e-04,  1.0462e-03],\n        [ 7.5579e-04,  2.8954e-03, -2.9144e-03,  ...,  2.0485e-03,\n          3.0017e-04,  2.7657e-03]])\nmodel.layers.9.mlp.down_proj.base_layer.weight: tensor([[-0.0086,  0.0053,  0.0023,  ...,  0.0070, -0.0014,  0.0036],\n        [ 0.0126,  0.0028, -0.0078,  ..., -0.0003, -0.0030,  0.0068],\n        [-0.0070,  0.0053,  0.0061,  ...,  0.0014,  0.0072, -0.0107],\n        ...,\n        [-0.0029, -0.0043, -0.0028,  ..., -0.0026,  0.0062,  0.0028],\n        [ 0.0078,  0.0067, -0.0056,  ..., -0.0086, -0.0024, -0.0034],\n        [-0.0125,  0.0030,  0.0016,  ..., -0.0001,  0.0059, -0.0076]])\nmodel.layers.9.mlp.down_proj.lora_A.default.weight: tensor([[ 2.4643e-03, -5.2986e-03,  1.6479e-03,  ..., -2.3403e-03,\n         -2.8610e-03,  2.1820e-03],\n        [-2.7504e-03,  1.8578e-03, -6.7806e-04,  ...,  1.9665e-03,\n          4.4556e-03,  3.5744e-03],\n        [ 2.0332e-03,  8.6594e-04, -2.3232e-03,  ...,  2.7065e-03,\n         -4.6387e-03,  5.6267e-04],\n        ...,\n        [-1.5287e-03, -1.0061e-03, -6.7444e-03,  ...,  4.1962e-05,\n         -1.0042e-03, -1.1140e-04],\n        [ 8.6212e-04, -2.8439e-03, -6.7902e-04,  ..., -3.9711e-03,\n         -3.4561e-03, -1.8635e-03],\n        [-2.8610e-04, -4.6539e-04,  3.9902e-03,  ...,  8.8501e-04,\n         -4.2992e-03, -4.1504e-03]])\nmodel.layers.9.mlp.down_proj.lora_B.default.weight: tensor([[ 1.0090e-03,  1.9789e-05, -1.3981e-03,  ...,  2.7180e-05,\n          1.0414e-03, -1.5564e-03],\n        [ 7.7963e-04, -2.6016e-03, -4.6539e-04,  ..., -3.7622e-04,\n         -5.5981e-04, -4.1747e-04],\n        [-4.0340e-04,  1.0500e-03,  1.1663e-03,  ...,  8.9598e-04,\n          4.0054e-04,  6.2752e-04],\n        ...,\n        [ 1.5907e-03, -4.7445e-04,  1.9217e-03,  ...,  1.9159e-03,\n         -8.5354e-04,  2.3613e-03],\n        [ 3.9959e-04,  1.6670e-03, -4.3488e-04,  ..., -1.4830e-03,\n          1.6956e-03, -2.4757e-03],\n        [ 1.0576e-03,  1.7662e-03, -5.0306e-05,  ..., -8.7833e-04,\n          2.7866e-03,  9.5081e-04]])\nmodel.layers.9.input_layernorm.weight: tensor([1.2656, 0.5898, 1.4453,  ..., 0.4551, 2.4844, 0.0098])\nmodel.layers.9.post_attention_layernorm.weight: tensor([2.5625, 0.9453, 2.6406,  ..., 1.2500, 3.3750, 0.8711])\nmodel.layers.10.self_attn.q_proj.base_layer.weight: tensor([[-0.0005,  0.0032,  0.0005,  ..., -0.0015, -0.0002, -0.0002],\n        [-0.0004,  0.0007,  0.0010,  ..., -0.0018, -0.0004,  0.0010],\n        [-0.0022, -0.0007,  0.0031,  ...,  0.0007, -0.0012,  0.0008],\n        ...,\n        [-0.0049, -0.0165,  0.0104,  ...,  0.0332,  0.0036, -0.0204],\n        [ 0.0022,  0.0164, -0.0013,  ..., -0.0027, -0.0107, -0.0091],\n        [-0.0050, -0.0056,  0.0027,  ..., -0.0101,  0.0266,  0.0221]])\nmodel.layers.10.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0117, -0.0041, -0.0035,  ...,  0.0216, -0.0019,  0.0049],\n        [ 0.0047, -0.0042,  0.0047,  ...,  0.0089,  0.0080,  0.0056],\n        [ 0.0023,  0.0030,  0.0142,  ..., -0.0030,  0.0152, -0.0011],\n        ...,\n        [ 0.0097,  0.0080,  0.0024,  ..., -0.0039,  0.0023,  0.0089],\n        [-0.0040,  0.0125, -0.0091,  ..., -0.0108, -0.0005, -0.0030],\n        [-0.0090, -0.0106, -0.0064,  ...,  0.0138,  0.0036,  0.0069]])\nmodel.layers.10.self_attn.q_proj.lora_B.default.weight: tensor([[ 8.0442e-04, -2.5415e-04,  1.6241e-03,  ...,  1.6184e-03,\n          5.0116e-04,  2.6917e-04],\n        [ 1.5259e-04, -1.1921e-03,  4.5300e-04,  ..., -7.8297e-04,\n          1.4706e-03, -2.6855e-03],\n        [-5.1880e-04, -1.3828e-03, -6.7616e-04,  ...,  8.9455e-04,\n         -2.6855e-03,  1.1187e-03],\n        ...,\n        [-1.0700e-03,  9.1982e-04, -1.2236e-03,  ...,  1.4229e-03,\n         -1.4973e-03,  1.0920e-03],\n        [ 2.0752e-03,  3.3379e-06,  6.1703e-04,  ...,  6.4659e-04,\n          5.3310e-04, -6.3324e-04],\n        [ 1.6270e-03,  1.8530e-03,  1.0109e-03,  ...,  1.0262e-03,\n          9.0599e-04,  1.1234e-03]])\nmodel.layers.10.self_attn.k_proj.base_layer.weight: tensor([[ 2.6512e-04, -4.2725e-03,  5.0049e-03,  ...,  5.0049e-03,\n         -4.4556e-03, -5.1575e-03],\n        [ 2.8229e-03,  6.8970e-03, -6.3477e-03,  ..., -1.0437e-02,\n          1.5640e-03,  3.0212e-03],\n        [ 3.3875e-03,  2.4109e-03,  1.0742e-02,  ...,  9.3994e-03,\n          3.6621e-03, -1.5616e-05],\n        ...,\n        [ 2.8381e-03, -6.0791e-02, -1.6113e-02,  ...,  4.1016e-02,\n         -2.7161e-03, -9.7656e-03],\n        [-7.6294e-03,  3.2959e-02,  1.2360e-03,  ..., -1.5137e-02,\n          9.3460e-04, -3.1982e-02],\n        [-1.2634e-02,  9.1553e-03, -3.5477e-04,  ..., -2.9541e-02,\n          5.4626e-03,  2.6367e-02]])\nmodel.layers.10.self_attn.k_proj.lora_A.default.weight: tensor([[-0.0042, -0.0046, -0.0181,  ...,  0.0006,  0.0010, -0.0065],\n        [-0.0015,  0.0062, -0.0042,  ..., -0.0038,  0.0018, -0.0113],\n        [ 0.0040, -0.0058, -0.0013,  ...,  0.0092, -0.0078, -0.0010],\n        ...,\n        [-0.0060, -0.0072, -0.0171,  ...,  0.0117,  0.0007,  0.0086],\n        [-0.0103,  0.0107, -0.0191,  ...,  0.0029, -0.0185,  0.0018],\n        [-0.0165, -0.0037, -0.0172,  ...,  0.0186,  0.0067,  0.0202]])\nmodel.layers.10.self_attn.k_proj.lora_B.default.weight: tensor([[-2.6779e-03,  3.2020e-04,  1.5717e-03,  ..., -1.1444e-04,\n          1.4706e-03,  1.3142e-03],\n        [-9.2268e-04, -1.1492e-04,  4.8113e-04,  ..., -2.0180e-03,\n          1.5583e-03, -3.9101e-05],\n        [-1.6975e-03, -3.0756e-04,  5.0306e-04,  ..., -1.0109e-03,\n          1.9207e-03,  1.5297e-03],\n        ...,\n        [ 1.2074e-03, -5.6648e-04, -3.9697e-04,  ..., -2.1629e-03,\n          2.6584e-04,  1.3218e-03],\n        [-5.1498e-05,  8.5115e-05, -1.2226e-03,  ..., -4.3058e-04,\n          7.5960e-04,  2.5988e-04],\n        [ 1.0500e-03,  2.3575e-03,  9.7084e-04,  ..., -2.4796e-05,\n         -9.2316e-04, -2.5368e-03]])\nmodel.layers.10.self_attn.v_proj.base_layer.weight: tensor([[-6.4087e-03, -2.3651e-03,  7.4387e-04,  ...,  8.4839e-03,\n          1.0834e-03,  8.0566e-03],\n        [ 1.0254e-02,  4.3030e-03,  3.0518e-03,  ..., -5.5237e-03,\n         -2.8687e-03,  7.2098e-04],\n        [-5.3272e-07, -4.3640e-03,  9.6436e-03,  ..., -1.1169e-02,\n          2.5940e-03,  5.2795e-03],\n        ...,\n        [ 1.2390e-02,  6.4697e-03,  2.4719e-03,  ..., -6.1951e-03,\n          5.6152e-03, -9.4604e-04],\n        [ 1.4526e-02, -1.5991e-02,  1.0132e-02,  ...,  6.7139e-03,\n         -1.8311e-03, -1.8158e-03],\n        [ 2.0599e-03,  1.6556e-03, -4.1580e-04,  ...,  3.5400e-03,\n          4.0588e-03,  5.6458e-03]])\nmodel.layers.10.self_attn.v_proj.lora_A.default.weight: tensor([[ 0.0008, -0.0012, -0.0140,  ...,  0.0090,  0.0026, -0.0005],\n        [ 0.0027, -0.0046,  0.0140,  ...,  0.0068,  0.0023,  0.0023],\n        [ 0.0025,  0.0027,  0.0173,  ...,  0.0057,  0.0198,  0.0109],\n        ...,\n        [ 0.0170,  0.0034,  0.0161,  ..., -0.0050, -0.0041, -0.0120],\n        [-0.0028, -0.0032,  0.0101,  ...,  0.0074,  0.0003,  0.0045],\n        [-0.0016, -0.0029, -0.0155,  ..., -0.0106, -0.0095, -0.0067]])\nmodel.layers.10.self_attn.v_proj.lora_B.default.weight: tensor([[ 2.3079e-04,  2.0638e-03, -1.2560e-03,  ...,  3.4857e-04,\n         -1.6918e-03, -5.1117e-04],\n        [-2.0313e-03,  9.2602e-04,  2.0027e-05,  ..., -2.4509e-04,\n          4.9734e-04, -2.1877e-03],\n        [ 9.6893e-04,  9.1171e-04, -2.7180e-03,  ..., -1.4973e-03,\n         -1.5545e-04, -4.2725e-04],\n        ...,\n        [-4.3678e-04,  1.0834e-03, -6.6662e-04,  ...,  4.1723e-04,\n         -5.7077e-04, -1.3266e-03],\n        [-4.6849e-04, -2.2392e-03,  2.1782e-03,  ...,  2.8305e-03,\n          4.1485e-04,  5.3406e-04],\n        [ 1.3084e-03,  1.8311e-03, -2.3861e-03,  ...,  2.0676e-03,\n          4.9686e-04, -1.8883e-04]])\nmodel.layers.10.self_attn.o_proj.base_layer.weight: tensor([[-0.0129, -0.0245,  0.0025,  ..., -0.0028, -0.0090,  0.0058],\n        [ 0.0051, -0.0015, -0.0030,  ...,  0.0160,  0.0012,  0.0099],\n        [ 0.0044, -0.0132, -0.0051,  ...,  0.0210, -0.0085, -0.0101],\n        ...,\n        [ 0.0171,  0.0066,  0.0062,  ..., -0.0132, -0.0015,  0.0021],\n        [ 0.0077,  0.0001, -0.0135,  ...,  0.0031, -0.0075,  0.0172],\n        [-0.0001,  0.0085, -0.0018,  ..., -0.0043,  0.0002,  0.0023]])\nmodel.layers.10.self_attn.o_proj.lora_A.default.weight: tensor([[ 0.0153,  0.0113,  0.0086,  ..., -0.0085, -0.0028,  0.0081],\n        [-0.0061,  0.0002, -0.0054,  ..., -0.0011,  0.0004,  0.0052],\n        [ 0.0005,  0.0050,  0.0123,  ..., -0.0025,  0.0118,  0.0019],\n        ...,\n        [-0.0190, -0.0154, -0.0134,  ...,  0.0006,  0.0077, -0.0025],\n        [ 0.0048,  0.0081, -0.0028,  ...,  0.0019, -0.0132, -0.0010],\n        [-0.0055,  0.0048,  0.0138,  ...,  0.0130,  0.0080,  0.0083]])\nmodel.layers.10.self_attn.o_proj.lora_B.default.weight: tensor([[-1.3199e-03,  4.7016e-04,  2.5253e-03,  ..., -3.4523e-04,\n          7.3719e-04, -8.4972e-04],\n        [ 2.0294e-03,  1.5774e-03, -5.5504e-04,  ...,  2.5415e-04,\n         -2.0409e-03, -2.2087e-03],\n        [ 1.1444e-05,  3.4332e-05,  1.7738e-03,  ...,  2.0313e-03,\n         -4.2057e-04, -6.0701e-04],\n        ...,\n        [-5.0020e-04,  2.5597e-03,  4.3058e-04,  ...,  2.0657e-03,\n         -6.0654e-04, -1.2417e-03],\n        [-1.1377e-03,  6.8760e-04, -1.4915e-03,  ..., -1.0691e-03,\n          8.2588e-04,  1.4687e-03],\n        [-5.9795e-04,  1.0757e-03, -1.0014e-04,  ..., -1.2293e-03,\n          2.7122e-03,  2.2106e-03]])\nmodel.layers.10.mlp.gate_proj.base_layer.weight: tensor([[-5.9204e-03,  6.0120e-03,  3.2043e-03,  ..., -1.5163e-04,\n          5.3406e-04, -3.3722e-03],\n        [ 2.8381e-03,  6.3705e-04, -3.9101e-04,  ..., -3.6011e-03,\n          5.0659e-03,  1.0071e-03],\n        [ 6.5002e-03,  3.2654e-03, -6.8970e-03,  ..., -8.5068e-04,\n         -2.2583e-03, -4.4556e-03],\n        ...,\n        [ 3.0670e-03,  8.3923e-05,  8.0490e-04,  ..., -8.4839e-03,\n         -1.2894e-03, -7.8735e-03],\n        [ 7.3242e-03,  1.9150e-03, -1.4725e-03,  ...,  5.6763e-03,\n         -7.1106e-03,  1.8311e-03],\n        [ 3.4637e-03,  2.5940e-03,  2.2430e-03,  ...,  6.5002e-03,\n         -2.3651e-03,  1.3885e-03]])\nmodel.layers.10.mlp.gate_proj.lora_A.default.weight: tensor([[ 1.5442e-02,  2.5101e-03,  7.5150e-03,  ...,  1.3489e-02,\n          1.0529e-03, -3.4599e-03],\n        [-1.0880e-02, -2.0172e-02,  3.2387e-03,  ...,  1.8295e-02,\n         -1.6937e-02,  1.7517e-02],\n        [ 6.3782e-03,  1.3931e-02,  1.1360e-02,  ...,  1.2100e-02,\n         -2.0409e-03,  1.1917e-02],\n        ...,\n        [ 6.3248e-03, -1.0948e-03,  7.6294e-05,  ...,  1.3771e-02,\n         -8.3389e-03, -5.1880e-03],\n        [-1.6861e-02, -2.0294e-02, -9.4223e-03,  ...,  1.1932e-02,\n         -9.1858e-03, -4.6616e-03],\n        [ 1.6899e-03, -1.1848e-02,  1.5076e-02,  ..., -6.3210e-03,\n         -5.0049e-03, -1.1345e-02]])\nmodel.layers.10.mlp.gate_proj.lora_B.default.weight: tensor([[-2.3246e-04,  1.7662e-03,  1.5240e-03,  ...,  7.5483e-04,\n         -1.9493e-03,  5.8317e-04],\n        [-2.1210e-03, -4.5824e-04, -9.3079e-04,  ..., -1.5173e-03,\n          6.0081e-04,  2.8763e-03],\n        [ 3.0375e-04,  1.7576e-03, -1.4267e-03,  ...,  3.3379e-05,\n          1.6642e-03, -4.5514e-04],\n        ...,\n        [ 1.2074e-03, -1.3418e-03,  7.0715e-04,  ...,  2.0905e-03,\n         -1.0376e-03, -1.6003e-03],\n        [-2.1935e-03,  4.6062e-04, -2.4223e-03,  ..., -1.6317e-03,\n         -4.4894e-04,  5.0068e-05],\n        [-2.1896e-03,  2.3727e-03,  6.4373e-04,  ..., -4.0627e-04,\n          4.3321e-04, -4.9686e-04]])\nmodel.layers.10.mlp.up_proj.base_layer.weight: tensor([[ 0.0042, -0.0013, -0.0044,  ..., -0.0018,  0.0043,  0.0022],\n        [-0.0046,  0.0070,  0.0007,  ..., -0.0104,  0.0013,  0.0096],\n        [-0.0036, -0.0045,  0.0058,  ...,  0.0022, -0.0073,  0.0016],\n        ...,\n        [ 0.0084, -0.0032,  0.0045,  ...,  0.0131,  0.0115, -0.0081],\n        [ 0.0012, -0.0046,  0.0006,  ...,  0.0017,  0.0131,  0.0026],\n        [ 0.0007,  0.0002,  0.0004,  ..., -0.0001, -0.0024,  0.0029]])\nmodel.layers.10.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0065, -0.0197,  0.0057,  ...,  0.0147, -0.0117,  0.0114],\n        [ 0.0014, -0.0064,  0.0139,  ..., -0.0099, -0.0058,  0.0012],\n        [ 0.0075,  0.0074,  0.0131,  ...,  0.0031, -0.0031,  0.0009],\n        ...,\n        [-0.0023,  0.0094,  0.0065,  ..., -0.0112, -0.0013,  0.0002],\n        [ 0.0144,  0.0044,  0.0076,  ..., -0.0040, -0.0032,  0.0061],\n        [-0.0045,  0.0129,  0.0059,  ...,  0.0026, -0.0143,  0.0100]])\nmodel.layers.10.mlp.up_proj.lora_B.default.weight: tensor([[ 6.8283e-04, -1.2341e-03, -7.4387e-05,  ..., -9.4461e-04,\n          1.2140e-03, -2.1667e-03],\n        [-2.9325e-04,  3.4165e-04,  2.5539e-03,  ...,  6.6710e-04,\n         -8.5640e-04, -1.3256e-03],\n        [-6.5422e-04, -1.0729e-04, -1.7757e-03,  ...,  9.4831e-05,\n         -2.9945e-04, -1.4029e-03],\n        ...,\n        [-2.4052e-03,  1.1539e-03, -8.3542e-04,  ..., -2.8095e-03,\n         -2.6379e-03, -1.5087e-03],\n        [-1.0004e-03, -1.7776e-03, -3.5739e-04,  ..., -3.4618e-04,\n         -7.8487e-04, -2.2163e-03],\n        [-2.1591e-03,  9.9087e-04, -1.6093e-04,  ...,  6.6805e-04,\n         -1.2016e-03, -2.2583e-03]])\nmodel.layers.10.mlp.down_proj.base_layer.weight: tensor([[-0.0031, -0.0039,  0.0060,  ...,  0.0110,  0.0042,  0.0022],\n        [-0.0013,  0.0069, -0.0037,  ...,  0.0020, -0.0007, -0.0011],\n        [ 0.0061,  0.0036,  0.0135,  ...,  0.0004,  0.0016, -0.0023],\n        ...,\n        [ 0.0063, -0.0014, -0.0060,  ...,  0.0022,  0.0007, -0.0026],\n        [-0.0065,  0.0054, -0.0002,  ...,  0.0106,  0.0038, -0.0020],\n        [-0.0029,  0.0101,  0.0053,  ..., -0.0038,  0.0019,  0.0093]])\nmodel.layers.10.mlp.down_proj.lora_A.default.weight: tensor([[-8.1787e-03, -6.4373e-05,  9.9182e-05,  ...,  7.3624e-04,\n          5.1155e-03, -4.2458e-03],\n        [ 3.6526e-04, -5.8899e-03, -2.8801e-03,  ..., -7.1640e-03,\n         -1.9073e-03, -6.2943e-03],\n        [ 3.2768e-03, -7.0419e-03,  1.1072e-03,  ..., -1.5621e-03,\n          4.0283e-03, -2.3575e-03],\n        ...,\n        [ 1.1978e-03, -2.8229e-04, -5.6038e-03,  ..., -4.2305e-03,\n         -1.8730e-03, -9.6464e-04],\n        [-5.8594e-03,  5.5122e-04, -2.6093e-03,  ...,  6.1874e-03,\n         -3.7689e-03, -1.7567e-03],\n        [-2.7733e-03, -6.9122e-03, -4.3602e-03,  ...,  5.8517e-03,\n          1.7796e-03,  8.4400e-04]])\nmodel.layers.10.mlp.down_proj.lora_B.default.weight: tensor([[-0.0006,  0.0010,  0.0007,  ...,  0.0011,  0.0022,  0.0020],\n        [ 0.0007, -0.0007, -0.0002,  ...,  0.0002,  0.0020,  0.0012],\n        [ 0.0033, -0.0002, -0.0006,  ..., -0.0015, -0.0015, -0.0003],\n        ...,\n        [ 0.0006, -0.0006,  0.0005,  ..., -0.0017,  0.0001,  0.0017],\n        [-0.0009,  0.0017,  0.0005,  ...,  0.0001,  0.0007, -0.0015],\n        [ 0.0007, -0.0013, -0.0020,  ..., -0.0005, -0.0015, -0.0008]])\nmodel.layers.10.input_layernorm.weight: tensor([1.6250, 0.8828, 1.8516,  ..., 0.8242, 3.1406, 0.2422])\nmodel.layers.10.post_attention_layernorm.weight: tensor([2.6094, 1.1172, 2.6406,  ..., 1.4531, 3.1719, 1.1406])\nmodel.layers.11.self_attn.q_proj.base_layer.weight: tensor([[ 7.0496e-03, -6.5918e-03,  6.5804e-05,  ...,  4.2419e-03,\n         -1.8082e-03, -3.9368e-03],\n        [ 7.7820e-04,  4.3335e-03, -1.5640e-03,  ..., -1.9150e-03,\n         -7.6675e-04, -1.8387e-03],\n        [ 3.2425e-04,  3.9368e-03, -2.3041e-03,  ..., -4.4441e-04,\n          7.2021e-03,  2.2125e-04],\n        ...,\n        [ 3.9673e-03, -1.1520e-03,  2.9907e-03,  ...,  3.3417e-03,\n          4.2114e-03,  8.5449e-03],\n        [-3.8452e-03,  2.4986e-04,  2.0905e-03,  ..., -2.6398e-03,\n          3.9062e-03, -1.0376e-03],\n        [-7.8201e-04,  1.3351e-03, -3.9062e-03,  ..., -2.2793e-04,\n         -6.7749e-03, -5.8289e-03]])\nmodel.layers.11.self_attn.q_proj.lora_A.default.weight: tensor([[-6.2065e-03, -6.1874e-03, -3.5629e-03,  ..., -1.7578e-02,\n          9.2468e-03, -4.0817e-03],\n        [ 1.0017e-02, -2.9488e-03, -3.4180e-03,  ...,  1.5312e-02,\n          1.0765e-02,  1.1444e-02],\n        [ 3.5896e-03,  7.2556e-03, -1.5213e-02,  ...,  2.5177e-03,\n          6.4087e-03, -1.6785e-04],\n        ...,\n        [-1.1772e-02,  4.5662e-03, -1.8051e-02,  ..., -1.0666e-02,\n         -1.9073e-02,  1.9836e-04],\n        [-6.3705e-03, -2.8954e-03,  4.9877e-04,  ...,  1.9440e-02,\n         -1.5259e-05,  6.2332e-03],\n        [-1.0887e-02,  3.8834e-03,  5.7831e-03,  ..., -1.4496e-02,\n          9.3079e-03, -8.1787e-03]])\nmodel.layers.11.self_attn.q_proj.lora_B.default.weight: tensor([[-0.0020,  0.0011,  0.0021,  ..., -0.0021,  0.0004, -0.0027],\n        [-0.0037,  0.0006, -0.0018,  ..., -0.0017,  0.0021, -0.0015],\n        [ 0.0011, -0.0009,  0.0018,  ...,  0.0011, -0.0008,  0.0003],\n        ...,\n        [-0.0021, -0.0015,  0.0011,  ..., -0.0010,  0.0013, -0.0016],\n        [ 0.0004,  0.0022,  0.0030,  ...,  0.0007, -0.0011, -0.0016],\n        [ 0.0009,  0.0023,  0.0011,  ...,  0.0022, -0.0017,  0.0001]])\nmodel.layers.11.self_attn.k_proj.base_layer.weight: tensor([[ 0.0046,  0.0061,  0.0003,  ..., -0.0017,  0.0036,  0.0018],\n        [ 0.0026, -0.0045, -0.0060,  ..., -0.0015,  0.0037, -0.0019],\n        [ 0.0053,  0.0039,  0.0011,  ...,  0.0015,  0.0041,  0.0020],\n        ...,\n        [-0.0042,  0.0091, -0.0096,  ..., -0.0024,  0.0113, -0.0070],\n        [ 0.0031,  0.0148,  0.0200,  ...,  0.0129,  0.0003, -0.0033],\n        [ 0.0001, -0.0288, -0.0126,  ...,  0.0210,  0.0029, -0.0067]])\nmodel.layers.11.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0086,  0.0086, -0.0024,  ...,  0.0051, -0.0004, -0.0087],\n        [-0.0010, -0.0095,  0.0014,  ..., -0.0073, -0.0172, -0.0109],\n        [-0.0044,  0.0032,  0.0028,  ..., -0.0047, -0.0036, -0.0103],\n        ...,\n        [-0.0020, -0.0031, -0.0064,  ...,  0.0033, -0.0109, -0.0073],\n        [-0.0147,  0.0036, -0.0026,  ..., -0.0026,  0.0052, -0.0044],\n        [-0.0054,  0.0088, -0.0032,  ...,  0.0007,  0.0144, -0.0090]])\nmodel.layers.11.self_attn.k_proj.lora_B.default.weight: tensor([[ 2.2354e-03,  5.7554e-04,  4.5371e-04,  ...,  2.9221e-03,\n         -1.1778e-03, -4.4155e-04],\n        [-1.5936e-03, -1.2569e-03, -1.7643e-03,  ..., -2.8000e-03,\n          8.9550e-04, -2.6283e-03],\n        [-6.8808e-04, -4.4441e-04, -7.1096e-04,  ..., -1.4086e-03,\n         -1.4420e-03, -2.4300e-03],\n        ...,\n        [-6.0177e-04,  7.3528e-04, -4.6468e-04,  ..., -6.4230e-04,\n         -1.7262e-03, -6.1321e-04],\n        [-2.7046e-03,  2.5558e-04, -1.1215e-03,  ...,  4.8161e-05,\n         -2.3079e-04,  8.5354e-05],\n        [ 2.0676e-03, -1.3638e-04, -6.5136e-04,  ...,  4.9639e-04,\n          6.4516e-04, -2.6093e-03]])\nmodel.layers.11.self_attn.v_proj.base_layer.weight: tensor([[-0.0160, -0.0055,  0.0057,  ...,  0.0105,  0.0069,  0.0028],\n        [-0.0007,  0.0074,  0.0060,  ...,  0.0083,  0.0101,  0.0010],\n        [-0.0052, -0.0008, -0.0025,  ...,  0.0001,  0.0070,  0.0024],\n        ...,\n        [ 0.0026, -0.0060,  0.0021,  ..., -0.0134,  0.0097, -0.0050],\n        [-0.0093, -0.0028,  0.0111,  ...,  0.0010, -0.0077, -0.0017],\n        [ 0.0011,  0.0006,  0.0066,  ...,  0.0168, -0.0011,  0.0032]])\nmodel.layers.11.self_attn.v_proj.lora_A.default.weight: tensor([[ 4.5776e-05, -1.3786e-02,  1.7853e-03,  ..., -4.6349e-03,\n          4.6082e-03,  2.9621e-03],\n        [ 1.4290e-02, -6.7177e-03,  1.0498e-02,  ..., -1.2138e-02,\n         -1.1246e-02, -1.0605e-02],\n        [-6.9046e-03, -1.9531e-02, -1.1383e-02,  ...,  2.9755e-04,\n         -6.6528e-03,  7.4081e-03],\n        ...,\n        [-4.0169e-03,  6.7291e-03, -2.7695e-03,  ..., -4.1428e-03,\n          4.6577e-03,  7.3090e-03],\n        [ 4.3221e-03,  1.2657e-02,  7.4768e-04,  ...,  1.1688e-02,\n         -5.2757e-03,  4.1008e-03],\n        [ 2.0332e-03, -9.3002e-03, -4.7302e-04,  ...,  1.5930e-02,\n         -9.8114e-03, -2.3575e-03]])\nmodel.layers.11.self_attn.v_proj.lora_B.default.weight: tensor([[ 2.3365e-04,  4.3106e-04,  5.2547e-04,  ..., -2.0142e-03,\n          4.3249e-04, -1.1177e-03],\n        [ 1.9512e-03, -1.7958e-03, -1.5049e-03,  ...,  2.2564e-03,\n         -1.1539e-04,  1.5717e-03],\n        [ 1.2436e-03,  1.0014e-03, -3.1948e-04,  ...,  7.8583e-04,\n         -8.3590e-04,  1.8632e-04],\n        ...,\n        [ 1.1559e-03,  9.0981e-04,  8.4782e-04,  ..., -6.2943e-04,\n          1.6284e-04,  7.6866e-04],\n        [-1.9569e-03,  8.8501e-04, -4.6325e-04,  ..., -6.5374e-04,\n          5.9605e-04, -2.8610e-03],\n        [-5.4836e-05,  1.3962e-03,  1.2856e-03,  ..., -1.0052e-03,\n         -4.1199e-04,  2.6569e-03]])\nmodel.layers.11.self_attn.o_proj.base_layer.weight: tensor([[-8.9111e-03, -1.6724e-02, -9.4604e-03,  ..., -1.3794e-02,\n         -9.3994e-03,  6.1798e-04],\n        [-1.4496e-03,  2.5177e-04, -1.4771e-02,  ..., -9.8419e-04,\n          6.1798e-04,  8.6060e-03],\n        [ 8.9722e-03,  5.2795e-03,  1.5442e-02,  ...,  5.4836e-05,\n          1.3611e-02,  2.8381e-03],\n        ...,\n        [ 1.0681e-02,  1.1475e-02,  3.6926e-03,  ..., -4.0283e-03,\n          2.7771e-03,  1.1597e-02],\n        [ 2.5368e-04, -1.2024e-02,  8.0566e-03,  ...,  8.3008e-03,\n         -4.0588e-03, -4.9744e-03],\n        [-3.2043e-03, -2.2736e-03, -1.4801e-03,  ..., -9.5825e-03,\n          5.3024e-04,  1.4954e-02]])\nmodel.layers.11.self_attn.o_proj.lora_A.default.weight: tensor([[ 0.0147,  0.0165, -0.0015,  ..., -0.0041, -0.0138, -0.0078],\n        [-0.0031, -0.0119,  0.0010,  ..., -0.0052, -0.0004, -0.0074],\n        [-0.0066, -0.0054, -0.0107,  ..., -0.0042, -0.0025,  0.0060],\n        ...,\n        [-0.0031, -0.0098, -0.0059,  ...,  0.0115, -0.0049,  0.0192],\n        [-0.0063, -0.0017,  0.0033,  ..., -0.0052,  0.0087, -0.0172],\n        [ 0.0011,  0.0005,  0.0074,  ...,  0.0038, -0.0005, -0.0010]])\nmodel.layers.11.self_attn.o_proj.lora_B.default.weight: tensor([[ 1.2150e-03,  5.9128e-04,  1.7738e-03,  ..., -2.4223e-04,\n          1.3828e-03, -1.9097e-04],\n        [ 1.6766e-03, -2.1572e-03, -3.9721e-04,  ...,  3.7289e-04,\n          1.6766e-03, -1.6241e-03],\n        [-1.6451e-03,  8.7690e-04,  5.4550e-04,  ...,  2.1439e-03,\n         -6.7234e-05, -3.1137e-04],\n        ...,\n        [ 9.7847e-04, -5.9700e-04,  8.1635e-04,  ...,  2.0866e-03,\n          4.8494e-04, -1.0595e-03],\n        [-8.9288e-05, -8.1539e-04, -1.1597e-03,  ...,  7.3910e-05,\n          8.2207e-04, -1.1425e-03],\n        [-1.7586e-03,  2.0218e-03,  8.4782e-04,  ..., -2.7180e-04,\n          1.9836e-03,  3.2282e-04]])\nmodel.layers.11.mlp.gate_proj.base_layer.weight: tensor([[ 0.0102,  0.0038,  0.0062,  ...,  0.0014, -0.0027, -0.0097],\n        [ 0.0001,  0.0015, -0.0040,  ..., -0.0020, -0.0101, -0.0123],\n        [-0.0029, -0.0167,  0.0014,  ..., -0.0017, -0.0004, -0.0116],\n        ...,\n        [-0.0041,  0.0038, -0.0023,  ...,  0.0065,  0.0065, -0.0045],\n        [-0.0041, -0.0063, -0.0030,  ...,  0.0013, -0.0013, -0.0053],\n        [-0.0023,  0.0074, -0.0019,  ..., -0.0067,  0.0071, -0.0002]])\nmodel.layers.11.mlp.gate_proj.lora_A.default.weight: tensor([[-7.5035e-03,  9.3460e-03,  1.1818e-02,  ..., -8.4000e-03,\n          9.0485e-03,  1.3771e-03],\n        [-8.2855e-03,  1.4809e-02, -9.1553e-04,  ..., -1.0902e-02,\n         -1.1299e-02, -1.1795e-02],\n        [-1.5038e-02, -8.6823e-03,  8.8501e-04,  ..., -6.2904e-03,\n          5.2185e-03, -4.5815e-03],\n        ...,\n        [ 1.7151e-02,  2.6169e-03,  2.5558e-04,  ..., -1.3199e-02,\n          6.6757e-03,  7.6294e-05],\n        [ 1.4809e-02,  1.1581e-02,  1.4008e-02,  ..., -8.0872e-03,\n          7.8583e-04,  5.9395e-03],\n        [ 1.0307e-02,  1.3733e-04, -3.8567e-03,  ..., -1.6983e-02,\n          1.1871e-02,  4.9934e-03]])\nmodel.layers.11.mlp.gate_proj.lora_B.default.weight: tensor([[ 3.7079e-03, -1.0462e-03,  2.6417e-03,  ...,  6.2180e-04,\n         -6.7425e-04, -1.7357e-03],\n        [-3.5763e-04,  6.2990e-04,  2.5654e-04,  ...,  1.7395e-03,\n          1.2302e-03, -1.2312e-03],\n        [ 4.6968e-04, -3.1233e-04, -1.3518e-04,  ...,  1.5125e-03,\n         -2.3079e-03,  1.8940e-03],\n        ...,\n        [ 3.5095e-03,  2.2507e-03,  2.2392e-03,  ..., -1.3723e-03,\n         -3.1013e-03,  5.8651e-05],\n        [-2.0409e-03,  6.3133e-04,  2.4033e-03,  ..., -2.0828e-03,\n          3.0441e-03, -2.3422e-03],\n        [ 1.9951e-03,  5.1069e-04,  1.6966e-03,  ..., -2.7313e-03,\n         -2.8467e-04, -7.3338e-04]])\nmodel.layers.11.mlp.up_proj.base_layer.weight: tensor([[-0.0013, -0.0083,  0.0044,  ..., -0.0186,  0.0024, -0.0110],\n        [-0.0046, -0.0012, -0.0092,  ..., -0.0032,  0.0092, -0.0059],\n        [ 0.0015, -0.0025, -0.0012,  ...,  0.0009,  0.0037,  0.0126],\n        ...,\n        [ 0.0017,  0.0008, -0.0017,  ...,  0.0041, -0.0060,  0.0110],\n        [-0.0046, -0.0136, -0.0007,  ...,  0.0045,  0.0068, -0.0138],\n        [-0.0029,  0.0029, -0.0014,  ..., -0.0019, -0.0003,  0.0003]])\nmodel.layers.11.mlp.up_proj.lora_A.default.weight: tensor([[-0.0048, -0.0031, -0.0098,  ...,  0.0094, -0.0092,  0.0201],\n        [-0.0128,  0.0053, -0.0110,  ...,  0.0168,  0.0134, -0.0005],\n        [ 0.0125, -0.0015,  0.0121,  ...,  0.0060,  0.0072,  0.0075],\n        ...,\n        [-0.0009, -0.0049,  0.0063,  ...,  0.0004, -0.0196,  0.0169],\n        [-0.0180, -0.0045, -0.0127,  ...,  0.0052,  0.0086,  0.0082],\n        [ 0.0116,  0.0122, -0.0051,  ..., -0.0006, -0.0040,  0.0175]])\nmodel.layers.11.mlp.up_proj.lora_B.default.weight: tensor([[ 2.1877e-03,  2.6016e-03, -1.8501e-04,  ..., -2.1172e-03,\n         -5.2071e-04, -4.1151e-04],\n        [ 2.5482e-03,  5.8556e-04, -6.8665e-05,  ..., -2.5272e-04,\n         -2.9755e-04,  5.1975e-04],\n        [-5.1498e-04,  8.1062e-05, -8.9073e-04,  ..., -1.1415e-03,\n          3.6335e-04,  1.0290e-03],\n        ...,\n        [-2.3270e-04, -4.0674e-04,  2.0180e-03,  ...,  4.2725e-04,\n          2.0599e-03, -2.5063e-03],\n        [ 2.2755e-03, -2.8000e-03,  5.1355e-04,  ..., -1.2741e-03,\n         -1.9703e-03,  2.3994e-03],\n        [-1.3351e-05, -2.0638e-03, -1.8330e-03,  ..., -4.7970e-04,\n         -1.4853e-04, -4.9543e-04]])\nmodel.layers.11.mlp.down_proj.base_layer.weight: tensor([[-0.0020, -0.0033,  0.0023,  ..., -0.0036,  0.0130,  0.0056],\n        [-0.0013, -0.0040, -0.0008,  ..., -0.0035, -0.0016, -0.0075],\n        [-0.0132,  0.0009,  0.0037,  ...,  0.0019,  0.0129,  0.0035],\n        ...,\n        [ 0.0038, -0.0004,  0.0003,  ...,  0.0084,  0.0037,  0.0035],\n        [ 0.0065,  0.0109, -0.0038,  ..., -0.0052, -0.0033,  0.0009],\n        [-0.0004, -0.0042,  0.0005,  ...,  0.0117,  0.0007,  0.0002]])\nmodel.layers.11.mlp.down_proj.lora_A.default.weight: tensor([[-2.7561e-03,  9.6226e-04, -8.7070e-04,  ..., -2.1782e-03,\n          5.9242e-03,  3.2196e-03],\n        [ 7.4291e-04, -1.1559e-03,  8.0204e-04,  ..., -8.3923e-05,\n         -2.2278e-03,  3.4962e-03],\n        [-2.6855e-03,  2.2888e-03,  1.6842e-03,  ...,  9.1553e-04,\n          5.2109e-03, -8.4591e-04],\n        ...,\n        [-9.8133e-04, -1.2560e-03, -5.0888e-03,  ...,  4.2725e-03,\n         -8.1177e-03,  1.7071e-04],\n        [-3.1700e-03, -4.5433e-03,  8.5258e-04,  ...,  1.7471e-03,\n          5.8937e-03, -1.1883e-03],\n        [ 5.0850e-03,  3.5000e-04,  5.9586e-03,  ..., -5.3978e-04,\n          3.8280e-03, -1.4915e-03]])\nmodel.layers.11.mlp.down_proj.lora_B.default.weight: tensor([[ 1.3065e-04,  4.7827e-04, -5.4169e-04,  ...,  1.1959e-03,\n          1.6994e-03,  2.0161e-03],\n        [ 8.5449e-04, -1.1425e-03, -1.0586e-04,  ...,  1.7223e-03,\n         -2.1410e-04, -1.9989e-03],\n        [-2.5139e-03,  2.3155e-03,  1.8764e-04,  ...,  1.2989e-03,\n         -1.7300e-03, -5.4646e-04],\n        ...,\n        [-1.0929e-03,  1.2817e-03,  1.6241e-03,  ...,  1.0624e-03,\n         -1.6928e-03, -9.1553e-05],\n        [ 1.9054e-03, -2.6283e-03, -1.1625e-03,  ..., -1.1597e-03,\n          1.1740e-03, -8.3780e-04],\n        [ 4.6968e-04,  2.5654e-04,  1.0796e-03,  ...,  2.0638e-03,\n         -2.0905e-03,  6.9237e-04]])\nmodel.layers.11.input_layernorm.weight: tensor([1.5469, 1.3281, 1.9062,  ..., 1.1016, 2.1875, 0.6406])\nmodel.layers.11.post_attention_layernorm.weight: tensor([2.3594, 1.1875, 2.4219,  ..., 1.4766, 2.7969, 1.2266])\nmodel.layers.12.self_attn.q_proj.base_layer.weight: tensor([[ 0.0066,  0.0036, -0.0038,  ...,  0.0011,  0.0114, -0.0014],\n        [-0.0020,  0.0079,  0.0090,  ..., -0.0025, -0.0189,  0.0060],\n        [-0.0178,  0.0064,  0.0105,  ..., -0.0086,  0.0046,  0.0088],\n        ...,\n        [-0.0266, -0.0265, -0.0040,  ..., -0.0048, -0.0137,  0.0092],\n        [-0.0102, -0.0097, -0.0018,  ..., -0.0005,  0.0403,  0.0080],\n        [-0.0142, -0.0012,  0.0153,  ...,  0.0029, -0.0449, -0.0013]])\nmodel.layers.12.self_attn.q_proj.lora_A.default.weight: tensor([[-1.3603e-02, -1.0872e-03,  1.4267e-03,  ..., -2.8992e-03,\n         -7.5150e-03,  3.6716e-03],\n        [-5.1117e-03, -1.5450e-03,  5.7220e-05,  ...,  1.2589e-03,\n         -2.4605e-03, -5.6839e-03],\n        [ 4.8866e-03, -3.2234e-03,  1.1475e-02,  ..., -4.6005e-03,\n         -5.7220e-03, -7.2708e-03],\n        ...,\n        [ 9.0408e-03, -9.6893e-04, -2.3155e-03,  ...,  4.2191e-03,\n         -1.1696e-02, -3.7041e-03],\n        [-2.8782e-03, -4.4212e-03,  1.6308e-04,  ...,  5.6610e-03,\n         -9.6970e-03,  1.2703e-03],\n        [-7.8812e-03,  4.5586e-03, -4.1084e-03,  ...,  5.1956e-03,\n          9.9792e-03,  5.8365e-03]])\nmodel.layers.12.self_attn.q_proj.lora_B.default.weight: tensor([[ 2.6226e-04, -1.4448e-04,  5.4550e-04,  ...,  2.4757e-03,\n          1.9131e-03,  9.7466e-04],\n        [ 1.4114e-04,  5.4741e-04,  2.2182e-03,  ...,  3.2005e-03,\n         -1.7166e-04,  2.6855e-03],\n        [ 9.7275e-05, -7.4339e-04,  3.9434e-04,  ...,  9.5844e-04,\n          2.3556e-03,  1.0977e-03],\n        ...,\n        [ 1.1806e-03, -2.5101e-03,  4.6873e-04,  ..., -1.4210e-03,\n         -3.8624e-04,  2.0981e-05],\n        [ 4.4227e-05, -2.6169e-03,  3.7956e-04,  ..., -1.6956e-03,\n         -1.1044e-03,  8.4877e-05],\n        [-1.1215e-03,  1.8883e-04, -4.7493e-04,  ...,  2.3003e-03,\n          4.2152e-04,  4.8304e-04]])\nmodel.layers.12.self_attn.k_proj.base_layer.weight: tensor([[ 7.9346e-03,  7.2632e-03, -9.4986e-04,  ..., -2.0447e-03,\n          3.3569e-04, -3.4637e-03],\n        [ 9.3079e-04, -3.1128e-03,  2.1057e-03,  ..., -3.0212e-03,\n         -1.9379e-03, -8.0585e-05],\n        [-5.9891e-04,  1.3657e-03, -1.3199e-03,  ..., -9.2163e-03,\n         -6.2561e-03, -1.9073e-03],\n        ...,\n        [ 4.5166e-03,  7.5684e-03,  2.2583e-02,  ..., -4.9210e-04,\n         -5.4626e-03, -5.8289e-03],\n        [ 1.8066e-02,  7.2632e-03, -3.8300e-03,  ...,  1.8433e-02,\n          7.5378e-03, -2.0386e-02],\n        [-3.5400e-03, -1.2207e-02,  7.1335e-04,  ..., -4.6692e-03,\n         -6.3171e-03, -9.9945e-04]])\nmodel.layers.12.self_attn.k_proj.lora_A.default.weight: tensor([[-0.0100, -0.0006, -0.0106,  ..., -0.0014, -0.0074, -0.0044],\n        [-0.0036,  0.0031,  0.0098,  ..., -0.0156, -0.0079, -0.0126],\n        [ 0.0026, -0.0150, -0.0119,  ..., -0.0119, -0.0149,  0.0042],\n        ...,\n        [ 0.0099, -0.0122,  0.0189,  ..., -0.0027, -0.0040, -0.0158],\n        [ 0.0050, -0.0074,  0.0055,  ...,  0.0096,  0.0028, -0.0048],\n        [ 0.0053, -0.0135,  0.0121,  ...,  0.0007,  0.0112,  0.0026]])\nmodel.layers.12.self_attn.k_proj.lora_B.default.weight: tensor([[-2.6550e-03, -1.2217e-03,  1.7185e-03,  ...,  6.9094e-04,\n         -1.9836e-03,  5.8126e-04],\n        [ 6.2561e-04,  1.3847e-03, -2.8839e-03,  ...,  2.5177e-03,\n          1.3514e-03,  9.3746e-04],\n        [-1.0891e-03, -3.3054e-03,  3.7813e-04,  ..., -1.6785e-03,\n         -3.3045e-04,  1.3962e-03],\n        ...,\n        [-2.8419e-04, -1.2865e-03,  1.0490e-03,  ...,  8.7738e-04,\n         -4.8876e-04,  1.0147e-03],\n        [ 2.5988e-04,  1.9894e-03, -2.2316e-03,  ...,  2.0981e-04,\n          9.3651e-04, -2.0504e-03],\n        [-3.5286e-05,  1.5850e-03, -2.0561e-03,  ..., -1.1520e-03,\n          2.0142e-03, -1.7719e-03]])\nmodel.layers.12.self_attn.v_proj.base_layer.weight: tensor([[ 0.0095, -0.0157, -0.0036,  ...,  0.0192,  0.0043, -0.0072],\n        [-0.0040, -0.0015, -0.0113,  ...,  0.0083, -0.0049,  0.0029],\n        [-0.0063, -0.0077, -0.0164,  ..., -0.0102,  0.0131, -0.0086],\n        ...,\n        [ 0.0010,  0.0014, -0.0051,  ...,  0.0087,  0.0085, -0.0031],\n        [-0.0173,  0.0038,  0.0013,  ...,  0.0028, -0.0115, -0.0195],\n        [-0.0031, -0.0108,  0.0092,  ...,  0.0087,  0.0098, -0.0124]])\nmodel.layers.12.self_attn.v_proj.lora_A.default.weight: tensor([[ 0.0044,  0.0176,  0.0046,  ...,  0.0020,  0.0126, -0.0062],\n        [ 0.0138, -0.0073,  0.0096,  ..., -0.0163, -0.0077, -0.0087],\n        [-0.0161, -0.0008, -0.0186,  ..., -0.0021,  0.0076,  0.0096],\n        ...,\n        [-0.0065, -0.0024, -0.0040,  ...,  0.0191,  0.0163,  0.0102],\n        [ 0.0096,  0.0082, -0.0013,  ...,  0.0123,  0.0079,  0.0077],\n        [ 0.0123, -0.0065,  0.0008,  ..., -0.0175, -0.0170, -0.0151]])\nmodel.layers.12.self_attn.v_proj.lora_B.default.weight: tensor([[ 1.0262e-03, -2.5725e-04,  2.8777e-04,  ...,  2.2936e-04,\n         -1.8501e-03,  2.3961e-04],\n        [ 1.4019e-03, -1.4858e-03,  3.5644e-04,  ...,  7.4482e-04,\n          3.3712e-04, -1.0033e-03],\n        [ 1.7548e-03,  4.5300e-05,  4.8065e-04,  ...,  2.3651e-04,\n         -3.8671e-04,  2.6817e-03],\n        ...,\n        [ 9.2316e-04, -1.4896e-03, -1.5182e-03,  ...,  3.4714e-04,\n          6.9332e-04,  5.8222e-04],\n        [-3.1948e-04,  1.5135e-03,  1.0309e-03,  ...,  6.3181e-04,\n         -6.5851e-04,  1.3704e-03],\n        [ 1.8501e-03,  1.7204e-03,  1.8024e-03,  ..., -3.3321e-03,\n          1.3094e-03,  1.6727e-03]])\nmodel.layers.12.self_attn.o_proj.base_layer.weight: tensor([[-1.7700e-03,  5.5847e-03,  3.9062e-03,  ...,  6.4373e-05,\n         -6.2561e-03,  4.5013e-04],\n        [ 1.2756e-02,  8.6060e-03,  8.6670e-03,  ...,  3.7003e-04,\n         -2.3804e-03,  3.8910e-03],\n        [ 2.2430e-03,  1.9775e-02,  1.3245e-02,  ...,  3.7079e-03,\n          1.4801e-03,  6.4087e-03],\n        ...,\n        [-2.4658e-02, -8.9111e-03,  1.2817e-02,  ...,  9.6436e-03,\n          2.9907e-03,  4.3945e-03],\n        [-6.8665e-04,  1.2207e-02, -1.2268e-02,  ...,  1.1902e-03,\n         -2.8076e-03,  8.1787e-03],\n        [ 1.0132e-02, -5.0354e-03,  1.1292e-02,  ..., -7.6294e-03,\n         -1.2878e-02, -2.6550e-03]])\nmodel.layers.12.self_attn.o_proj.lora_A.default.weight: tensor([[ 4.8904e-03, -7.2632e-03, -2.3003e-03,  ...,  5.0201e-03,\n          2.0142e-02,  1.4206e-02],\n        [ 4.1351e-03, -6.9656e-03, -1.1925e-02,  ...,  5.4512e-03,\n         -8.0872e-04, -1.3294e-03],\n        [-8.2016e-05, -4.9095e-03,  1.0849e-02,  ...,  4.3945e-03,\n         -1.6022e-02,  1.0941e-02],\n        ...,\n        [-9.0714e-03,  1.7548e-03, -1.5488e-02,  ...,  5.4550e-03,\n         -1.6495e-02,  9.7198e-03],\n        [-5.2338e-03,  1.0246e-02, -4.0741e-03,  ...,  6.9580e-03,\n          2.2202e-03, -2.0428e-03],\n        [-1.0010e-02,  5.0354e-03, -5.5695e-04,  ..., -1.1005e-03,\n         -4.4479e-03, -1.6441e-03]])\nmodel.layers.12.self_attn.o_proj.lora_B.default.weight: tensor([[-1.8625e-03,  1.5860e-03, -7.4291e-04,  ...,  6.2084e-04,\n          1.2770e-03,  1.5354e-03],\n        [-1.9717e-04,  2.8152e-03, -7.8344e-04,  ..., -2.7370e-04,\n          2.3155e-03,  1.5535e-03],\n        [ 8.8310e-04, -3.3545e-04,  5.4932e-04,  ...,  2.0905e-03,\n          5.1880e-04,  1.9779e-03],\n        ...,\n        [-1.1787e-03,  1.0052e-03,  1.5163e-04,  ..., -1.3580e-03,\n         -1.5926e-03,  1.4992e-03],\n        [ 7.9918e-04,  5.6267e-05,  3.0947e-04,  ...,  4.0102e-04,\n          6.7282e-04, -5.5790e-04],\n        [ 2.3651e-03,  1.0786e-03,  6.1655e-04,  ...,  3.8981e-04,\n         -3.7718e-04,  1.0462e-03]])\nmodel.layers.12.mlp.gate_proj.base_layer.weight: tensor([[ 4.6692e-03, -4.0588e-03,  2.1973e-03,  ...,  4.6082e-03,\n          2.2888e-03, -8.8501e-03],\n        [ 1.5488e-03,  4.5166e-03,  3.1433e-03,  ..., -8.9722e-03,\n          8.0872e-04, -5.5237e-03],\n        [ 1.1780e-02,  9.2697e-04,  4.0894e-03,  ...,  1.9379e-03,\n         -1.2131e-03, -4.9744e-03],\n        ...,\n        [ 1.7624e-03, -7.1335e-04,  8.1787e-03,  ..., -8.6670e-03,\n          5.1260e-05,  2.7657e-04],\n        [-6.0425e-03, -7.5073e-03, -1.9043e-02,  ...,  1.8921e-03,\n         -4.6692e-03,  8.1253e-04],\n        [-2.0142e-03,  6.7520e-04,  1.6251e-03,  ...,  7.5912e-04,\n          1.0300e-03,  4.8218e-03]])\nmodel.layers.12.mlp.gate_proj.lora_A.default.weight: tensor([[ 0.0035,  0.0012, -0.0004,  ...,  0.0081,  0.0182, -0.0025],\n        [ 0.0045,  0.0017, -0.0085,  ...,  0.0091,  0.0101, -0.0076],\n        [-0.0006,  0.0097,  0.0055,  ..., -0.0009, -0.0143, -0.0043],\n        ...,\n        [-0.0096, -0.0018, -0.0094,  ..., -0.0017,  0.0097, -0.0049],\n        [ 0.0140, -0.0011,  0.0070,  ..., -0.0041, -0.0004, -0.0043],\n        [ 0.0012,  0.0166,  0.0037,  ...,  0.0009,  0.0027, -0.0038]])\nmodel.layers.12.mlp.gate_proj.lora_B.default.weight: tensor([[ 8.9741e-04,  4.2725e-04, -1.7109e-03,  ..., -6.2084e-04,\n         -3.4046e-04, -1.8482e-03],\n        [ 2.0142e-03,  1.0490e-05, -4.9353e-04,  ..., -1.0777e-03,\n          1.1482e-03, -2.4910e-03],\n        [-4.6587e-04,  2.9850e-04,  4.8590e-04,  ..., -1.3504e-03,\n          1.8272e-03, -8.8692e-05],\n        ...,\n        [-1.2150e-03,  4.7588e-04, -2.2850e-03,  ...,  1.8015e-03,\n          5.3883e-05, -1.7815e-03],\n        [-7.7915e-04, -2.1648e-04, -4.9400e-04,  ...,  1.2817e-03,\n          1.9875e-03,  1.9722e-03],\n        [-2.1667e-03, -1.2693e-03, -1.2245e-03,  ...,  7.9155e-05,\n         -1.1473e-03, -1.4200e-03]])\nmodel.layers.12.mlp.up_proj.base_layer.weight: tensor([[ 4.9973e-04,  4.6921e-04,  1.6403e-03,  ...,  2.3804e-03,\n         -1.1492e-04, -1.6708e-03],\n        [-1.6556e-03, -1.8311e-04,  6.2866e-03,  ...,  3.5400e-03,\n         -2.4048e-02, -1.6403e-03],\n        [-8.3008e-03,  1.0864e-02, -8.6212e-04,  ..., -2.2736e-03,\n         -2.2736e-03,  9.5215e-03],\n        ...,\n        [-9.0408e-04, -2.1057e-03, -1.8477e-05,  ...,  2.7313e-03,\n         -4.3030e-03,  1.9684e-03],\n        [-1.0300e-03, -2.8992e-03,  3.7231e-03,  ..., -3.5400e-03,\n          4.3945e-03,  2.7771e-03],\n        [ 1.1444e-03, -1.9836e-03, -6.2561e-03,  ..., -6.3477e-03,\n          8.3618e-03, -4.8523e-03]])\nmodel.layers.12.mlp.up_proj.lora_A.default.weight: tensor([[-1.1444e-04, -8.8959e-03,  1.7059e-02,  ..., -9.6741e-03,\n         -2.2926e-03, -2.1591e-03],\n        [-2.7046e-03, -6.8512e-03, -2.0485e-03,  ...,  7.3318e-03,\n          2.9087e-03,  6.4926e-03],\n        [ 1.0712e-02,  1.2161e-02,  1.3100e-02,  ...,  4.0131e-03,\n         -1.5266e-02,  1.2711e-02],\n        ...,\n        [-9.4070e-03, -4.7302e-03,  6.2332e-03,  ...,  2.3842e-05,\n          9.9945e-03,  1.3931e-02],\n        [-1.0185e-02,  1.5053e-02, -3.2845e-03,  ..., -1.6586e-02,\n          9.5901e-03, -3.8643e-03],\n        [ 1.3931e-02, -2.5578e-03,  1.4069e-02,  ...,  1.0704e-02,\n          2.9755e-04,  1.4816e-02]])\nmodel.layers.12.mlp.up_proj.lora_B.default.weight: tensor([[-1.3828e-04,  2.1820e-03, -5.2404e-04,  ...,  7.8869e-04,\n         -2.0065e-03,  2.7847e-04],\n        [-1.5154e-03,  1.3771e-03, -5.0068e-04,  ...,  2.2774e-03,\n          6.6471e-04, -1.2398e-04],\n        [-2.3098e-03, -1.2703e-03,  1.7757e-03,  ..., -2.5978e-03,\n         -2.0943e-03, -7.3671e-04],\n        ...,\n        [-1.1024e-03,  9.9373e-04,  5.8317e-04,  ..., -3.7670e-04,\n         -3.7479e-04, -9.3460e-04],\n        [-1.9493e-03, -1.1568e-03,  4.6778e-04,  ..., -9.0599e-05,\n          1.4744e-03, -1.1005e-03],\n        [ 8.5258e-04,  1.4138e-04, -1.9150e-03,  ...,  1.5469e-03,\n          1.5869e-03,  1.5631e-03]])\nmodel.layers.12.mlp.down_proj.base_layer.weight: tensor([[-0.0013,  0.0082,  0.0120,  ...,  0.0115,  0.0011, -0.0026],\n        [-0.0037, -0.0041, -0.0018,  ...,  0.0130,  0.0050,  0.0010],\n        [ 0.0001,  0.0054, -0.0001,  ..., -0.0069,  0.0104,  0.0057],\n        ...,\n        [ 0.0002,  0.0051,  0.0071,  ..., -0.0041, -0.0004,  0.0009],\n        [-0.0070, -0.0102, -0.0021,  ..., -0.0049, -0.0010, -0.0038],\n        [-0.0001,  0.0022, -0.0077,  ..., -0.0034,  0.0023,  0.0044]])\nmodel.layers.12.mlp.down_proj.lora_A.default.weight: tensor([[-2.7733e-03, -4.6768e-03, -1.6508e-03,  ..., -3.0499e-03,\n         -2.5272e-04,  6.8474e-04],\n        [-7.5388e-04, -4.0092e-03,  3.0556e-03,  ..., -1.0624e-03,\n          1.2236e-03, -3.1166e-03],\n        [-5.3978e-03, -3.0155e-03, -5.9319e-03,  ..., -8.6021e-04,\n         -3.2330e-04, -4.2000e-03],\n        ...,\n        [-2.0885e-03,  2.7237e-03,  1.3485e-03,  ..., -2.0771e-03,\n          5.0507e-03, -6.0310e-03],\n        [ 1.6575e-03, -5.8784e-03, -1.2722e-03,  ..., -6.1340e-03,\n         -2.0218e-04, -1.8711e-03],\n        [-4.5090e-03, -2.5406e-03, -9.4414e-05,  ...,  5.9929e-03,\n         -4.5967e-03,  4.3602e-03]])\nmodel.layers.12.mlp.down_proj.lora_B.default.weight: tensor([[ 6.1750e-04, -2.5196e-03, -5.5313e-05,  ...,  1.5593e-03,\n         -1.1406e-03, -1.2875e-03],\n        [ 8.8596e-04,  2.3532e-04, -1.1311e-03,  ...,  1.5450e-03,\n          1.2178e-03, -3.1734e-04],\n        [ 8.4019e-04,  2.6989e-04,  7.7295e-04,  ...,  1.2875e-03,\n          2.2659e-03, -8.8692e-05],\n        ...,\n        [ 1.3685e-04, -1.8673e-03,  2.5234e-03,  ...,  1.4668e-03,\n          7.1096e-04, -1.6155e-03],\n        [ 1.0166e-03,  2.7733e-03,  5.5695e-04,  ..., -1.5478e-03,\n          8.5163e-04,  9.8991e-04],\n        [ 1.9760e-03,  1.0042e-03, -1.2846e-03,  ..., -1.7395e-03,\n          4.5013e-04, -4.5252e-04]])\nmodel.layers.12.input_layernorm.weight: tensor([1.4453, 1.1094, 1.8047,  ..., 1.4219, 2.1250, 1.1016])\nmodel.layers.12.post_attention_layernorm.weight: tensor([2.4375, 1.4219, 2.4219,  ..., 1.8359, 2.6094, 1.6328])\nmodel.layers.13.self_attn.q_proj.base_layer.weight: tensor([[-0.0088, -0.0160, -0.0034,  ..., -0.0033, -0.0077,  0.0016],\n        [ 0.0057,  0.0023, -0.0069,  ..., -0.0061,  0.0005,  0.0024],\n        [ 0.0009, -0.0183, -0.0036,  ..., -0.0002, -0.0085, -0.0016],\n        ...,\n        [-0.0034,  0.0034,  0.0014,  ..., -0.0045,  0.0026, -0.0056],\n        [-0.0008, -0.0003, -0.0059,  ..., -0.0039,  0.0001,  0.0055],\n        [-0.0116, -0.0053, -0.0069,  ...,  0.0026, -0.0043, -0.0012]])\nmodel.layers.13.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0030, -0.0070, -0.0144,  ..., -0.0146,  0.0107, -0.0070],\n        [ 0.0104, -0.0100,  0.0131,  ...,  0.0124,  0.0103,  0.0132],\n        [-0.0025,  0.0156, -0.0027,  ..., -0.0147, -0.0148, -0.0077],\n        ...,\n        [ 0.0181,  0.0124,  0.0010,  ...,  0.0176, -0.0056,  0.0112],\n        [ 0.0059,  0.0079,  0.0098,  ..., -0.0150,  0.0149, -0.0061],\n        [-0.0022,  0.0016,  0.0026,  ...,  0.0085,  0.0032,  0.0113]])\nmodel.layers.13.self_attn.q_proj.lora_B.default.weight: tensor([[ 2.1324e-03, -1.4524e-03, -1.8301e-03,  ..., -1.4000e-03,\n          2.5177e-03,  2.6779e-03],\n        [-1.4687e-03, -2.3079e-03,  1.3418e-03,  ...,  1.1492e-03,\n         -2.0218e-03, -3.5477e-03],\n        [-1.4324e-03, -2.4910e-03, -6.8903e-04,  ...,  1.3571e-03,\n         -4.4823e-05,  3.2711e-04],\n        ...,\n        [ 3.7003e-04, -5.1975e-04,  9.1505e-04,  ...,  1.4830e-03,\n          2.6474e-03,  3.1090e-04],\n        [-7.9155e-05, -1.1597e-03, -6.5947e-04,  ...,  2.2430e-03,\n          1.3590e-03,  5.4359e-04],\n        [-7.4577e-04, -1.4639e-03, -1.1473e-03,  ..., -2.1553e-04,\n          3.3283e-04, -1.2932e-03]])\nmodel.layers.13.self_attn.k_proj.base_layer.weight: tensor([[ 0.0002,  0.0091,  0.0004,  ..., -0.0049,  0.0011,  0.0009],\n        [-0.0009,  0.0023, -0.0055,  ...,  0.0047, -0.0014,  0.0059],\n        [ 0.0025,  0.0120,  0.0008,  ...,  0.0061, -0.0033,  0.0011],\n        ...,\n        [ 0.0006, -0.0206,  0.0052,  ...,  0.0045,  0.0020, -0.0112],\n        [ 0.0008, -0.0121, -0.0005,  ..., -0.0187, -0.0056, -0.0204],\n        [ 0.0061, -0.0078, -0.0128,  ...,  0.0142,  0.0034, -0.0111]])\nmodel.layers.13.self_attn.k_proj.lora_A.default.weight: tensor([[-6.8130e-03, -4.0960e-04, -1.6266e-02,  ...,  1.8997e-03,\n          2.4395e-03, -3.4103e-03],\n        [-6.6757e-06, -9.3460e-04, -9.2773e-03,  ...,  2.1515e-03,\n          9.9564e-03,  7.5150e-03],\n        [ 3.6659e-03,  2.0256e-03,  1.7452e-03,  ...,  8.3160e-04,\n         -1.4771e-02,  5.7983e-03],\n        ...,\n        [-4.4975e-03,  3.6221e-03,  7.9880e-03,  ...,  5.8212e-03,\n          6.0501e-03,  8.2550e-03],\n        [-3.7231e-03,  1.1909e-02,  7.5378e-03,  ...,  1.2016e-03,\n         -5.5161e-03,  1.0132e-02],\n        [-4.4022e-03,  6.0310e-03, -6.9885e-03,  ...,  1.1482e-02,\n          1.7471e-03,  1.5144e-03]])\nmodel.layers.13.self_attn.k_proj.lora_B.default.weight: tensor([[ 4.6730e-05,  4.3201e-04,  3.8099e-04,  ..., -3.1328e-04,\n          1.5640e-04,  6.7043e-04],\n        [ 1.0490e-05, -7.2765e-04, -3.9196e-04,  ...,  4.1771e-04,\n         -1.4420e-03, -1.5726e-03],\n        [-7.9060e-04,  7.6532e-04,  2.2430e-03,  ..., -1.9512e-03,\n          2.6779e-03, -8.4114e-04],\n        ...,\n        [ 1.8349e-03,  1.1110e-03, -1.5163e-04,  ...,  2.9488e-03,\n          3.1471e-05, -5.8270e-04],\n        [ 7.5912e-04, -1.3924e-03, -9.5367e-06,  ..., -2.3413e-04,\n         -4.4918e-04, -2.5330e-03],\n        [-1.1063e-03, -1.2712e-03, -1.5068e-04,  ...,  2.0065e-03,\n          7.1239e-04,  1.8072e-03]])\nmodel.layers.13.self_attn.v_proj.base_layer.weight: tensor([[-4.3640e-03,  1.0498e-02,  1.6937e-03,  ..., -1.1902e-02,\n          1.8677e-02,  2.1667e-03],\n        [-6.9809e-04,  1.5320e-02,  2.0905e-03,  ...,  8.4229e-03,\n         -7.7820e-03,  3.5400e-03],\n        [-5.0049e-03, -1.4038e-03, -7.3853e-03,  ..., -8.4229e-03,\n          4.6082e-03,  2.2705e-02],\n        ...,\n        [ 3.8147e-03,  7.4463e-03,  2.8801e-04,  ...,  2.0294e-03,\n          8.7280e-03,  1.9165e-02],\n        [ 8.1177e-03,  8.4229e-03, -4.6692e-03,  ...,  1.6022e-03,\n         -3.3188e-04, -1.3672e-02],\n        [ 6.0425e-03,  4.7607e-03, -8.1539e-05,  ...,  4.0588e-03,\n          7.5073e-03, -1.3367e-02]])\nmodel.layers.13.self_attn.v_proj.lora_A.default.weight: tensor([[-0.0105, -0.0144,  0.0009,  ...,  0.0030, -0.0014, -0.0019],\n        [ 0.0060,  0.0110, -0.0049,  ..., -0.0053, -0.0016,  0.0050],\n        [ 0.0084,  0.0036,  0.0049,  ..., -0.0119,  0.0094, -0.0184],\n        ...,\n        [ 0.0096,  0.0068,  0.0015,  ..., -0.0106,  0.0078,  0.0105],\n        [ 0.0008,  0.0115, -0.0052,  ...,  0.0055, -0.0098,  0.0130],\n        [ 0.0029,  0.0141, -0.0031,  ...,  0.0098,  0.0075,  0.0025]])\nmodel.layers.13.self_attn.v_proj.lora_B.default.weight: tensor([[-2.4719e-03, -2.0523e-03, -1.1425e-03,  ..., -2.7657e-03,\n         -2.4052e-03,  1.4305e-05],\n        [ 2.0447e-03, -1.6727e-03,  3.1490e-03,  ...,  8.2874e-04,\n          2.1601e-04,  1.3847e-03],\n        [ 3.3092e-04, -3.1424e-04,  2.2278e-03,  ...,  8.5926e-04,\n          1.0908e-04,  4.2391e-04],\n        ...,\n        [ 4.2200e-04, -1.2407e-03,  1.1425e-03,  ..., -8.5545e-04,\n         -4.2295e-04,  1.4191e-03],\n        [ 5.7602e-04, -6.3419e-04, -2.0790e-04,  ..., -1.0443e-04,\n         -9.8705e-05, -1.5106e-03],\n        [ 2.0618e-03,  1.1568e-03, -2.2583e-03,  ..., -2.8419e-04,\n         -1.9321e-03,  7.7248e-05]])\nmodel.layers.13.self_attn.o_proj.base_layer.weight: tensor([[-0.0025,  0.0011,  0.0090,  ..., -0.0064, -0.0090,  0.0153],\n        [ 0.0118,  0.0100, -0.0059,  ...,  0.0033, -0.0019,  0.0125],\n        [ 0.0014, -0.0006, -0.0088,  ..., -0.0003,  0.0038, -0.0071],\n        ...,\n        [ 0.0006, -0.0023, -0.0104,  ...,  0.0010, -0.0052, -0.0262],\n        [ 0.0013, -0.0147, -0.0055,  ..., -0.0022, -0.0003,  0.0009],\n        [-0.0057,  0.0003,  0.0027,  ..., -0.0078,  0.0008, -0.0253]])\nmodel.layers.13.self_attn.o_proj.lora_A.default.weight: tensor([[ 0.0090,  0.0138,  0.0094,  ..., -0.0034,  0.0058, -0.0020],\n        [ 0.0051,  0.0080,  0.0085,  ...,  0.0194,  0.0014,  0.0043],\n        [-0.0145, -0.0121, -0.0225,  ...,  0.0071,  0.0089, -0.0042],\n        ...,\n        [-0.0137, -0.0014, -0.0038,  ..., -0.0088,  0.0066, -0.0039],\n        [-0.0008, -0.0107, -0.0114,  ..., -0.0103,  0.0038, -0.0090],\n        [-0.0008, -0.0044, -0.0093,  ..., -0.0154, -0.0131, -0.0181]])\nmodel.layers.13.self_attn.o_proj.lora_B.default.weight: tensor([[ 0.0026, -0.0007, -0.0003,  ..., -0.0025,  0.0006,  0.0002],\n        [-0.0006, -0.0003,  0.0018,  ..., -0.0013, -0.0004,  0.0007],\n        [ 0.0017,  0.0002, -0.0005,  ...,  0.0019, -0.0004,  0.0019],\n        ...,\n        [ 0.0026,  0.0002,  0.0010,  ...,  0.0001,  0.0003,  0.0009],\n        [-0.0028,  0.0012, -0.0008,  ...,  0.0030,  0.0005, -0.0001],\n        [-0.0015, -0.0009, -0.0007,  ...,  0.0002,  0.0011, -0.0011]])\nmodel.layers.13.mlp.gate_proj.base_layer.weight: tensor([[-9.5215e-03,  1.1215e-03,  5.9204e-03,  ...,  1.6937e-03,\n          4.4250e-03,  9.7046e-03],\n        [ 3.5553e-03, -1.1963e-02, -4.6997e-03,  ..., -1.1108e-02,\n          1.1414e-02,  2.2949e-02],\n        [-6.8359e-03,  1.3000e-02, -4.3640e-03,  ..., -1.0803e-02,\n          6.1340e-03,  4.9744e-03],\n        ...,\n        [ 9.5825e-03, -3.7994e-03, -1.8188e-02,  ..., -2.3346e-03,\n          5.5847e-03,  1.0559e-02],\n        [ 9.1553e-03,  5.2795e-03, -4.3335e-03,  ..., -3.8338e-04,\n          7.2002e-05, -2.3804e-03],\n        [ 9.3384e-03,  8.9645e-04, -2.0905e-03,  ...,  1.2085e-02,\n         -1.0605e-03,  4.9133e-03]])\nmodel.layers.13.mlp.gate_proj.lora_A.default.weight: tensor([[ 0.0036,  0.0011,  0.0057,  ..., -0.0136,  0.0115, -0.0002],\n        [ 0.0123, -0.0024,  0.0202,  ..., -0.0036,  0.0043,  0.0059],\n        [ 0.0005,  0.0153,  0.0216,  ...,  0.0147,  0.0021,  0.0128],\n        ...,\n        [-0.0002, -0.0003, -0.0134,  ...,  0.0015, -0.0059, -0.0061],\n        [-0.0048, -0.0034, -0.0119,  ..., -0.0043,  0.0030, -0.0078],\n        [ 0.0100, -0.0149,  0.0103,  ..., -0.0133, -0.0001, -0.0016]])\nmodel.layers.13.mlp.gate_proj.lora_B.default.weight: tensor([[-5.2261e-04, -2.3136e-03,  1.6727e-03,  ..., -1.0548e-03,\n         -7.7248e-04, -2.4700e-04],\n        [ 8.0347e-04,  1.0691e-03,  5.2333e-05,  ..., -1.8826e-03,\n         -4.7684e-07, -1.7033e-03],\n        [-3.7003e-04, -1.1578e-03,  2.4986e-03,  ..., -1.1816e-03,\n         -2.6169e-03, -7.1526e-05],\n        ...,\n        [ 7.0572e-05,  1.8234e-03, -2.2774e-03,  ...,  1.5430e-03,\n          1.5974e-03,  9.2983e-04],\n        [ 5.2357e-04, -1.9121e-03, -6.9714e-04,  ..., -2.4109e-03,\n          1.8864e-03,  3.2806e-03],\n        [-6.7520e-04,  4.2772e-04, -2.7542e-03,  ..., -4.7147e-05,\n          2.2202e-03,  4.9496e-04]])\nmodel.layers.13.mlp.up_proj.base_layer.weight: tensor([[-9.0027e-04,  5.1270e-03,  7.3853e-03,  ..., -1.4343e-03,\n          2.6093e-03, -3.7079e-03],\n        [ 2.4414e-03, -9.0408e-04, -1.3184e-02,  ...,  3.9673e-03,\n         -7.9956e-03, -8.0566e-03],\n        [-5.7373e-03, -4.2725e-03, -1.7452e-04,  ..., -1.5182e-03,\n          1.3611e-02, -9.1553e-03],\n        ...,\n        [-2.5635e-03, -6.0120e-03, -1.8311e-02,  ...,  4.0283e-03,\n         -8.3008e-03, -1.8555e-02],\n        [ 5.6458e-03, -7.7820e-03, -1.8311e-03,  ..., -8.7891e-03,\n          5.9509e-03, -4.1504e-03],\n        [-4.1809e-03, -6.9885e-03, -9.7046e-03,  ...,  4.7207e-05,\n         -9.1553e-03,  1.3123e-03]])\nmodel.layers.13.mlp.up_proj.lora_A.default.weight: tensor([[ 1.2817e-02, -9.5444e-03,  1.4061e-02,  ..., -1.0689e-02,\n          4.9210e-03, -1.3397e-02],\n        [ 3.0632e-03, -9.5901e-03, -1.5312e-02,  ...,  2.1362e-04,\n         -5.5809e-03,  5.0659e-03],\n        [ 1.0162e-02, -1.0681e-02,  8.6899e-03,  ...,  7.1754e-03,\n         -6.0501e-03,  4.6425e-03],\n        ...,\n        [-2.6703e-04, -6.3820e-03, -3.8452e-03,  ...,  9.0027e-03,\n         -4.6120e-03,  1.8204e-02],\n        [ 1.5259e-05,  8.8654e-03, -3.2654e-03,  ..., -1.7462e-03,\n         -1.4816e-02, -3.6869e-03],\n        [-6.2180e-04,  1.1154e-02,  1.7593e-02,  ..., -1.8368e-03,\n          1.5869e-02,  6.4659e-03]])\nmodel.layers.13.mlp.up_proj.lora_B.default.weight: tensor([[ 1.8330e-03, -2.9449e-03,  1.5574e-03,  ..., -5.9748e-04,\n         -1.8167e-03,  9.4652e-04],\n        [-1.9951e-03, -2.5406e-03,  1.3809e-03,  ..., -1.9140e-03,\n          1.1969e-03,  3.3188e-03],\n        [ 4.3058e-04,  5.7602e-04, -1.5354e-04,  ...,  1.3647e-03,\n         -5.7220e-05, -2.4700e-04],\n        ...,\n        [ 3.7670e-04,  1.6270e-03, -5.8556e-04,  ..., -2.1610e-03,\n         -3.3894e-03,  1.5354e-04],\n        [-7.5245e-04,  7.8249e-04, -9.9754e-04,  ...,  1.7376e-03,\n          9.9468e-04, -2.2113e-04],\n        [-2.4414e-04, -1.6661e-03, -1.3723e-03,  ..., -1.3418e-03,\n         -1.4162e-03,  2.8534e-03]])\nmodel.layers.13.mlp.down_proj.base_layer.weight: tensor([[-0.0164, -0.0079,  0.0038,  ...,  0.0079,  0.0080,  0.0029],\n        [ 0.0015, -0.0096, -0.0004,  ...,  0.0065, -0.0078,  0.0033],\n        [ 0.0047,  0.0027,  0.0038,  ..., -0.0035, -0.0046,  0.0001],\n        ...,\n        [ 0.0092,  0.0033,  0.0098,  ..., -0.0055, -0.0024,  0.0115],\n        [ 0.0008, -0.0015,  0.0103,  ..., -0.0011,  0.0039,  0.0049],\n        [ 0.0135,  0.0058,  0.0013,  ...,  0.0205,  0.0030, -0.0117]])\nmodel.layers.13.mlp.down_proj.lora_A.default.weight: tensor([[ 4.2725e-03, -1.1692e-03,  1.3466e-03,  ...,  5.3520e-03,\n          5.3749e-03, -1.9703e-03],\n        [ 1.0872e-04, -4.7073e-03, -7.1411e-03,  ...,  1.3123e-03,\n         -3.7918e-03,  1.8425e-03],\n        [-4.3068e-03,  2.9697e-03, -9.2888e-04,  ...,  6.4697e-03,\n          5.5771e-03,  1.8368e-03],\n        ...,\n        [-5.9319e-03,  2.7542e-03, -3.3760e-04,  ...,  2.7275e-03,\n         -2.0933e-04,  5.2414e-03],\n        [ 1.3866e-03, -5.1880e-03, -2.0695e-04,  ...,  3.9558e-03,\n          1.5240e-03,  9.3460e-04],\n        [-8.8215e-05,  5.6267e-05,  1.1864e-03,  ..., -8.3313e-03,\n         -8.3160e-04, -9.1171e-04]])\nmodel.layers.13.mlp.down_proj.lora_B.default.weight: tensor([[-3.9172e-04,  1.8349e-03, -1.7776e-03,  ..., -9.7179e-04,\n          2.3804e-03,  1.4896e-03],\n        [ 9.1553e-05,  1.6003e-03, -2.6178e-04,  ..., -5.1260e-04,\n          1.1301e-03, -8.7309e-04],\n        [ 1.3256e-03,  1.7862e-03, -3.6669e-04,  ..., -2.8610e-06,\n         -1.0672e-03,  2.0409e-03],\n        ...,\n        [-5.4073e-04,  1.4248e-03,  1.0109e-04,  ..., -1.0853e-03,\n         -9.1410e-04,  1.2150e-03],\n        [-1.2331e-03, -2.5444e-03,  9.2602e-04,  ..., -4.1199e-04,\n         -1.5564e-03, -1.3485e-03],\n        [-1.4343e-03,  8.1635e-04,  1.4887e-03,  ...,  5.0354e-04,\n         -1.6270e-03, -1.9569e-03]])\nmodel.layers.13.input_layernorm.weight: tensor([1.1250, 1.3516, 1.1562,  ..., 1.1641, 1.3984, 0.7734])\nmodel.layers.13.post_attention_layernorm.weight: tensor([2.4062, 1.6797, 2.4531,  ..., 2.0156, 2.7344, 1.9219])\nmodel.layers.14.self_attn.q_proj.base_layer.weight: tensor([[ 0.0022,  0.0010, -0.0089,  ..., -0.0043,  0.0044, -0.0070],\n        [-0.0042,  0.0019, -0.0003,  ...,  0.0017, -0.0055,  0.0039],\n        [ 0.0030,  0.0004, -0.0019,  ..., -0.0049,  0.0085, -0.0021],\n        ...,\n        [ 0.0104, -0.0253, -0.0051,  ..., -0.0034, -0.0001, -0.0033],\n        [ 0.0031, -0.0119, -0.0108,  ..., -0.0029, -0.0242, -0.0009],\n        [ 0.0099,  0.0109,  0.0042,  ..., -0.0096, -0.0036, -0.0089]])\nmodel.layers.14.self_attn.q_proj.lora_A.default.weight: tensor([[-0.0005, -0.0172,  0.0042,  ..., -0.0096, -0.0060, -0.0101],\n        [-0.0231, -0.0117, -0.0093,  ..., -0.0077,  0.0049,  0.0053],\n        [ 0.0015,  0.0158, -0.0004,  ...,  0.0030, -0.0083,  0.0062],\n        ...,\n        [-0.0036, -0.0068, -0.0001,  ..., -0.0003, -0.0146, -0.0086],\n        [-0.0066,  0.0103, -0.0018,  ...,  0.0033,  0.0018, -0.0065],\n        [ 0.0052, -0.0082,  0.0124,  ...,  0.0036,  0.0057, -0.0060]])\nmodel.layers.14.self_attn.q_proj.lora_B.default.weight: tensor([[ 2.0027e-05, -4.9305e-04,  1.5583e-03,  ..., -1.0376e-03,\n          2.9564e-03,  6.4707e-04],\n        [ 6.6757e-06,  2.1095e-03, -5.2452e-04,  ..., -2.6436e-03,\n          7.2765e-04,  2.0275e-03],\n        [-1.9703e-03, -1.2188e-03,  3.7842e-03,  ..., -6.6614e-04,\n          2.6665e-03,  8.3733e-04],\n        ...,\n        [-2.5558e-03,  2.6550e-03,  2.9259e-03,  ..., -5.7411e-04,\n          3.1128e-03,  1.3733e-04],\n        [-2.6684e-03, -1.4801e-03,  6.1703e-04,  ..., -4.2605e-04,\n          5.0640e-04, -1.8539e-03],\n        [ 4.2033e-04,  1.1206e-03,  1.2016e-04,  ..., -5.1928e-04,\n          6.6519e-04,  8.3065e-04]])\nmodel.layers.14.self_attn.k_proj.base_layer.weight: tensor([[ 0.0068, -0.0056,  0.0056,  ...,  0.0006,  0.0003, -0.0022],\n        [ 0.0040,  0.0059,  0.0054,  ..., -0.0131, -0.0018, -0.0056],\n        [-0.0024,  0.0012, -0.0016,  ...,  0.0042, -0.0056,  0.0052],\n        ...,\n        [-0.0013, -0.0037, -0.0304,  ...,  0.0059, -0.0066,  0.0039],\n        [-0.0022, -0.0077,  0.0139,  ...,  0.0107,  0.0156,  0.0087],\n        [-0.0129,  0.0352, -0.0303,  ..., -0.0294, -0.0154,  0.0126]])\nmodel.layers.14.self_attn.k_proj.lora_A.default.weight: tensor([[-0.0020, -0.0005,  0.0130,  ...,  0.0042,  0.0035, -0.0022],\n        [ 0.0056, -0.0037,  0.0063,  ...,  0.0154,  0.0099, -0.0008],\n        [ 0.0139, -0.0111,  0.0073,  ...,  0.0140, -0.0034,  0.0029],\n        ...,\n        [ 0.0021,  0.0035, -0.0015,  ..., -0.0113,  0.0210, -0.0002],\n        [ 0.0104,  0.0090,  0.0150,  ...,  0.0027,  0.0023, -0.0087],\n        [-0.0109, -0.0003,  0.0008,  ..., -0.0058, -0.0081, -0.0065]])\nmodel.layers.14.self_attn.k_proj.lora_B.default.weight: tensor([[ 1.5678e-03,  2.6913e-03,  3.5095e-04,  ..., -5.5790e-04,\n         -2.0199e-03, -6.1989e-04],\n        [-3.2082e-03, -7.1239e-04, -2.8534e-03,  ..., -2.7504e-03,\n         -1.9045e-03,  1.4544e-03],\n        [ 2.0638e-03, -2.6417e-04,  1.9369e-03,  ...,  2.5253e-03,\n         -1.2112e-03,  0.0000e+00],\n        ...,\n        [ 3.3998e-04, -3.0160e-04, -8.4877e-05,  ...,  9.8228e-05,\n          1.1778e-04, -2.2078e-04],\n        [ 2.5406e-03, -1.6165e-04,  1.2388e-03,  ..., -3.4976e-04,\n          2.4834e-03,  2.0170e-04],\n        [-2.7313e-03,  1.6575e-03,  3.2425e-05,  ...,  2.7428e-03,\n         -8.0252e-04, -2.2144e-03]])\nmodel.layers.14.self_attn.v_proj.base_layer.weight: tensor([[-0.0045,  0.0003,  0.0057,  ..., -0.0067, -0.0094,  0.0032],\n        [-0.0015, -0.0036, -0.0082,  ..., -0.0092, -0.0098, -0.0002],\n        [ 0.0007, -0.0027,  0.0143,  ...,  0.0046,  0.0033,  0.0054],\n        ...,\n        [ 0.0049, -0.0063,  0.0030,  ...,  0.0037, -0.0085, -0.0140],\n        [ 0.0047,  0.0039,  0.0041,  ..., -0.0067,  0.0086,  0.0011],\n        [-0.0016, -0.0022,  0.0075,  ..., -0.0064,  0.0009,  0.0082]])\nmodel.layers.14.self_attn.v_proj.lora_A.default.weight: tensor([[-1.1200e-02,  1.6052e-02, -9.2697e-03,  ...,  1.1078e-02,\n         -1.0681e-03, -1.6212e-03],\n        [ 7.6828e-03, -1.1444e-03, -3.2463e-03,  ...,  2.0065e-03,\n          9.2316e-04, -6.5727e-03],\n        [ 5.9509e-04, -8.5754e-03,  2.8877e-03,  ...,  1.1734e-02,\n         -3.8147e-05,  1.3397e-02],\n        ...,\n        [ 1.6365e-03, -1.6571e-02,  1.3321e-02,  ...,  7.8049e-03,\n         -1.2772e-02, -1.1131e-02],\n        [ 4.8828e-04, -5.6076e-03, -2.0599e-04,  ...,  6.2981e-03,\n          2.2354e-02, -7.7820e-03],\n        [ 5.1651e-03,  2.4490e-03, -1.3962e-02,  ...,  7.0572e-03,\n         -1.1444e-02,  5.3596e-03]])\nmodel.layers.14.self_attn.v_proj.lora_B.default.weight: tensor([[-0.0018, -0.0002, -0.0007,  ..., -0.0024, -0.0003, -0.0001],\n        [-0.0001,  0.0001,  0.0002,  ...,  0.0017, -0.0018, -0.0016],\n        [-0.0005, -0.0021, -0.0002,  ...,  0.0013,  0.0025,  0.0012],\n        ...,\n        [ 0.0005, -0.0002, -0.0002,  ...,  0.0013,  0.0020,  0.0018],\n        [-0.0013, -0.0013,  0.0016,  ...,  0.0011,  0.0016,  0.0021],\n        [ 0.0015, -0.0008, -0.0001,  ...,  0.0013,  0.0023, -0.0015]])\nmodel.layers.14.self_attn.o_proj.base_layer.weight: tensor([[-0.0084, -0.0075, -0.0063,  ..., -0.0051,  0.0027, -0.0016],\n        [ 0.0034, -0.0027,  0.0077,  ...,  0.0102, -0.0034, -0.0066],\n        [-0.0002, -0.0010,  0.0113,  ...,  0.0015, -0.0011, -0.0023],\n        ...,\n        [-0.0076, -0.0012,  0.0005,  ...,  0.0073,  0.0126,  0.0026],\n        [-0.0092,  0.0023, -0.0016,  ...,  0.0087, -0.0013, -0.0193],\n        [-0.0054, -0.0014,  0.0017,  ...,  0.0042,  0.0084,  0.0034]])\nmodel.layers.14.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0003,  0.0023,  0.0011,  ...,  0.0110, -0.0059, -0.0058],\n        [-0.0006, -0.0063,  0.0092,  ..., -0.0086, -0.0140, -0.0010],\n        [-0.0107,  0.0013,  0.0036,  ..., -0.0056,  0.0018, -0.0032],\n        ...,\n        [-0.0029,  0.0176, -0.0019,  ...,  0.0030, -0.0023, -0.0041],\n        [ 0.0006, -0.0048, -0.0026,  ..., -0.0034,  0.0017,  0.0025],\n        [-0.0082,  0.0108,  0.0047,  ...,  0.0146,  0.0046, -0.0004]])\nmodel.layers.14.self_attn.o_proj.lora_B.default.weight: tensor([[ 1.2760e-03,  9.2030e-04, -1.0605e-03,  ...,  1.8539e-03,\n          1.9169e-04,  1.9193e-05],\n        [ 2.8076e-03,  1.3924e-03, -8.3923e-05,  ..., -1.8501e-03,\n          2.4557e-05, -2.1114e-03],\n        [ 4.8590e-04,  5.3406e-04, -1.5545e-03,  ..., -2.7676e-03,\n          1.2226e-03, -2.2278e-03],\n        ...,\n        [ 6.3992e-04, -2.5101e-03, -7.4863e-04,  ...,  1.4620e-03,\n          1.5249e-03, -5.0783e-04],\n        [ 2.0981e-04, -2.0504e-03, -2.0385e-04,  ...,  4.0913e-04,\n         -5.5885e-04,  1.4153e-03],\n        [-1.4896e-03,  9.1648e-04,  1.6975e-03,  ...,  4.0817e-04,\n         -1.1673e-03, -5.2452e-06]])\nmodel.layers.14.mlp.gate_proj.base_layer.weight: tensor([[ 0.0008,  0.0003,  0.0008,  ...,  0.0021, -0.0024,  0.0033],\n        [ 0.0001,  0.0023,  0.0054,  ...,  0.0036,  0.0026,  0.0033],\n        [-0.0093, -0.0031, -0.0053,  ...,  0.0002,  0.0075,  0.0025],\n        ...,\n        [ 0.0104,  0.0055, -0.0060,  ...,  0.0066,  0.0016,  0.0013],\n        [ 0.0028, -0.0010, -0.0025,  ...,  0.0049,  0.0072,  0.0017],\n        [-0.0044, -0.0089,  0.0057,  ..., -0.0076,  0.0009, -0.0078]])\nmodel.layers.14.mlp.gate_proj.lora_A.default.weight: tensor([[ 0.0105, -0.0028,  0.0022,  ...,  0.0056, -0.0133,  0.0053],\n        [-0.0004,  0.0151,  0.0002,  ..., -0.0086,  0.0003, -0.0125],\n        [ 0.0023, -0.0067,  0.0105,  ..., -0.0156,  0.0043, -0.0047],\n        ...,\n        [ 0.0132,  0.0136,  0.0113,  ..., -0.0052, -0.0068,  0.0101],\n        [-0.0110,  0.0009, -0.0011,  ...,  0.0146,  0.0070,  0.0048],\n        [-0.0054, -0.0144, -0.0071,  ..., -0.0089, -0.0036, -0.0033]])\nmodel.layers.14.mlp.gate_proj.lora_B.default.weight: tensor([[ 1.7414e-03, -2.5845e-04,  8.7404e-04,  ...,  1.9722e-03,\n          1.1921e-04,  1.9989e-03],\n        [-2.0409e-03, -2.4247e-04, -1.6727e-03,  ..., -1.0748e-03,\n          3.1796e-03,  4.7493e-04],\n        [ 1.3018e-03, -3.6955e-04,  1.5621e-03,  ...,  1.0386e-03,\n         -2.5291e-03,  8.3923e-05],\n        ...,\n        [-2.3289e-03,  4.0221e-04, -6.6376e-04,  ..., -6.4754e-04,\n          1.6432e-03, -8.0967e-04],\n        [-9.3269e-04, -1.4830e-03, -2.9182e-04,  ..., -1.0157e-03,\n          7.9775e-04,  2.8992e-03],\n        [-1.8654e-03,  9.7275e-04, -2.8858e-03,  ...,  2.2678e-03,\n         -4.0817e-04, -2.0218e-03]])\nmodel.layers.14.mlp.up_proj.base_layer.weight: tensor([[ 3.7537e-03, -2.8992e-03,  9.2163e-03,  ..., -7.3242e-03,\n         -5.3711e-03, -8.5068e-04],\n        [ 8.4686e-04, -8.3618e-03,  4.1809e-03,  ..., -6.5002e-03,\n         -9.4604e-03,  2.0294e-03],\n        [ 1.7014e-03, -7.1411e-03, -3.2349e-03,  ..., -4.7302e-03,\n          8.6212e-04, -4.4250e-03],\n        ...,\n        [ 5.0049e-03,  5.1575e-03, -1.0681e-03,  ..., -8.1177e-03,\n          2.6550e-03, -7.5684e-03],\n        [ 3.6469e-03,  1.8463e-03,  4.6082e-03,  ..., -2.6584e-05,\n         -1.9670e-05,  4.3335e-03],\n        [ 3.6621e-03,  5.3711e-03, -4.9438e-03,  ..., -4.6997e-03,\n         -6.5613e-03,  7.6294e-04]])\nmodel.layers.14.mlp.up_proj.lora_A.default.weight: tensor([[-0.0197,  0.0041, -0.0031,  ...,  0.0028, -0.0014, -0.0082],\n        [ 0.0047,  0.0072,  0.0031,  ...,  0.0104, -0.0043,  0.0083],\n        [ 0.0047,  0.0134,  0.0144,  ...,  0.0021,  0.0026, -0.0103],\n        ...,\n        [-0.0095,  0.0051, -0.0027,  ...,  0.0129, -0.0080,  0.0092],\n        [ 0.0057,  0.0143,  0.0084,  ...,  0.0009,  0.0151,  0.0043],\n        [ 0.0221, -0.0010,  0.0145,  ...,  0.0126, -0.0005, -0.0023]])\nmodel.layers.14.mlp.up_proj.lora_B.default.weight: tensor([[-1.4887e-03, -2.5253e-03,  1.7357e-04,  ...,  7.1812e-04,\n          2.0599e-03,  3.6430e-04],\n        [ 1.1597e-03, -1.0223e-03,  5.8937e-04,  ...,  7.8535e-04,\n          1.3885e-03, -6.4373e-05],\n        [-9.4557e-04, -1.0538e-03, -1.6832e-03,  ..., -7.2145e-04,\n         -3.1757e-04, -1.0710e-03],\n        ...,\n        [ 3.2997e-03, -2.5177e-03, -3.6011e-03,  ..., -3.7136e-03,\n         -1.1997e-03, -1.1578e-03],\n        [-1.1230e-04,  1.2550e-03, -2.6608e-04,  ..., -5.3024e-04,\n          7.8678e-04,  1.2569e-03],\n        [-2.4223e-03,  3.3779e-03, -3.0594e-03,  ..., -1.8082e-03,\n         -5.8699e-04, -7.1716e-04]])\nmodel.layers.14.mlp.down_proj.base_layer.weight: tensor([[-5.6458e-03,  9.5215e-03,  5.6458e-03,  ..., -7.1049e-05,\n          1.8539e-03,  1.7776e-03],\n        [ 1.4191e-03, -1.2512e-02, -3.9062e-03,  ..., -8.9111e-03,\n         -1.0559e-02, -9.9945e-04],\n        [-1.3733e-02,  5.2795e-03, -2.8992e-03,  ..., -2.1057e-03,\n         -5.9509e-03,  5.7678e-03],\n        ...,\n        [ 4.4556e-03,  8.0872e-04,  3.7079e-03,  ...,  4.8637e-04,\n          4.0283e-03,  2.3346e-03],\n        [ 1.1215e-03,  8.7891e-03, -3.9062e-03,  ...,  2.3193e-03,\n         -6.5918e-03,  8.2397e-03],\n        [-3.5553e-03,  6.2866e-03, -8.3008e-03,  ...,  2.3041e-03,\n         -1.3000e-02, -2.1515e-03]])\nmodel.layers.14.mlp.down_proj.lora_A.default.weight: tensor([[-0.0006, -0.0031,  0.0035,  ...,  0.0045,  0.0056,  0.0019],\n        [-0.0020, -0.0020, -0.0019,  ..., -0.0034, -0.0023, -0.0002],\n        [-0.0005, -0.0035,  0.0009,  ...,  0.0037, -0.0027, -0.0034],\n        ...,\n        [ 0.0053, -0.0052, -0.0016,  ...,  0.0012,  0.0016,  0.0009],\n        [-0.0042,  0.0011, -0.0023,  ...,  0.0089,  0.0060,  0.0075],\n        [-0.0025,  0.0044, -0.0011,  ...,  0.0035,  0.0034,  0.0042]])\nmodel.layers.14.mlp.down_proj.lora_B.default.weight: tensor([[ 8.5592e-04,  2.5864e-03,  1.4343e-03,  ...,  9.8133e-04,\n          4.2343e-04,  5.0640e-04],\n        [ 1.9855e-03,  1.5697e-03,  2.9812e-03,  ...,  1.2722e-03,\n          8.9979e-04,  1.4191e-03],\n        [-8.0109e-05, -1.7214e-03, -2.4223e-04,  ...,  2.0218e-03,\n         -1.8654e-03,  1.9894e-03],\n        ...,\n        [ 1.1683e-03,  2.1434e-04, -8.5831e-05,  ...,  2.8858e-03,\n         -6.5279e-04,  3.3140e-04],\n        [-1.3351e-04, -8.0776e-04,  5.9700e-04,  ..., -2.3594e-03,\n         -2.8419e-03, -1.7090e-03],\n        [-5.6505e-05,  7.7581e-04,  2.1482e-04,  ...,  1.4381e-03,\n          2.4872e-03, -1.8013e-04]])\nmodel.layers.14.input_layernorm.weight: tensor([0.6484, 1.4219, 1.0469,  ..., 0.7109, 1.0312, 0.7266])\nmodel.layers.14.post_attention_layernorm.weight: tensor([2.6562, 1.9297, 2.5938,  ..., 2.4531, 2.7656, 2.3906])\nmodel.layers.15.self_attn.q_proj.base_layer.weight: tensor([[ 6.3477e-03, -9.4223e-04, -3.7384e-03,  ..., -1.2817e-03,\n         -3.4637e-03, -3.2806e-03],\n        [-2.4567e-03,  1.0986e-03,  1.4648e-03,  ...,  1.0834e-03,\n          2.1667e-03,  1.8387e-03],\n        [ 7.3242e-03, -6.6223e-03, -4.3869e-04,  ..., -1.0757e-03,\n         -9.3460e-04, -5.9843e-05],\n        ...,\n        [ 3.3112e-03,  2.8419e-04,  2.4128e-04,  ..., -5.9509e-03,\n         -8.7891e-03, -5.1575e-03],\n        [ 2.8381e-03, -5.6152e-03,  6.6223e-03,  ...,  1.3123e-03,\n         -6.1340e-03,  6.1646e-03],\n        [ 2.7771e-03,  1.6357e-02,  1.5564e-03,  ...,  4.6387e-03,\n         -5.4016e-03, -7.9956e-03]])\nmodel.layers.15.self_attn.q_proj.lora_A.default.weight: tensor([[-0.0092,  0.0094, -0.0123,  ...,  0.0039, -0.0091,  0.0170],\n        [ 0.0093,  0.0142,  0.0141,  ...,  0.0109,  0.0069,  0.0141],\n        [ 0.0054,  0.0138, -0.0029,  ..., -0.0087, -0.0122, -0.0086],\n        ...,\n        [-0.0126,  0.0092,  0.0022,  ..., -0.0118,  0.0022,  0.0068],\n        [-0.0098,  0.0132,  0.0026,  ...,  0.0017, -0.0062,  0.0103],\n        [-0.0011, -0.0014, -0.0022,  ...,  0.0005, -0.0137, -0.0187]])\nmodel.layers.15.self_attn.q_proj.lora_B.default.weight: tensor([[-1.0777e-04,  1.8024e-04, -1.8339e-03,  ..., -8.8596e-04,\n          9.3842e-04, -1.2264e-03],\n        [-1.0099e-03,  3.4809e-04, -1.7338e-03,  ...,  8.3113e-04,\n         -2.5024e-03,  8.6689e-04],\n        [ 2.0256e-03, -1.7738e-04,  7.4196e-04,  ...,  4.1389e-04,\n         -2.3556e-04, -2.4662e-03],\n        ...,\n        [-1.2054e-03,  1.7290e-03,  5.3215e-04,  ..., -7.9870e-04,\n          3.8910e-04, -1.5793e-03],\n        [ 6.3229e-04,  5.0664e-05,  2.7599e-03,  ..., -8.2111e-04,\n          4.2629e-04,  4.5371e-04],\n        [-6.4945e-04, -1.5526e-03,  1.2856e-03,  ..., -1.6141e-04,\n          5.3358e-04,  2.4948e-03]])\nmodel.layers.15.self_attn.k_proj.base_layer.weight: tensor([[-4.1504e-03,  4.1199e-03,  2.0142e-03,  ..., -8.2016e-04,\n         -1.2512e-03, -4.3030e-03],\n        [-2.5177e-03,  4.9133e-03, -2.2888e-04,  ...,  3.9062e-03,\n         -2.9755e-03,  4.3106e-04],\n        [-6.5308e-03,  1.9379e-03, -9.9945e-04,  ...,  1.6022e-03,\n          5.3787e-04,  2.7924e-03],\n        ...,\n        [-2.2736e-03,  1.9043e-02,  2.1729e-02,  ..., -1.9775e-02,\n         -8.1062e-05,  1.3977e-02],\n        [-1.0071e-02,  2.4658e-02, -1.0620e-02,  ..., -1.2207e-02,\n          3.5858e-03,  3.8757e-03],\n        [-3.3112e-03,  4.8523e-03,  1.1414e-02,  ..., -2.2583e-02,\n          1.8387e-03, -1.0681e-03]])\nmodel.layers.15.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0089, -0.0039,  0.0122,  ..., -0.0022, -0.0030,  0.0037],\n        [ 0.0055,  0.0068, -0.0114,  ...,  0.0236, -0.0028,  0.0015],\n        [-0.0208, -0.0185, -0.0128,  ..., -0.0190, -0.0049, -0.0149],\n        ...,\n        [-0.0077, -0.0085,  0.0042,  ...,  0.0099, -0.0142,  0.0086],\n        [-0.0064,  0.0030, -0.0113,  ..., -0.0112, -0.0094, -0.0162],\n        [ 0.0070,  0.0005,  0.0050,  ..., -0.0064, -0.0136, -0.0075]])\nmodel.layers.15.self_attn.k_proj.lora_B.default.weight: tensor([[ 1.8959e-03,  1.8978e-04,  1.9684e-03,  ...,  4.9019e-04,\n          1.4963e-03, -2.8133e-04],\n        [-3.1681e-03, -1.5430e-03, -7.5054e-04,  ..., -3.5572e-04,\n         -1.7319e-03,  1.3037e-03],\n        [ 9.6321e-05,  5.6171e-04, -1.1444e-03,  ...,  1.0395e-04,\n         -8.5688e-04, -2.0218e-04],\n        ...,\n        [ 3.5143e-04,  7.0000e-04, -7.7057e-04,  ..., -2.3308e-03,\n         -8.7023e-04,  7.7724e-05],\n        [-2.6264e-03, -2.2774e-03, -1.5509e-04,  ...,  2.4948e-03,\n         -5.9032e-04,  1.8129e-03],\n        [-1.7262e-03, -1.3900e-04,  1.4019e-04,  ...,  2.3117e-03,\n         -1.7357e-03,  6.1035e-05]])\nmodel.layers.15.self_attn.v_proj.base_layer.weight: tensor([[-0.0311,  0.0063,  0.0466,  ..., -0.0115, -0.0143,  0.0228],\n        [-0.0173,  0.0033,  0.0146,  ..., -0.0065,  0.0142, -0.0203],\n        [ 0.0201,  0.0002, -0.0055,  ..., -0.0298,  0.0139,  0.0078],\n        ...,\n        [-0.0251, -0.0074, -0.0037,  ...,  0.0061, -0.0110,  0.0005],\n        [ 0.0210, -0.0016,  0.0110,  ..., -0.0073, -0.0107, -0.0223],\n        [-0.0178, -0.0131, -0.0058,  ..., -0.0222,  0.0056,  0.0034]])\nmodel.layers.15.self_attn.v_proj.lora_A.default.weight: tensor([[ 1.1230e-02,  6.1531e-03, -8.9874e-03,  ...,  5.2643e-03,\n         -7.4883e-03,  1.0704e-02],\n        [ 5.5313e-05, -4.4632e-04,  1.1841e-02,  ...,  6.4278e-04,\n         -9.8953e-03, -5.0316e-03],\n        [ 7.5836e-03,  2.1591e-03,  1.0651e-02,  ..., -1.6907e-02,\n          4.5013e-04, -1.7029e-02],\n        ...,\n        [ 1.1871e-02, -1.1688e-02,  9.6588e-03,  ...,  3.6621e-03,\n         -5.9414e-04,  5.9891e-03],\n        [ 1.9875e-03,  7.1640e-03, -2.1019e-03,  ...,  9.7275e-04,\n          1.3046e-02, -1.3741e-02],\n        [ 1.0452e-02, -1.2100e-02,  1.0269e-02,  ...,  1.1566e-02,\n          9.3689e-03,  4.1656e-03]])\nmodel.layers.15.self_attn.v_proj.lora_B.default.weight: tensor([[ 4.3726e-04, -2.2316e-03,  3.0041e-04,  ...,  3.6955e-04,\n         -1.2188e-03, -1.9550e-03],\n        [ 1.4133e-03, -6.9332e-04, -1.0319e-03,  ..., -1.0586e-04,\n          1.3142e-03, -1.4954e-03],\n        [-7.5531e-04, -1.8263e-03, -8.4734e-04,  ..., -4.8089e-04,\n          7.1049e-04, -2.2068e-03],\n        ...,\n        [ 2.1782e-03,  1.3647e-03,  2.2621e-03,  ...,  1.4305e-05,\n          1.2970e-04,  3.3283e-04],\n        [ 4.0770e-05,  1.0996e-03, -6.9904e-04,  ...,  4.1151e-04,\n          1.0185e-03,  1.7452e-03],\n        [ 5.0545e-05,  6.8808e-04, -2.2469e-03,  ..., -5.3978e-04,\n          2.1248e-03, -1.3924e-03]])\nmodel.layers.15.self_attn.o_proj.base_layer.weight: tensor([[ 0.0034,  0.0132, -0.0006,  ..., -0.0074,  0.0024, -0.0047],\n        [ 0.0041, -0.0025,  0.0054,  ..., -0.0145,  0.0041,  0.0009],\n        [-0.0057, -0.0040,  0.0006,  ..., -0.0031,  0.0072, -0.0147],\n        ...,\n        [-0.0078, -0.0017, -0.0022,  ...,  0.0110, -0.0070, -0.0031],\n        [-0.0176, -0.0034, -0.0071,  ..., -0.0146, -0.0054,  0.0075],\n        [ 0.0036, -0.0052, -0.0049,  ..., -0.0029, -0.0083,  0.0148]])\nmodel.layers.15.self_attn.o_proj.lora_A.default.weight: tensor([[-0.0110,  0.0041, -0.0157,  ...,  0.0041, -0.0060,  0.0120],\n        [ 0.0081,  0.0111,  0.0089,  ..., -0.0124,  0.0057, -0.0150],\n        [ 0.0019, -0.0044, -0.0060,  ...,  0.0184,  0.0123, -0.0014],\n        ...,\n        [-0.0188,  0.0117, -0.0173,  ..., -0.0062, -0.0068, -0.0195],\n        [-0.0115, -0.0092, -0.0037,  ..., -0.0136,  0.0157, -0.0159],\n        [ 0.0152, -0.0088,  0.0120,  ..., -0.0014,  0.0004,  0.0025]])\nmodel.layers.15.self_attn.o_proj.lora_B.default.weight: tensor([[-1.9264e-04,  3.1624e-03,  2.5101e-03,  ..., -5.8174e-04,\n         -2.5787e-03,  2.1000e-03],\n        [ 7.6294e-05,  2.4529e-03, -1.2131e-03,  ..., -2.2182e-03,\n         -7.2098e-04,  1.2875e-04],\n        [ 1.7138e-03, -2.3918e-03, -6.1035e-05,  ..., -1.4839e-03,\n         -1.0204e-04,  6.1226e-04],\n        ...,\n        [-1.4782e-04, -1.5378e-04, -3.4475e-04,  ...,  2.6112e-03,\n          5.4407e-04,  5.2834e-04],\n        [-2.8114e-03,  1.2302e-03,  1.4839e-03,  ...,  1.1377e-03,\n         -3.0661e-04, -1.4954e-03],\n        [-1.0281e-03,  1.6642e-04,  6.5279e-04,  ...,  4.7207e-05,\n         -1.6727e-03,  4.5609e-04]])\nmodel.layers.15.mlp.gate_proj.base_layer.weight: tensor([[-0.0031, -0.0002,  0.0045,  ..., -0.0024,  0.0024,  0.0008],\n        [ 0.0084, -0.0066,  0.0177,  ..., -0.0016, -0.0042, -0.0047],\n        [-0.0130,  0.0029, -0.0026,  ..., -0.0103, -0.0006, -0.0030],\n        ...,\n        [ 0.0042,  0.0026,  0.0005,  ...,  0.0075, -0.0056,  0.0094],\n        [-0.0021, -0.0044,  0.0066,  ...,  0.0015,  0.0038, -0.0084],\n        [-0.0054, -0.0003,  0.0046,  ..., -0.0036,  0.0028,  0.0039]])\nmodel.layers.15.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0130, -0.0160, -0.0077,  ..., -0.0072,  0.0100, -0.0081],\n        [ 0.0031, -0.0088,  0.0071,  ...,  0.0018,  0.0105,  0.0029],\n        [ 0.0069,  0.0026, -0.0036,  ...,  0.0057,  0.0048,  0.0109],\n        ...,\n        [-0.0158, -0.0046,  0.0003,  ...,  0.0063,  0.0051,  0.0147],\n        [-0.0099,  0.0067,  0.0024,  ..., -0.0015,  0.0019,  0.0230],\n        [-0.0116, -0.0002, -0.0068,  ..., -0.0073, -0.0056, -0.0009]])\nmodel.layers.15.mlp.gate_proj.lora_B.default.weight: tensor([[ 0.0008, -0.0012,  0.0002,  ..., -0.0007,  0.0027, -0.0002],\n        [ 0.0011,  0.0006,  0.0012,  ...,  0.0007,  0.0006, -0.0021],\n        [ 0.0005,  0.0027,  0.0006,  ...,  0.0002,  0.0028,  0.0013],\n        ...,\n        [ 0.0010,  0.0028, -0.0024,  ..., -0.0018, -0.0012,  0.0023],\n        [ 0.0013,  0.0018,  0.0022,  ..., -0.0005, -0.0002,  0.0022],\n        [ 0.0022, -0.0015, -0.0011,  ..., -0.0036, -0.0038,  0.0011]])\nmodel.layers.15.mlp.up_proj.base_layer.weight: tensor([[ 0.0037, -0.0106,  0.0015,  ..., -0.0027,  0.0038, -0.0033],\n        [-0.0015,  0.0128,  0.0022,  ..., -0.0034,  0.0069,  0.0003],\n        [ 0.0037,  0.0074, -0.0107,  ..., -0.0066, -0.0103,  0.0053],\n        ...,\n        [ 0.0033, -0.0041, -0.0023,  ...,  0.0019, -0.0059, -0.0029],\n        [-0.0017, -0.0036,  0.0086,  ...,  0.0019,  0.0041, -0.0002],\n        [-0.0004,  0.0029,  0.0061,  ..., -0.0030, -0.0060, -0.0023]])\nmodel.layers.15.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0064,  0.0016,  0.0023,  ..., -0.0069,  0.0050, -0.0177],\n        [-0.0002, -0.0191, -0.0107,  ...,  0.0046, -0.0168,  0.0012],\n        [-0.0166,  0.0094, -0.0111,  ...,  0.0101, -0.0045,  0.0055],\n        ...,\n        [ 0.0052, -0.0032,  0.0165,  ...,  0.0062, -0.0072,  0.0194],\n        [-0.0116, -0.0015, -0.0005,  ...,  0.0049,  0.0116,  0.0051],\n        [ 0.0084, -0.0005,  0.0040,  ..., -0.0134,  0.0117, -0.0033]])\nmodel.layers.15.mlp.up_proj.lora_B.default.weight: tensor([[-6.6185e-04,  1.1692e-03, -3.1662e-04,  ..., -3.3245e-03,\n          1.1616e-03,  2.8419e-04],\n        [ 1.9026e-04,  2.4643e-03, -1.3542e-03,  ..., -1.1377e-03,\n          2.7351e-03,  2.5511e-04],\n        [-2.2068e-03, -3.0308e-03,  1.0338e-03,  ...,  1.0061e-04,\n          9.8801e-04, -5.1165e-04],\n        ...,\n        [ 5.4693e-04,  3.0308e-03, -8.7404e-04,  ...,  1.7633e-03,\n         -8.4400e-05,  6.0081e-04],\n        [-1.0214e-03, -2.9182e-03,  1.4296e-03,  ...,  2.4586e-03,\n          5.7316e-04,  2.6369e-04],\n        [-8.6117e-04, -2.9373e-03,  3.6335e-03,  ...,  3.2997e-04,\n         -5.6648e-04,  1.6956e-03]])\nmodel.layers.15.mlp.down_proj.base_layer.weight: tensor([[ 1.5793e-03, -1.0681e-02, -6.1951e-03,  ...,  1.9455e-03,\n          1.1536e-02,  2.3804e-03],\n        [ 6.8359e-03,  4.5776e-03, -9.8705e-05,  ...,  3.8605e-03,\n         -8.0566e-03, -3.1433e-03],\n        [-3.7384e-03, -6.0425e-03, -2.6894e-04,  ...,  7.1716e-03,\n          1.3000e-02, -1.2146e-02],\n        ...,\n        [-8.2779e-04,  5.0354e-04, -1.0254e-02,  ..., -4.6082e-03,\n         -2.7924e-03, -4.8218e-03],\n        [-5.9814e-03,  2.5177e-03, -9.9182e-04,  ...,  9.5825e-03,\n          1.0147e-03, -1.2665e-03],\n        [-6.7749e-03,  5.7983e-03, -1.2390e-02,  ..., -2.4719e-03,\n          4.6387e-03,  9.0942e-03]])\nmodel.layers.15.mlp.down_proj.lora_A.default.weight: tensor([[ 1.7643e-03, -3.9787e-03,  4.8876e-04,  ..., -3.5439e-03,\n         -2.4109e-03,  3.5477e-03],\n        [-7.4577e-04,  2.6131e-03, -3.2043e-04,  ..., -6.1989e-03,\n         -4.1962e-03, -1.2474e-03],\n        [ 1.7872e-03, -2.5558e-04, -4.8399e-04,  ...,  1.7204e-03,\n          4.9171e-03,  1.1101e-03],\n        ...,\n        [ 8.7929e-04,  2.3861e-03, -7.5340e-04,  ..., -1.3075e-03,\n          3.3760e-03, -2.7161e-03],\n        [-1.5850e-03, -6.4583e-03, -4.9438e-03,  ...,  4.1542e-03,\n         -6.5136e-04, -3.6716e-04],\n        [-3.9101e-03,  7.6151e-04,  2.4796e-05,  ..., -5.3711e-03,\n          2.4166e-03,  2.4700e-03]])\nmodel.layers.15.mlp.down_proj.lora_B.default.weight: tensor([[-9.6893e-04,  1.4620e-03, -2.8563e-04,  ..., -1.6298e-03,\n          5.4312e-04, -3.8052e-04],\n        [-4.2582e-04, -1.5717e-03,  8.1539e-04,  ...,  1.6155e-03,\n          2.1005e-04,  7.2241e-04],\n        [ 1.6060e-03, -2.0676e-03,  5.1022e-04,  ..., -1.9588e-03,\n         -2.4605e-03,  1.5259e-03],\n        ...,\n        [-1.3714e-03, -8.6117e-04,  2.4796e-03,  ...,  2.0313e-03,\n         -7.6294e-04,  1.0099e-03],\n        [ 1.3809e-03,  1.8005e-03,  1.0862e-03,  ..., -6.1035e-05,\n          6.6757e-04, -1.7796e-03],\n        [ 1.4133e-03,  2.8253e-04,  7.3338e-04,  ..., -1.0157e-03,\n         -9.2316e-04, -1.6947e-03]])\nmodel.layers.15.input_layernorm.weight: tensor([0.7344, 1.6875, 1.0078,  ..., 0.8086, 0.9922, 1.5000])\nmodel.layers.15.post_attention_layernorm.weight: tensor([2.7031, 2.2812, 2.7188,  ..., 2.6094, 2.8594, 2.5938])\nmodel.layers.16.self_attn.q_proj.base_layer.weight: tensor([[-0.0117, -0.0038, -0.0025,  ...,  0.0014, -0.0025, -0.0010],\n        [-0.0025,  0.0029, -0.0029,  ..., -0.0053, -0.0002, -0.0054],\n        [ 0.0014, -0.0011, -0.0006,  ..., -0.0070,  0.0032, -0.0029],\n        ...,\n        [ 0.0049, -0.0063,  0.0039,  ...,  0.0004, -0.0023,  0.0028],\n        [-0.0033, -0.0135,  0.0105,  ...,  0.0063,  0.0010,  0.0254],\n        [-0.0020, -0.0081,  0.0042,  ..., -0.0056, -0.0014,  0.0104]])\nmodel.layers.16.self_attn.q_proj.lora_A.default.weight: tensor([[ 0.0175, -0.0116,  0.0113,  ...,  0.0062,  0.0162,  0.0009],\n        [ 0.0011, -0.0079, -0.0023,  ...,  0.0079,  0.0025,  0.0138],\n        [ 0.0043, -0.0018, -0.0031,  ...,  0.0175,  0.0108,  0.0160],\n        ...,\n        [ 0.0065,  0.0031, -0.0073,  ..., -0.0070,  0.0008,  0.0070],\n        [ 0.0007,  0.0090, -0.0114,  ..., -0.0068,  0.0019,  0.0155],\n        [-0.0119,  0.0158, -0.0091,  ..., -0.0061, -0.0046,  0.0083]])\nmodel.layers.16.self_attn.q_proj.lora_B.default.weight: tensor([[-1.6880e-04,  2.0504e-03,  2.7466e-04,  ...,  1.3304e-03,\n         -2.8515e-03,  3.6526e-04],\n        [ 1.6174e-03,  2.5749e-03,  1.4877e-03,  ...,  5.4550e-04,\n         -5.0402e-04,  7.3242e-04],\n        [-1.2159e-05,  2.5463e-04,  2.2125e-03,  ...,  6.7759e-04,\n          6.4039e-04,  9.0933e-04],\n        ...,\n        [-9.5654e-04, -2.5215e-03,  4.1962e-05,  ...,  4.7493e-04,\n          2.9831e-03, -4.5776e-05],\n        [ 1.5249e-03,  1.9398e-03,  8.1301e-04,  ...,  1.0109e-03,\n         -1.3924e-03,  1.8349e-03],\n        [-2.3327e-03,  7.5340e-04,  2.2469e-03,  ...,  1.4458e-03,\n         -2.4014e-03, -2.4242e-03]])\nmodel.layers.16.self_attn.k_proj.base_layer.weight: tensor([[ 0.0066, -0.0115,  0.0002,  ...,  0.0044, -0.0039, -0.0095],\n        [ 0.0009, -0.0089, -0.0056,  ..., -0.0089,  0.0010, -0.0060],\n        [ 0.0016, -0.0004, -0.0017,  ..., -0.0023,  0.0049, -0.0011],\n        ...,\n        [-0.0057,  0.0136, -0.0037,  ...,  0.0078,  0.0025,  0.0048],\n        [ 0.0072, -0.0195,  0.0050,  ..., -0.0063, -0.0052,  0.0042],\n        [ 0.0098,  0.0198, -0.0108,  ...,  0.0225,  0.0024, -0.0099]])\nmodel.layers.16.self_attn.k_proj.lora_A.default.weight: tensor([[ 0.0072,  0.0124, -0.0014,  ...,  0.0022, -0.0029,  0.0095],\n        [-0.0029, -0.0015, -0.0071,  ...,  0.0152,  0.0047,  0.0097],\n        [-0.0086, -0.0047, -0.0064,  ..., -0.0019, -0.0045, -0.0034],\n        ...,\n        [-0.0095,  0.0107, -0.0111,  ...,  0.0095,  0.0041,  0.0035],\n        [ 0.0036, -0.0012, -0.0020,  ..., -0.0007, -0.0066, -0.0023],\n        [ 0.0025,  0.0039,  0.0081,  ...,  0.0014,  0.0047,  0.0080]])\nmodel.layers.16.self_attn.k_proj.lora_B.default.weight: tensor([[-1.5430e-03,  5.4598e-04, -7.3910e-05,  ...,  1.9493e-03,\n          2.5864e-03, -2.3232e-03],\n        [ 5.3883e-05,  5.0735e-04, -6.6757e-06,  ..., -2.7657e-03,\n          7.3051e-04, -1.2321e-03],\n        [ 6.2275e-04,  2.9373e-04, -2.7924e-03,  ..., -3.2787e-03,\n          1.7958e-03, -2.6360e-03],\n        ...,\n        [-1.0338e-03,  4.0054e-05,  9.9373e-04,  ...,  4.9782e-04,\n         -1.6718e-03,  1.3275e-03],\n        [-7.5340e-05,  1.3094e-03, -7.7391e-04,  ..., -3.4571e-04,\n          1.0309e-03,  2.0599e-03],\n        [ 2.2984e-04,  1.7939e-03,  1.3170e-03,  ...,  2.1839e-03,\n          7.9203e-04, -2.3956e-03]])\nmodel.layers.16.self_attn.v_proj.base_layer.weight: tensor([[ 0.0152,  0.0159, -0.0084,  ..., -0.0068, -0.0019, -0.0063],\n        [ 0.0134,  0.0031, -0.0093,  ..., -0.0019, -0.0179,  0.0063],\n        [ 0.0030,  0.0236, -0.0127,  ...,  0.0041,  0.0058, -0.0039],\n        ...,\n        [-0.0077,  0.0063,  0.0071,  ...,  0.0168, -0.0039, -0.0118],\n        [ 0.0036,  0.0214, -0.0013,  ..., -0.0007,  0.0052, -0.0149],\n        [-0.0203, -0.0018,  0.0184,  ..., -0.0148,  0.0024,  0.0148]])\nmodel.layers.16.self_attn.v_proj.lora_A.default.weight: tensor([[ 1.0124e-02, -7.1716e-04, -2.2354e-03,  ...,  1.8250e-02,\n         -1.3687e-02,  5.8365e-03],\n        [-1.0147e-02, -6.6566e-03, -9.2010e-03,  ...,  5.3864e-03,\n          1.3046e-02,  9.4604e-03],\n        [ 1.0468e-02,  1.6373e-02, -5.3406e-05,  ..., -8.4915e-03,\n         -5.2109e-03,  5.8670e-03],\n        ...,\n        [ 1.0468e-02, -1.3657e-02,  4.9553e-03,  ..., -1.5961e-02,\n          2.3727e-03, -8.1787e-03],\n        [-4.2038e-03,  1.5364e-03,  3.9902e-03,  ...,  1.5442e-02,\n         -6.9008e-03, -1.6785e-03],\n        [-1.8806e-03, -5.5962e-03,  7.3814e-04,  ..., -5.0240e-03,\n         -7.2823e-03,  3.8414e-03]])\nmodel.layers.16.self_attn.v_proj.lora_B.default.weight: tensor([[ 1.5373e-03,  4.2295e-04, -1.7405e-03,  ...,  1.1129e-03,\n          4.3869e-04, -9.2745e-04],\n        [ 1.5068e-04, -3.5834e-04,  1.3466e-03,  ...,  4.1723e-04,\n          1.1959e-03,  2.1477e-03],\n        [ 5.1641e-04, -2.0657e-03,  1.5097e-03,  ...,  1.4706e-03,\n         -2.0256e-03, -6.6090e-04],\n        ...,\n        [-1.4944e-03, -2.2888e-05,  1.6899e-03,  ..., -1.5450e-03,\n          1.4601e-03,  5.5504e-04],\n        [ 2.9106e-03,  9.5081e-04, -2.5330e-03,  ...,  2.2697e-03,\n          6.7806e-04, -1.1139e-03],\n        [ 3.4833e-04,  1.1368e-03,  3.7813e-04,  ..., -8.9264e-04,\n         -1.0185e-03, -4.3964e-04]])\nmodel.layers.16.self_attn.o_proj.base_layer.weight: tensor([[ 5.1575e-03,  2.3346e-03, -9.5367e-04,  ..., -7.7209e-03,\n          7.5912e-04, -1.1841e-02],\n        [ 9.4604e-03,  4.7302e-03,  1.3550e-02,  ...,  4.0283e-03,\n          1.7166e-03,  4.2419e-03],\n        [-9.0790e-04, -2.4719e-03, -3.5095e-03,  ..., -9.1195e-06,\n         -2.5177e-03,  6.9580e-03],\n        ...,\n        [-6.1646e-03,  7.1335e-04, -4.3106e-04,  ...,  3.4637e-03,\n          1.9302e-03, -2.4872e-03],\n        [ 1.2207e-03, -3.1281e-03,  5.4321e-03,  ..., -1.8539e-03,\n          1.7853e-03,  6.0425e-03],\n        [-3.4943e-03,  7.1106e-03, -1.1230e-02,  ...,  5.0964e-03,\n         -4.4556e-03,  3.7231e-03]])\nmodel.layers.16.self_attn.o_proj.lora_A.default.weight: tensor([[ 0.0032,  0.0058,  0.0024,  ...,  0.0028, -0.0010,  0.0161],\n        [-0.0023,  0.0056, -0.0091,  ..., -0.0046,  0.0151, -0.0014],\n        [ 0.0032,  0.0029, -0.0075,  ..., -0.0101,  0.0001, -0.0030],\n        ...,\n        [ 0.0022, -0.0014, -0.0151,  ..., -0.0016, -0.0097, -0.0033],\n        [ 0.0011,  0.0074,  0.0134,  ...,  0.0051, -0.0115, -0.0098],\n        [-0.0198, -0.0118, -0.0133,  ..., -0.0091, -0.0135,  0.0029]])\nmodel.layers.16.self_attn.o_proj.lora_B.default.weight: tensor([[ 5.1379e-05, -4.0054e-05,  2.5320e-04,  ...,  2.1992e-03,\n          5.7268e-04, -4.0460e-04],\n        [-1.4496e-04, -2.5368e-03, -3.2234e-03,  ...,  2.5082e-03,\n          1.9133e-04,  3.1681e-03],\n        [-1.9226e-03, -1.3423e-04, -2.5043e-03,  ..., -5.3024e-04,\n          3.7956e-04,  9.3079e-04],\n        ...,\n        [ 2.5291e-03, -1.3123e-03, -2.3918e-03,  ...,  3.2604e-05,\n         -1.0815e-03,  1.0548e-03],\n        [-1.2989e-03,  8.1396e-04, -1.0529e-03,  ..., -8.9359e-04,\n         -1.1864e-03,  2.8920e-04],\n        [ 9.0694e-04, -7.7772e-04, -1.4029e-03,  ..., -4.8733e-04,\n          1.0729e-04,  2.8467e-04]])\nmodel.layers.16.mlp.gate_proj.base_layer.weight: tensor([[ 0.0057,  0.0025, -0.0003,  ...,  0.0046,  0.0039,  0.0074],\n        [ 0.0018, -0.0014, -0.0005,  ...,  0.0005, -0.0017,  0.0018],\n        [-0.0051, -0.0011, -0.0005,  ...,  0.0045,  0.0012, -0.0060],\n        ...,\n        [ 0.0023,  0.0057,  0.0033,  ..., -0.0060,  0.0076,  0.0015],\n        [ 0.0045,  0.0042, -0.0002,  ...,  0.0040, -0.0049,  0.0034],\n        [ 0.0041,  0.0096,  0.0065,  ...,  0.0030,  0.0042, -0.0045]])\nmodel.layers.16.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0016,  0.0082,  0.0067,  ..., -0.0103, -0.0080,  0.0023],\n        [-0.0179,  0.0032, -0.0080,  ...,  0.0063,  0.0018, -0.0104],\n        [-0.0030, -0.0153,  0.0035,  ...,  0.0150,  0.0074,  0.0127],\n        ...,\n        [ 0.0075,  0.0153,  0.0094,  ...,  0.0161, -0.0101,  0.0035],\n        [-0.0121,  0.0023, -0.0172,  ..., -0.0066, -0.0026, -0.0033],\n        [-0.0121,  0.0002, -0.0049,  ..., -0.0056,  0.0122, -0.0023]])\nmodel.layers.16.mlp.gate_proj.lora_B.default.weight: tensor([[ 5.5432e-05, -1.3819e-03,  1.5230e-03,  ...,  3.8266e-05,\n         -1.4086e-03, -1.6775e-03],\n        [-1.0433e-03, -4.8637e-05,  1.4915e-03,  ..., -2.1935e-04,\n         -2.4109e-03,  1.0166e-03],\n        [-1.7834e-03, -2.1648e-04,  3.3455e-03,  ...,  2.0695e-03,\n         -3.5973e-03,  3.3588e-03],\n        ...,\n        [ 1.2217e-03,  1.9588e-03, -1.1311e-03,  ...,  2.0809e-03,\n          2.9774e-03, -1.6880e-03],\n        [-9.5701e-04,  2.1687e-03, -2.4509e-04,  ..., -2.0103e-03,\n         -6.0463e-04,  2.6798e-03],\n        [-7.5531e-04, -1.9321e-03,  4.0054e-04,  ..., -1.2445e-03,\n         -8.0872e-04,  3.0708e-03]])\nmodel.layers.16.mlp.up_proj.base_layer.weight: tensor([[ 0.0032,  0.0035,  0.0108,  ...,  0.0118, -0.0096, -0.0100],\n        [-0.0027,  0.0009, -0.0037,  ...,  0.0067, -0.0102,  0.0023],\n        [ 0.0028,  0.0015,  0.0052,  ...,  0.0010, -0.0037, -0.0064],\n        ...,\n        [ 0.0131, -0.0072, -0.0002,  ..., -0.0029, -0.0002, -0.0074],\n        [-0.0037,  0.0096, -0.0039,  ...,  0.0008,  0.0043, -0.0040],\n        [-0.0037, -0.0050, -0.0063,  ..., -0.0023,  0.0066, -0.0031]])\nmodel.layers.16.mlp.up_proj.lora_A.default.weight: tensor([[-0.0007, -0.0130, -0.0116,  ...,  0.0164,  0.0103,  0.0193],\n        [ 0.0082,  0.0160,  0.0055,  ...,  0.0021,  0.0032, -0.0056],\n        [ 0.0184,  0.0049,  0.0044,  ...,  0.0104, -0.0002,  0.0042],\n        ...,\n        [-0.0107,  0.0021, -0.0065,  ...,  0.0105,  0.0086, -0.0039],\n        [ 0.0160,  0.0077,  0.0215,  ...,  0.0114,  0.0101,  0.0078],\n        [ 0.0133, -0.0002,  0.0129,  ..., -0.0032,  0.0004, -0.0157]])\nmodel.layers.16.mlp.up_proj.lora_B.default.weight: tensor([[ 1.5974e-03, -7.0190e-04, -2.0742e-04,  ..., -7.5817e-04,\n         -1.7033e-03,  2.9564e-05],\n        [ 1.5955e-03,  4.3988e-05, -1.9875e-03,  ..., -3.0661e-04,\n          1.7405e-05, -1.9026e-03],\n        [-1.2045e-03,  5.0306e-04,  1.0805e-03,  ..., -1.6069e-04,\n         -7.4625e-04,  8.3113e-04],\n        ...,\n        [-1.2932e-03,  9.9182e-04, -1.1978e-03,  ...,  7.1669e-04,\n         -6.6280e-04, -1.3008e-03],\n        [-3.0556e-03, -1.6623e-03, -2.9716e-03,  ...,  3.5629e-03,\n         -2.1687e-03,  3.3226e-03],\n        [-4.1294e-04, -1.1885e-04,  2.9993e-04,  ...,  1.0586e-03,\n          5.7888e-04,  5.6267e-05]])\nmodel.layers.16.mlp.down_proj.base_layer.weight: tensor([[ 0.0013,  0.0024,  0.0092,  ..., -0.0080,  0.0050, -0.0036],\n        [ 0.0045, -0.0021, -0.0055,  ..., -0.0038, -0.0072, -0.0028],\n        [-0.0109, -0.0057, -0.0065,  ..., -0.0029, -0.0027, -0.0068],\n        ...,\n        [ 0.0019, -0.0026, -0.0009,  ...,  0.0038, -0.0007,  0.0067],\n        [-0.0046, -0.0035,  0.0059,  ..., -0.0168, -0.0051, -0.0090],\n        [ 0.0048,  0.0038,  0.0006,  ..., -0.0026, -0.0010,  0.0013]])\nmodel.layers.16.mlp.down_proj.lora_A.default.weight: tensor([[-8.5602e-03,  2.4071e-03,  6.1035e-05,  ...,  2.6093e-03,\n         -1.9646e-03, -1.9073e-05],\n        [-5.8746e-03, -2.6741e-03, -4.5738e-03,  ..., -4.0741e-03,\n         -5.7297e-03, -5.3253e-03],\n        [-6.9733e-03, -4.0131e-03,  5.3787e-04,  ...,  1.0471e-03,\n          1.7748e-03, -1.4019e-03],\n        ...,\n        [ 3.1605e-03,  1.5612e-03, -6.5613e-04,  ...,  6.3972e-03,\n         -4.5357e-03,  4.4098e-03],\n        [-1.8597e-03,  3.3684e-03,  3.1853e-03,  ..., -8.5068e-04,\n          4.7073e-03,  2.1553e-03],\n        [ 1.2178e-03,  3.8147e-03,  1.8501e-04,  ...,  1.4286e-03,\n          3.5820e-03,  6.4011e-03]])\nmodel.layers.16.mlp.down_proj.lora_B.default.weight: tensor([[ 0.0024,  0.0008,  0.0009,  ..., -0.0010,  0.0031, -0.0020],\n        [-0.0008, -0.0001,  0.0011,  ...,  0.0010, -0.0021,  0.0012],\n        [-0.0002,  0.0025, -0.0003,  ..., -0.0011, -0.0021,  0.0004],\n        ...,\n        [ 0.0013, -0.0017,  0.0010,  ..., -0.0024,  0.0026, -0.0014],\n        [-0.0002,  0.0006, -0.0027,  ..., -0.0006,  0.0023, -0.0021],\n        [ 0.0004, -0.0002, -0.0004,  ..., -0.0015,  0.0002, -0.0024]])\nmodel.layers.16.input_layernorm.weight: tensor([0.8359, 1.5234, 1.2344,  ..., 0.8789, 1.3750, 0.9688])\nmodel.layers.16.post_attention_layernorm.weight: tensor([2.6406, 2.4688, 2.8125,  ..., 2.6719, 2.8594, 2.7031])\nmodel.layers.17.self_attn.q_proj.base_layer.weight: tensor([[-0.0004, -0.0166,  0.0009,  ...,  0.0073, -0.0013, -0.0022],\n        [-0.0259, -0.0041,  0.0066,  ...,  0.0209,  0.0039,  0.0142],\n        [ 0.0073,  0.0010, -0.0104,  ...,  0.0034, -0.0061,  0.0017],\n        ...,\n        [ 0.0080, -0.0072,  0.0030,  ..., -0.0038, -0.0085,  0.0008],\n        [-0.0029, -0.0018, -0.0033,  ..., -0.0004, -0.0027, -0.0023],\n        [ 0.0060, -0.0029,  0.0023,  ...,  0.0032,  0.0031,  0.0054]])\nmodel.layers.17.self_attn.q_proj.lora_A.default.weight: tensor([[-0.0060, -0.0006,  0.0077,  ..., -0.0168,  0.0078, -0.0034],\n        [ 0.0094,  0.0081, -0.0022,  ...,  0.0136,  0.0017,  0.0053],\n        [ 0.0041,  0.0077, -0.0018,  ..., -0.0134, -0.0112, -0.0095],\n        ...,\n        [ 0.0069, -0.0115,  0.0147,  ...,  0.0110, -0.0080, -0.0006],\n        [ 0.0050, -0.0188, -0.0030,  ..., -0.0085,  0.0034, -0.0036],\n        [-0.0132,  0.0033, -0.0183,  ..., -0.0084, -0.0034,  0.0020]])\nmodel.layers.17.self_attn.q_proj.lora_B.default.weight: tensor([[ 5.8651e-04, -2.0218e-03, -1.6222e-03,  ..., -5.3883e-04,\n         -1.4000e-03, -2.6474e-03],\n        [-2.5368e-03,  2.3098e-03,  2.7523e-03,  ...,  2.9707e-04,\n          5.2643e-04,  2.5291e-03],\n        [ 2.2659e-03, -1.9608e-03, -1.5898e-03,  ..., -1.4706e-03,\n          1.7605e-03, -2.3289e-03],\n        ...,\n        [-1.7738e-03, -1.6537e-03, -5.7411e-04,  ..., -8.2397e-04,\n         -2.9945e-03,  2.3079e-03],\n        [-9.3842e-04,  2.3880e-03, -5.9891e-04,  ..., -4.8971e-04,\n         -3.4666e-04,  2.1076e-03],\n        [-2.2926e-03,  3.1471e-04,  3.2425e-05,  ..., -1.5869e-03,\n         -2.8458e-03, -1.1158e-03]])\nmodel.layers.17.self_attn.k_proj.base_layer.weight: tensor([[-0.0087, -0.0017,  0.0008,  ...,  0.0042,  0.0004,  0.0034],\n        [-0.0017, -0.0027, -0.0067,  ..., -0.0014, -0.0029, -0.0082],\n        [-0.0025, -0.0065,  0.0033,  ...,  0.0069,  0.0014, -0.0037],\n        ...,\n        [-0.0060, -0.0042,  0.0020,  ..., -0.0111, -0.0058, -0.0058],\n        [-0.0038,  0.0055, -0.0044,  ...,  0.0088, -0.0015, -0.0032],\n        [ 0.0069, -0.0046, -0.0162,  ..., -0.0011,  0.0044,  0.0118]])\nmodel.layers.17.self_attn.k_proj.lora_A.default.weight: tensor([[ 6.1188e-03, -3.8586e-03,  3.4161e-03,  ...,  1.2604e-02,\n          1.3428e-02, -4.5815e-03],\n        [ 1.5747e-02, -1.9028e-02,  4.4327e-03,  ...,  6.9046e-04,\n          7.1602e-03, -6.4087e-03],\n        [-3.8624e-03, -1.1620e-02, -1.1200e-02,  ..., -9.4223e-04,\n          3.4924e-03,  1.4938e-02],\n        ...,\n        [-1.3260e-02, -9.9182e-04, -1.3107e-02,  ...,  1.4030e-02,\n          7.3662e-03,  1.1955e-02],\n        [-2.0051e-04,  1.4496e-02, -1.4496e-03,  ...,  6.8665e-05,\n         -7.6256e-03,  4.5471e-03],\n        [ 1.1616e-03,  1.4107e-02,  4.4289e-03,  ..., -6.8283e-03,\n          1.9302e-03, -1.6052e-02]])\nmodel.layers.17.self_attn.k_proj.lora_B.default.weight: tensor([[ 1.3142e-03, -1.4086e-03, -2.1667e-03,  ...,  1.1692e-03,\n          3.0065e-04,  1.9932e-03],\n        [ 4.0054e-05,  2.7962e-03,  3.8147e-04,  ...,  4.0722e-04,\n          1.2836e-03,  2.0027e-04],\n        [ 1.6689e-03, -8.1491e-04,  2.9659e-04,  ..., -6.9809e-04,\n          3.1614e-04, -1.8930e-04],\n        ...,\n        [ 2.0409e-03, -1.5898e-03,  4.1032e-04,  ..., -1.9226e-03,\n          1.7061e-03,  3.4952e-04],\n        [-3.1567e-04, -5.0259e-04, -3.9721e-04,  ...,  2.4929e-03,\n         -2.0828e-03,  1.4658e-03],\n        [-4.3392e-04,  7.9918e-04, -3.8624e-04,  ...,  3.3855e-05,\n          1.3266e-03, -2.0695e-04]])\nmodel.layers.17.self_attn.v_proj.base_layer.weight: tensor([[ 0.0008,  0.0008, -0.0221,  ..., -0.0138, -0.0147,  0.0071],\n        [-0.0074,  0.0099,  0.0050,  ..., -0.0095, -0.0085,  0.0014],\n        [-0.0103,  0.0051, -0.0019,  ...,  0.0009, -0.0093,  0.0003],\n        ...,\n        [ 0.0026, -0.0092,  0.0012,  ...,  0.0034, -0.0047, -0.0101],\n        [-0.0044,  0.0073,  0.0036,  ..., -0.0194, -0.0164, -0.0052],\n        [ 0.0029,  0.0048, -0.0061,  ..., -0.0018,  0.0061, -0.0095]])\nmodel.layers.17.self_attn.v_proj.lora_A.default.weight: tensor([[ 1.5450e-02,  1.3512e-02,  1.4572e-02,  ..., -1.3283e-02,\n         -1.5350e-02, -1.5808e-02],\n        [-1.3443e-02, -1.6312e-02, -1.1063e-03,  ...,  1.8448e-02,\n         -7.1602e-03,  1.3618e-02],\n        [ 9.5367e-04,  7.9193e-03,  6.6528e-03,  ..., -1.3733e-02,\n         -7.4997e-03,  3.8147e-05],\n        ...,\n        [-3.4180e-03,  1.1345e-02,  1.2085e-02,  ...,  4.3030e-03,\n          7.6294e-04,  1.3260e-02],\n        [-1.3298e-02, -6.6795e-03,  3.7003e-04,  ...,  4.6997e-03,\n          5.9624e-03,  1.7746e-02],\n        [-1.2909e-02,  6.8741e-03, -1.1536e-02,  ..., -1.0376e-02,\n          1.5610e-02,  4.8523e-03]])\nmodel.layers.17.self_attn.v_proj.lora_B.default.weight: tensor([[ 4.1890e-04,  1.7395e-03,  9.0933e-04,  ..., -9.8991e-04,\n         -1.4009e-03, -3.4809e-05],\n        [ 4.3416e-04, -9.4366e-04, -1.4086e-03,  ...,  1.1005e-03,\n          1.1177e-03,  3.5477e-04],\n        [ 2.6798e-04,  3.4142e-04,  1.5688e-04,  ...,  1.8520e-03,\n          9.2506e-04,  3.7718e-04],\n        ...,\n        [-1.2217e-03,  1.4200e-03,  1.9150e-03,  ..., -6.8092e-04,\n         -7.5293e-04, -5.7697e-04],\n        [-7.6246e-04,  1.9598e-04,  6.0844e-04,  ..., -1.9779e-03,\n         -3.8624e-04, -4.0674e-04],\n        [-1.3733e-03, -5.9557e-04,  1.0240e-04,  ..., -1.3180e-03,\n         -1.8430e-04, -1.2054e-03]])\nmodel.layers.17.self_attn.o_proj.base_layer.weight: tensor([[ 6.8359e-03,  3.8605e-03, -1.3367e-02,  ...,  9.5215e-03,\n          5.4321e-03, -6.1646e-03],\n        [ 1.0925e-02,  2.9602e-03,  1.8677e-02,  ...,  5.7068e-03,\n         -2.4414e-03, -6.5918e-03],\n        [-2.4567e-03, -3.1433e-03,  2.5024e-03,  ...,  1.7334e-02,\n         -1.0376e-02,  1.5564e-02],\n        ...,\n        [-9.0942e-03, -1.1963e-02, -9.3994e-03,  ...,  1.0872e-04,\n          1.1230e-02,  7.2327e-03],\n        [-1.6174e-03, -7.9346e-04, -2.3926e-02,  ...,  2.8381e-03,\n          1.0498e-02, -2.8849e-05],\n        [-8.0566e-03, -9.8267e-03,  8.4229e-03,  ...,  1.7334e-02,\n          6.6223e-03,  1.5625e-02]])\nmodel.layers.17.self_attn.o_proj.lora_A.default.weight: tensor([[-6.0387e-03, -1.6769e-02, -1.7319e-03,  ...,  1.5411e-02,\n          2.3651e-04,  7.9155e-04],\n        [ 1.9730e-02,  8.7738e-03, -1.2894e-03,  ..., -1.0971e-02,\n          1.8707e-02, -1.0300e-02],\n        [ 6.5651e-03,  1.8215e-03,  5.2185e-03,  ..., -3.0899e-04,\n         -1.4343e-03, -1.6418e-02],\n        ...,\n        [-1.2634e-02,  3.5934e-03, -1.0239e-02,  ..., -4.1618e-03,\n         -1.3321e-02,  8.6212e-03],\n        [-1.0864e-02,  2.0081e-02,  1.5831e-03,  ..., -1.3168e-02,\n         -4.2725e-03, -1.4374e-02],\n        [-3.5591e-03, -1.4687e-03,  1.6327e-03,  ..., -8.3923e-05,\n          2.4605e-04, -4.2305e-03]])\nmodel.layers.17.self_attn.o_proj.lora_B.default.weight: tensor([[-2.3437e-04,  2.6741e-03,  3.5810e-04,  ..., -2.1696e-04,\n         -1.7757e-03, -6.3705e-04],\n        [-5.6648e-04, -3.4237e-04, -8.8120e-04,  ...,  4.4155e-04,\n          4.1914e-04,  3.9864e-04],\n        [-5.9319e-04,  7.7868e-04,  1.4439e-03,  ..., -1.8854e-03,\n         -4.3321e-04, -1.8263e-04],\n        ...,\n        [ 2.4986e-03,  3.2940e-03,  9.0742e-04,  ...,  7.0238e-04,\n         -5.4550e-04,  2.8038e-04],\n        [-6.8665e-04,  1.1806e-03, -1.8966e-04,  ...,  9.8133e-04,\n         -8.7881e-04,  8.0967e-04],\n        [-4.3082e-04, -6.9475e-04,  7.5340e-05,  ...,  3.9053e-04,\n          1.6928e-03,  2.4643e-03]])\nmodel.layers.17.mlp.gate_proj.base_layer.weight: tensor([[-0.0037, -0.0089,  0.0052,  ...,  0.0028,  0.0065, -0.0025],\n        [ 0.0009, -0.0034, -0.0032,  ...,  0.0014, -0.0065,  0.0006],\n        [-0.0074,  0.0107,  0.0001,  ...,  0.0012, -0.0027,  0.0040],\n        ...,\n        [ 0.0023, -0.0010,  0.0009,  ...,  0.0004, -0.0037, -0.0020],\n        [ 0.0077, -0.0016,  0.0057,  ...,  0.0078,  0.0011,  0.0065],\n        [ 0.0096,  0.0099,  0.0071,  ...,  0.0086, -0.0034,  0.0003]])\nmodel.layers.17.mlp.gate_proj.lora_A.default.weight: tensor([[-0.0015, -0.0105,  0.0072,  ...,  0.0046,  0.0190, -0.0010],\n        [ 0.0106,  0.0111,  0.0134,  ...,  0.0103,  0.0153,  0.0062],\n        [-0.0021,  0.0097, -0.0091,  ..., -0.0051,  0.0005, -0.0034],\n        ...,\n        [ 0.0008,  0.0080, -0.0132,  ...,  0.0036,  0.0041,  0.0099],\n        [ 0.0038,  0.0078,  0.0037,  ..., -0.0036, -0.0044,  0.0116],\n        [ 0.0036,  0.0067,  0.0036,  ...,  0.0082,  0.0063,  0.0082]])\nmodel.layers.17.mlp.gate_proj.lora_B.default.weight: tensor([[-0.0017,  0.0016,  0.0012,  ...,  0.0004,  0.0013,  0.0013],\n        [ 0.0024,  0.0004, -0.0009,  ...,  0.0001,  0.0003,  0.0005],\n        [-0.0030,  0.0012, -0.0008,  ..., -0.0023, -0.0020,  0.0027],\n        ...,\n        [ 0.0015, -0.0004, -0.0023,  ...,  0.0004, -0.0003, -0.0001],\n        [-0.0014,  0.0109,  0.0013,  ...,  0.0014,  0.0014,  0.0031],\n        [-0.0002, -0.0021,  0.0009,  ..., -0.0006,  0.0005, -0.0011]])\nmodel.layers.17.mlp.up_proj.base_layer.weight: tensor([[-3.6774e-03,  2.8229e-03,  8.1787e-03,  ..., -6.1035e-03,\n         -1.2054e-03,  4.0233e-06],\n        [-1.3504e-03, -6.1646e-03, -5.6076e-04,  ...,  7.5073e-03,\n         -6.5994e-04,  1.0010e-02],\n        [-1.9836e-03, -5.7373e-03, -6.9885e-03,  ...,  9.1076e-05,\n         -1.0529e-03, -4.7607e-03],\n        ...,\n        [-1.8768e-03,  1.1063e-03, -2.3041e-03,  ...,  4.2114e-03,\n          1.7395e-03,  1.7700e-03],\n        [ 5.2795e-03, -3.6621e-03, -7.8735e-03,  ..., -3.1281e-03,\n         -1.4343e-03, -1.4191e-03],\n        [ 2.6398e-03, -1.7548e-03, -2.1057e-03,  ..., -1.3885e-03,\n         -4.0894e-03, -1.5137e-02]])\nmodel.layers.17.mlp.up_proj.lora_A.default.weight: tensor([[ 0.0068,  0.0124,  0.0006,  ..., -0.0113,  0.0138, -0.0105],\n        [ 0.0204,  0.0018,  0.0048,  ..., -0.0142, -0.0018, -0.0015],\n        [ 0.0037,  0.0023,  0.0068,  ...,  0.0007, -0.0063,  0.0191],\n        ...,\n        [ 0.0099,  0.0015,  0.0014,  ..., -0.0186, -0.0047, -0.0192],\n        [-0.0087,  0.0023, -0.0091,  ..., -0.0106, -0.0011,  0.0047],\n        [-0.0124, -0.0022, -0.0120,  ...,  0.0177,  0.0093,  0.0065]])\nmodel.layers.17.mlp.up_proj.lora_B.default.weight: tensor([[-1.3285e-03, -9.6750e-04,  9.0361e-04,  ..., -3.6383e-04,\n          1.3504e-03,  2.5654e-04],\n        [ 4.8018e-04,  1.4658e-03,  9.0599e-05,  ..., -8.6403e-04,\n         -1.3723e-03,  1.0557e-03],\n        [ 2.2354e-03,  2.2240e-03,  9.2697e-04,  ..., -1.3828e-05,\n          1.6136e-03,  1.3819e-03],\n        ...,\n        [-3.3855e-04, -2.5368e-03, -3.0632e-03,  ..., -8.3303e-04,\n         -1.2169e-03, -2.1362e-04],\n        [ 2.7008e-03,  1.2302e-03,  1.2207e-03,  ...,  6.1750e-04,\n          1.3580e-03,  1.3323e-03],\n        [-2.1019e-03, -2.1896e-03, -6.2561e-04,  ..., -2.1324e-03,\n          1.2360e-03,  9.9373e-04]])\nmodel.layers.17.mlp.down_proj.base_layer.weight: tensor([[ 4.5395e-04, -1.0681e-04,  1.1597e-03,  ...,  1.4420e-03,\n          7.7209e-03, -7.1106e-03],\n        [ 3.3264e-03,  4.7874e-04,  5.0049e-03,  ...,  1.6479e-03,\n          5.2490e-03,  7.8201e-04],\n        [-1.2207e-04,  4.7607e-03, -5.4550e-04,  ...,  4.6730e-04,\n         -6.1035e-03, -1.0010e-02],\n        ...,\n        [-6.5918e-03,  9.2773e-03, -6.7139e-04,  ..., -1.5869e-03,\n         -6.5308e-03,  8.5449e-04],\n        [-4.0894e-03, -2.6855e-03, -5.8889e-05,  ..., -9.5749e-04,\n         -7.4158e-03,  1.3184e-02],\n        [ 3.7003e-04, -1.5945e-03,  2.3499e-03,  ..., -1.7166e-04,\n          9.5215e-03, -5.2795e-03]])\nmodel.layers.17.mlp.down_proj.lora_A.default.weight: tensor([[-1.1663e-03,  7.4196e-03,  1.5450e-04,  ...,  3.7980e-04,\n          2.7580e-03, -1.7977e-03],\n        [-2.6131e-03,  2.1610e-03, -5.4817e-03,  ...,  4.3182e-03,\n         -3.0632e-03,  2.6798e-03],\n        [ 3.3550e-03, -1.8597e-04,  3.2310e-03,  ..., -4.6158e-03,\n          3.0289e-03, -3.1052e-03],\n        ...,\n        [-5.4855e-03,  4.2953e-03, -1.3409e-03,  ...,  3.5858e-03,\n          7.5722e-04,  4.0588e-03],\n        [ 3.1586e-03,  5.4474e-03,  4.7607e-03,  ..., -4.0054e-05,\n         -5.4703e-03, -2.5387e-03],\n        [-3.8300e-03,  1.6212e-05, -4.5013e-03,  ..., -2.1439e-03,\n         -2.6417e-04,  3.8357e-03]])\nmodel.layers.17.mlp.down_proj.lora_B.default.weight: tensor([[ 2.4719e-03, -2.4395e-03, -2.4490e-03,  ...,  2.1973e-03,\n         -6.6757e-06,  3.3903e-04],\n        [ 5.8365e-04, -6.9809e-04,  1.0252e-03,  ...,  4.7112e-04,\n          4.1604e-04, -1.2732e-03],\n        [ 1.2970e-03, -1.0967e-03, -1.1272e-03,  ..., -1.7905e-04,\n          1.3723e-03, -2.5024e-03],\n        ...,\n        [ 9.2316e-04,  1.2004e-04, -1.1939e-04,  ...,  6.3610e-04,\n         -1.7941e-04,  1.8311e-04],\n        [ 2.7199e-03, -2.8381e-03, -2.6817e-03,  ...,  2.4948e-03,\n         -5.7220e-05,  2.4033e-04],\n        [ 2.6512e-03, -1.3647e-03, -4.1842e-04,  ..., -1.0557e-03,\n         -1.3332e-03, -1.5182e-03]])\nmodel.layers.17.input_layernorm.weight: tensor([1.3438, 2.1094, 1.4453,  ..., 1.6719, 1.5938, 1.6406])\nmodel.layers.17.post_attention_layernorm.weight: tensor([2.2656, 2.8125, 2.9531,  ..., 2.7188, 2.6406, 2.8438])\nmodel.norm.weight: tensor([0.1562, 0.6172, 0.4609,  ..., 0.6094, 0.3789, 0.6445])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from transformers import pipeline\n\n# Create the pipeline with a different variable name\ntext_generator = pipeline(\n    \"text-generation\",\n    model=global_model,\n    tokenizer=tokenizer,\n)\n\n# Your example question\nquestion = \"What is the difference between a variable and an object\"\n\n# Create the message format\nmessage = [\n    {\"role\": \"user\", \"content\": question},\n]\n\n# Apply the chat template\nprompt = text_generator.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n\n# Generate the output\noutputs = text_generator(\n    prompt,\n    max_new_tokens=512,\n    add_special_tokens=True,\n    do_sample=True,\n    temperature=0.7,\n    top_k=10,\n    top_p=0.95\n)\n\n# Display the result\nMarkdown(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T09:09:04.393765Z","iopub.execute_input":"2025-02-02T09:09:04.394246Z","iopub.status.idle":"2025-02-02T09:09:28.049939Z","shell.execute_reply.started":"2025-02-02T09:09:04.394212Z","shell.execute_reply":"2025-02-02T09:09:28.049137Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Sure, here's the difference between a variable and an object:\n\n**Variable:**\n\n* A variable is a storage location that holds a single value.\n* It is identified by a name and has a specific scope within the program.\n* Once a variable is initialized, its value cannot be changed.\n* Variables are commonly used to store data and make it accessible throughout the program.\n\n**Object:**\n\n* An object is a complex data structure that contains multiple variables and methods.\n* It is an instance of a class, which defines the structure and behavior of the object.\n* Objects can have their own values and can interact with each other.\n* Objects are created from classes and can be used to represent real-world entities.\n\n**Here's an example to illustrate the difference:**\n\n```python\n# Variable\nname = \"John\"\n\n# Object\nperson = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n```\n\nIn this example, `name` is a variable that stores a string, while `person` is an object that contains multiple variables and a class definition.\n\n**Key differences:**\n\n| Feature | Variable | Object |\n|---|---|---|\n| Scope | Local | Global or local |\n| Value | Single | Multiple |\n| Type | Any data type | Class or object type |\n| Creation | Defined at initialization | Created when an object is created |\n| Data structure | Simple (string) | Complex (data structure) |\n| Interactivity | Read-only | Read-write |\n| Use case | Storing and accessing data, making it accessible throughout the program | Creating complex data structures, representing real-world entities |\n\n**In summary:**\n\n* A variable is a storage location for a single value.\n* An object is a complex data structure that contains multiple variables and methods.\n* Objects can interact with each other and have their own values."},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"# Evaluation ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.tokenize import word_tokenize\nfrom nltk.metrics import jaccard_distance\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T09:13:25.856478Z","iopub.execute_input":"2025-02-02T09:13:25.856994Z","iopub.status.idle":"2025-02-02T09:13:25.862436Z","shell.execute_reply.started":"2025-02-02T09:13:25.856942Z","shell.execute_reply":"2025-02-02T09:13:25.861445Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Download required NLTK data\nnltk.download('punkt')\n\ndef calculate_metrics(generated_text, reference_text):\n    \"\"\"\n    Calculate various metrics for text generation evaluation\n    \n    Args:\n        generated_text (str): The model generated text\n        reference_text (str): The ground truth reference text\n    \n    Returns:\n        dict: Dictionary containing various metric scores\n    \"\"\"\n    metrics = {}\n    \n    # Tokenize texts\n    generated_tokens = word_tokenize(generated_text.lower())\n    reference_tokens = [word_tokenize(reference_text.lower())]\n    \n    # BLEU Score\n    smoother = SmoothingFunction().method1\n    try:\n        bleu_score = sentence_bleu(reference_tokens, generated_tokens, \n                                 smoothing_function=smoother)\n        metrics['bleu'] = bleu_score\n    except Exception as e:\n        metrics['bleu'] = 0\n        print(f\"BLEU score calculation failed: {e}\")\n\n    # Jaccard Similarity (1 - distance)\n    gen_set = set(generated_tokens)\n    ref_set = set(reference_tokens[0])\n    try:\n        jaccard_sim = 1 - jaccard_distance(gen_set, ref_set)\n        metrics['jaccard_similarity'] = jaccard_sim\n    except Exception as e:\n        metrics['jaccard_similarity'] = 0\n        print(f\"Jaccard calculation failed: {e}\")\n    \n    # Token overlap ratio\n    common_tokens = len(gen_set.intersection(ref_set))\n    metrics['token_overlap'] = common_tokens / len(ref_set)\n    \n    # Length metrics\n    metrics['generated_length'] = len(generated_tokens)\n    metrics['reference_length'] = len(reference_tokens[0])\n    metrics['length_ratio'] = len(generated_tokens) / len(reference_tokens[0])\n    \n    return metrics\n\ndef evaluate_model(text_generator, eval_data):\n    \"\"\"\n    Evaluate the model on a set of test examples\n    \n    Args:\n        text_generator: The pipeline instance\n        eval_data: List of tuples containing (question, reference_answer)\n    \n    Returns:\n        dict: Aggregated metrics across all examples\n    \"\"\"\n    all_metrics = []\n    \n    for question, reference in eval_data:\n        # Generate response\n        message = [{\"role\": \"user\", \"content\": question}]\n        prompt = text_generator.tokenizer.apply_chat_template(\n            message, tokenize=False, add_generation_prompt=True\n        )\n        \n        outputs = text_generator(\n            prompt,\n            max_new_tokens=512,\n            add_special_tokens=True,\n            do_sample=True,\n            temperature=0.7,\n            top_k=10,\n            top_p=0.95\n        )\n        \n        generated_text = outputs[0][\"generated_text\"][len(prompt):]\n        \n        # Calculate metrics\n        metrics = calculate_metrics(generated_text, reference)\n        all_metrics.append(metrics)\n        \n        # Print individual results\n        print(f\"\\nQuestion: {question}\")\n        print(f\"Generated: {generated_text[:200]}...\")\n        print(f\"Reference: {reference[:200]}...\")\n        print(\"Metrics:\", {k: f\"{v:.4f}\" for k, v in metrics.items()})\n    \n    # Aggregate metrics\n    aggregated_metrics = {}\n    for metric in all_metrics[0].keys():\n        values = [m[metric] for m in all_metrics]\n        aggregated_metrics[f'avg_{metric}'] = np.mean(values)\n        aggregated_metrics[f'std_{metric}'] = np.std(values)\n    \n    return aggregated_metrics\n\n# Example usage:\neval_data = [\n    (\n        \"What is the difference between a variable and an object?\",\n        \"A variable is a named storage location that holds a value, while an object is an instance of a class that contains both data and methods.\"\n    ),\n    (\n        \"Explain what is inheritance in programming?\",\n        \"Inheritance is a fundamental concept in object-oriented programming where a class can inherit properties and methods from another class. This promotes code reuse and establishes a relationship between parent and child classes.\"\n    )\n]\n\n# Run evaluation\nprint(\"Running evaluation...\")\nmetrics = evaluate_model(text_generator, eval_data)\n\nprint(\"\\nAggregated Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T09:13:48.376399Z","iopub.execute_input":"2025-02-02T09:13:48.376785Z","iopub.status.idle":"2025-02-02T09:14:32.738853Z","shell.execute_reply.started":"2025-02-02T09:13:48.376752Z","shell.execute_reply":"2025-02-02T09:14:32.738006Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nRunning evaluation...\n\nQuestion: What is the difference between a variable and an object?\nGenerated: Sure, here's the difference between a variable and an object:\n\n**Variable:**\n\n* A variable is a memory location that stores a single value.\n* It is declared using a keyword (e.g., `int age;`) and assi...\nReference: A variable is a named storage location that holds a value, while an object is an instance of a class that contains both data and methods....\nMetrics: {'bleu': '0.0199', 'jaccard_similarity': '0.1164', 'token_overlap': '0.7727', 'generated_length': '341.0000', 'reference_length': '28.0000', 'length_ratio': '12.1786'}\n\nQuestion: Explain what is inheritance in programming?\nGenerated: Sure. Here's a detailed explanation of inheritance in programming:\n\n**Inheritance** is a mechanism in object-oriented programming (OOP) where a new class is created that inherits properties and behavi...\nReference: Inheritance is a fundamental concept in object-oriented programming where a class can inherit properties and methods from another class. This promotes code reuse and establishes a relationship between...\nMetrics: {'bleu': '0.0068', 'jaccard_similarity': '0.1156', 'token_overlap': '0.7143', 'generated_length': '467.0000', 'reference_length': '34.0000', 'length_ratio': '13.7353'}\n\nAggregated Metrics:\navg_bleu: 0.0133\nstd_bleu: 0.0065\navg_jaccard_similarity: 0.1160\nstd_jaccard_similarity: 0.0004\navg_token_overlap: 0.7435\nstd_token_overlap: 0.0292\navg_generated_length: 404.0000\nstd_generated_length: 63.0000\navg_reference_length: 31.0000\nstd_reference_length: 3.0000\navg_length_ratio: 12.9569\nstd_length_ratio: 0.7784\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}